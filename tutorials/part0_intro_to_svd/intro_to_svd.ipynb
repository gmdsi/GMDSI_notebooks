{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "> \"A singularly valuable decomposition\" \n",
    ">--Dan Kalman \n",
    "\n",
    "> \"Singular Value Decomposition. Love it, learn it.\"\n",
    ">--Michael Basial\n",
    "\n",
    "> \"SVD? Magic, simply magic.\"\n",
    ">--John Doherty\n",
    "\n",
    " As we've said, the key to representative environmental models is allowing high levels of flexibility through a highly parameterized approach.  But this makes our parameter estimation problem ill-posed and underdetermined, which means our solution is nonunique even if we overcome problems of increased instability and longer runtimes.  Here we use a \"regularized inversion\" approach to overcome these problems.  Regularization is anything that makes an intractable problem solvable; for example, using a small number of zones (not highly-parameterized) is a way to regularize an ill-posed problem.  Regularization as we use here can be grouped into two broad categories: \n",
    " 1. adding soft-knowledge to the problem (Tikhonov regularization) and \n",
    " 2. mathematically reducing the dimensionality of the model (subspace regularization via singular value decomposition, or \"SVD\"). In practice we typically use a combination (\"hybrid\") of these two approaches. \n",
    "\n",
    " It is worth expounding on this difference in regularization approaches. In contrast to Tikhonov regularization, which adds information to the calibration process to achieve numerical stability, subspace methods achieve stability through subtracting parameters, and/or parameter combinations, from the calibration process (making a \"subspace\" of the full parameter space). Now the calibration process is no longer required to estimate either individual parameters or combinations of correlated parameters that are inestimable given the calibration dataset we have. What combinations are estimable are automatically determined through SVD. \n",
    "\n",
    " The effort needed to take advantage of these regularization strategies is also appreciably different, where SVD is relatively easily brought to bear and becomes \"set it and forget it\". Moreover, when SVD is used the parameter estimation problem always becomes __unconditionally stable__! Neither of these is true in all cases when adding soft knowledge using Tikhonov regularization. \n",
    "\n",
    " In summary, SVD benefits apply to all models so it is worth widely invoking when using PEST and PEST++.  SVD involves few parameter estimation inputs, default values work for a large range of problems, and it addresses instability for all problems. Can you catch that we can't overemphasize the importance of SVD to parameter estimation?  \"Magic\" indeed! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this and the next notebooks we'll get under the hood of SVD and see what it does.  A high-level understanding is not needed to take advantage of the power of SVD for your typical calibration parameter estimation problem (\"set it and forget it\").  BUT in addition to the glow of knowledge that they impart, these SVD concepts will cascade into understanding other tools such as parameter identifiability, calculation of uncertainty, and null-space Monte Carlo.  \n",
    "\n",
    "> #### We highly recommend going through:\n",
    "> - Gregory Gunderson's [Singular Value Decomposition as Simply as Possible](https://gregorygundersen.com/blog/2018/12/10/svd/#:~:text=The%20singular%20values%20referred%20to,our%20transformation%20flattens%20our%20square.). An excellent place to start to gain an intuitive understanding of SVD. \n",
    "> -  Frank Cleary's [introduction to SVD notebook](https://gist.github.com/frankcleary/a89da479d85c98f86e31)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "Linear Algebra is the foundation of much of our maths and modeling. At the basis of this is matrices, which are containing vector information like spatial array of properties, mappings from one set of properties to another, the variability of properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Another example of a matrix is just a photograph. It turns out, much of the information contained in a matrix is redundant. If we think of the columns of a matrix as vectors, they are orthogonal but maybe aren't quite the right basis for the infromation. What if we could find another basis, where we rotate to a more suitable set of orthogonal basis vectors and maybe even stretch them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any matrix can be decomposed into 3 matrices:\n",
    "$$\\mathbf{M}=\\mathbf{U}\\mathbf{S}\\mathbf{V}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "sys.path.append(\"..\")\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# sys.path.insert(0,os.path.join(\"..\", \"..\", \"dependencies\"))\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm     \n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "import numpy as np\n",
    "import shutil \n",
    "\n",
    "noPIL=False\n",
    "try:\n",
    "    from PIL import Image\n",
    "except:\n",
    "    noPIL=True\n",
    "\n",
    "plt.rcParams['font.size'] = 10\n",
    "pyemu.plot_utils.font =10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Image is also a Matrix\n",
    "Let's evaluate how this works by exploring the information content in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo = Image.open('clands.JPG')\n",
    "\n",
    "plt.imshow(photo, interpolation='nearest')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to grayscale\n",
    "\n",
    "By converting to grayscale, what we are left with is a matrix of information where each pixel (e.g. a cell in rows/columns of the matrix) has a value between 0 and 255 indicating intensity. This is then just a matrix with information in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not noPIL:\n",
    "    photogray = np.array(photo.convert('L'))\n",
    "    np.savetxt('clands_gray.dat', photogray, fmt='%d')\n",
    "else:\n",
    "    photogray = np.loadtxt('clands_gray.dat', dtype=int)\n",
    "plt.imshow(photogray, interpolation='nearest', cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat this like any matrix and perform SVD. In python, `numpy` makes this easy. (_Go through Frank Clearly's [notebooks](https://gist.github.com/frankcleary/a89da479d85c98f86e31) for details on the maths behind all of this._)\n",
    "\n",
    "In the next cell we use `numpy` to  decompose the `photogray` matrix using the equiation shown earlier: $\\mathbf{M}=\\mathbf{U}\\mathbf{S}\\mathbf{V}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, sigma, V = np.linalg.svd(photogray,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sigma` is a 1D array that contains the singular values of `photogray`. $\\mathbf{S}$ = `np.diag(sigma)`. Singular values provide a measure of the \"amount of information\" in each vector. So, the first column of $\\mathbf{U}$ and row of $\\mathbf{V}^T$ contain the most information, the second the second most, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigma)\n",
    "plt.grid()\n",
    "plt.title('{0} Singular values in descending order'.format(len(sigma)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same thing on a $Log_{10}$ scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigma)\n",
    "plt.grid()\n",
    "plt.title('{0} Singular values in descending order'.format(len(sigma)));\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a little function for using a subset of left-singular vectors (i.e. columns from $\\mathbf{U}$) to reconstitute the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_image(U,sigma,V,numsing=1, photo=None, printflag=False):\n",
    "    #reconimg = USV\n",
    "    reconimg = np.dot(np.dot(U[:,:numsing], np.diag(sigma[:numsing])),V[:numsing,:])\n",
    "    basis_vec = np.dot(np.dot(np.atleast_2d(U[:,numsing-1]).T, sigma[numsing-1]),np.atleast_2d(V[numsing-1,:]))\n",
    "    fig,ax = plt.subplots(ncols=2, figsize=(12,12))\n",
    "    ax[0].imshow(basis_vec, interpolation='nearest', cmap='gray')\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Basis Image')\n",
    "    ax[1].imshow(reconimg, interpolation='nearest', cmap='gray')\n",
    "    ax[1].axis('off')\n",
    "    ss = 's'\n",
    "    if numsing==1:\n",
    "        ss = ''\n",
    "    ax[1].set_title('Reconstruction using {0} singular value{1}'.format(numsing,ss))\n",
    "    plt.tight_layout()\n",
    "    if printflag==True:\n",
    "        plt.savefig(os.path.join('pngs','svd_{0}.png'.format(numsing)), bbox_inches='tight', pad_inches=0.2)\n",
    "        plt.close()\n",
    "    return basis_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this function is doing is computing an approximation of the image (the matrix) using `numsing` number of vectors of the singular value decomposition. Recall from above that we have a maximum of 351 singular values.\n",
    "\n",
    "Computing an approximation of the image using the first column of $\\mathbf{U}$ (and weighted according to the first row of $\\mathbf{V}^T$) reproduces the most prominent features of the image. The left panel shows the unique information contributed by the current singular value (and associated vectors), and the right panel shows the combination of all the singular values up to `numsing` used to reconstruct the image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec=recon_image(U,sigma,V,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how many are \"enough\"? Check out the plots of singular values above. At what _number of singular values_ (x-axis) do the _singular values_ (y-axis) start to flatten out? Somewhere around 25? This  suggests that all the “action” of the matrix happens along only these few dimensions. So we should be able to reconstruct a decent approximation with only 25 pieces of information, instead of the total 350:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec=recon_image(U,sigma,V,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey that's pretty good! And with a fraction of the total information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does this have to do with groundwater modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cook up a Jacobian Matrix. You should be familiar with this process now. We are going to re-construct the Freyberg pilot point PEST setup. Then run PEST++GLM once, with NOPMAX set to -1 to calculate a Jacobian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd\n",
    "\n",
    "# folder containing original model files\n",
    "org_d = os.path.join('..', '..', 'models', 'monthly_model_files_1lyr_newstress')\n",
    "# a dir to hold a copy of the org model files\n",
    "tmp_d = os.path.join('freyberg_mf6')\n",
    "if os.path.exists(tmp_d):\n",
    "    shutil.rmtree(tmp_d)\n",
    "shutil.copytree(org_d,tmp_d)\n",
    "# get executables\n",
    "hbd.prep_bins(tmp_d)\n",
    "# get dependency folders\n",
    "hbd.prep_deps(tmp_d)\n",
    "# run our convenience functions to prepare the PEST and model folder\n",
    "hbd.prep_pest(tmp_d)\n",
    "# convenience function that builds a new control file with pilot point parameters for hk\n",
    "hbd.add_ppoints(tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run PEST++GLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(tmp_d,'freyberg_pp.pst'))\n",
    "pst.control_data.noptmax = -1\n",
    "pst.write(os.path.join(tmp_d, 'freyberg_pp.pst'))\n",
    "m_d = 'master_pp'\n",
    "num_workers=6\n",
    "pyemu.os_utils.start_workers(tmp_d, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-glm', #the PEST software version we want to run\n",
    "                            'freyberg_pp.pst', # the control file to use with PEST\n",
    "                            num_workers=num_workers, #how many agents to deploy\n",
    "                            worker_root='.', #where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, #the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form up the normal equations matrix\n",
    "\n",
    "This matrix is $\\mathbf{X}^\\top\\mathbf{QX}$. The Jacobian matrix contains values for $\\mathbf{X}$. The observation noise is represented by $\\mathbf{Q}$.\n",
    "\n",
    "Let's get to our Jacobian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injac = pyemu.Jco.from_binary(os.path.join(m_d, 'freyberg_pp.jcb'))\n",
    "\n",
    "X = injac.df().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Jacobian matrix is...a matrix! Rows are observations, columns are parameters. The plot below displays parameter sensitivities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(12,7))\n",
    "plt.imshow(np.log10(abs(X )))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get our matrix of observation noise from the weights in the PEST control file (this assumes that observation weights are the inverse of observation noise and that noise is independent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpst = pyemu.Pst(os.path.join(m_d,'freyberg_pp.pst'))\n",
    "Q = inpst.observation_data.weight.values\n",
    "Q = np.diag(Q)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(15,15))\n",
    "plt.imshow(Q, interpolation='nearest', cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to SVD\n",
    "\n",
    "We can now undertake SVD on the normal matrix $\\mathbf{X}^\\top\\mathbf{QX}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtQX=X.T.dot(Q).dot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did above for the photograph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, sigma, V = np.linalg.svd(XtQX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the singular values (note the log-10 scale on the y-axis). Note that the total number of singular values matches the number of weighted observations in the PEST control file. Why? Because that is the total \"amount\" of information we have available to inform parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigma)\n",
    "plt.yscale('log')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get fancy, you can explore which parameters inform which singular value vector. The interactive plot below allows you to select the singular vect, and plots the parameter contributions to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbd.interactive_sv_vec_plot(inpst, U);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great - finally how does this impact our calibration of a K-field?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below pulls in the \"true\" hydraulic conductivity from our Freyberg model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwf = hbd.plot_truth_k(m_d);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now reconstruct the K field\n",
    "\n",
    "Let's see if we can reconstruct this field.\n",
    "\n",
    "The next cell implements a function similar to what we did for the image at the start of this notebook and plots the outcomes.\n",
    "\n",
    "Play with the slider in the figure below. As you move it to the right, more singular values are employed. After a certain number of SVs, the reconstructed parameter field starts to become similar to the true field. Magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbd.svd_enchilada(gwf, m_d);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
