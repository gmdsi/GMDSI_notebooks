{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing the Ill-Posed Inverse Problem \n",
    "This tutorial picks up following the \"Freyberg pilot points\" tutorial in which we added a lot more adjustable parameters. We saw that this allowed us to get an excellent fit with measured data (too good!) but resulted in unrealistic parameter fields. Not to mention that we still underepresented forecast uncertainty.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "We have more unkown parameters than observations. Thus, we have an ill-posed inverse problem. The mathematical term for the process through which a unique solution is sought for a nonunique inverse problem is “regularization”. The goal of regularised inversion is to seek a unique parameter field that results in a suitable fit between model outputs and field measurements, whilst minimizing the potential for wrongness in model predictions. That is, out of all the ways to fit a calibration dataset, regularized inversion seeks the parameter set of minimum error variance.\n",
    "\n",
    "There are two main approaches used to stabilize this problem: \n",
    " 1) adding soft knowledge and/or \n",
    " 2) reducing the problem dimensionality. \n",
    "\n",
    "These methods can be used by themselves, but are most commonly applied together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tikhonov Regularization\n",
    "We have moved up the simplicity-complexity curve because now we have many more parameters, even more than our observations.  But have we passed the sweet spot? One way to seek a parameter field of minimum error variance is to seek a parameter field that allows the model to fit the calibration dataset, but whose values are also as close as possible to a set of “preferred parameter values”.\n",
    "\n",
    "<img src=\"intro_to_regularization_files/Hunt1998_sweetspot.png\" style=\"float: center; width:300px;\">\n",
    "\n",
    "Again, it is not simply the number of parameters that is at issue.  The better way to think of it is that we just want to avoid \"living beyond our means\". That is, we do not bring more parameters to bear than we have the ability to constrain.  We can constrain parameters using observations as we have seen so far, but we also know things about the system that are not hard data like measurements. This \"soft knowledge\" can also be applied to constrain our parameters through a mechanism called __\"Tikhonov regularization\"__.  In this formulation of the inverse problem, we add a second term to our \"best fit\" metric Phi. This second term reflects the deviation from our soft-knowledge of the system, and is a penalty to our fit. Here's how it looks using the [Anderson et al. (2015)](https://www.sciencedirect.com/book/9780120581030/applied-groundwater-modeling) formulation:\n",
    "\n",
    "<img src=\"intro_to_regularization_files/tik-reg_eq9.8.png\" style=\"float: center; width:700px;\">\n",
    "\n",
    "The first term after the equals sign is our __measurement objective function__, which is what we've been working with so far.  The last term on the right is called the __\"regularization objective function\"__. These 2 terms combine to create a __total Phi__ on the left.  __This total Phi is what we minimize__, which means we are minimizing our observed-to-simulated residuals __AND__ the deviation from soft knowledge. So in this way Tikhonov regularization is a \"dual-constrained minimization\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrey Tikhonov\n",
    "\n",
    "<img src=\"intro_to_regularization_files/Tychonoff.jpg\" style=\"float: center; width:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Anderson et al. (2015)](https://www.sciencedirect.com/book/9780120581030/applied-groundwater-modeling) looks a little closer at this in equation 9.9:\n",
    "\n",
    "\n",
    "<img src=\"intro_to_regularization_files/tik-reg_eq9.9.png\" style=\"float: center; width:700px;\">\n",
    "\n",
    "The first term to the right of the equals sign is the measurement objective function from\n",
    "Eqn (9.6), which is calculated as the sum of squared weighted residuals, where $n$ residuals\n",
    "$r_i$ are calculated from hard knowledge and $w_i$ are their respective weights. The second\n",
    "term quantifies the penalty resulting from deviations from soft knowledge as the sum\n",
    "of $q$ deviations from soft-knowledge conditions $f_j$, where $f_j$ is a function of model parameters\n",
    "$p$. A calibrated model, therefore, is found by minimizing both the measurement\n",
    "objective function (hard data) and the soft knowledge penalty.\n",
    "\n",
    "> __Take-home point #1 from these equations:__\n",
    "> When Tikhonov regularization is set up correctly, PEST(++) should only deviate from the preferred condition when there is a sufficient improvement in our fit to the observations (i.e. the measurement objective function).  \n",
    ">\n",
    "> __Take-home point #2 from these equations:__  \n",
    "> The two contributors to our total Phi are __*carried separately in the parameter estimation*__.  \n",
    ">\n",
    "This is important. It allows us to control the balance between fitting measured data and respecting expert knowledge. This is how we avoid __overfitting__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Information\n",
    "\n",
    "How do we express soft knowledge quantitatively so we can minimize it?\n",
    "\n",
    "PEST(++) provides a user with a great deal of flexibility in how Tikhonov constraints can be introduced to an inversion process. The easiest way to do this is through the use of prior information equations. When prior information equations are employed, Tikhonov constraints are expressed through preferred values that are assigned to linear relationships between parameters. (Equality is the simplest type of linear relationship.) Weights must be assigned to these equations. As described in the PEST documentation, when PEST is run in “regularization” mode it makes internal adjustments to the weights that are assigned to any observations or prior information equations that belong to special observation groups referred to as “regularization groups”. \n",
    "\n",
    "We add preferred conditions.  These are typically:\n",
    "\n",
    "1. *preferred value* - \"I believe this horizontal conductivity parameter $K_h$ is around 1 m/d.\"\n",
    "2. *preferred difference* - \"I believe this area has a horizontal conductivity $K_h$ 10 m/d higher than that area.\" \n",
    "\n",
    "One of the most useful preferred conditions for collapsing all these parameters to fewer bins is a special case of preferred difference where the difference = 0.  This is often called __\"preferred homogeneity\"__, which equates to something along the lines of \"I believe this area has homogeneous $K_h$\".\n",
    "\n",
    "__Preferred value__ is the easiest to implement and least memory-intensive preferred condition. `pyemu` implements this with functionality called \"*__zero_order_tikhonov__*\". (Make sure the initial values in the control file represent your soft knowledge!) `pyemu` also has preferred difference available: look for *\"__first_order_pearson_tikhonov__\"*.  We'll see both of these in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pilot point regularization can be propogated to other pilot points, or not.\n",
    "\n",
    "Here are two examples from [Anderson et al. (2015)](https://www.sciencedirect.com/book/9780120581030/applied-groundwater-modeling).  For \"preferred value\" __(below (a), left)__ there is no cross-talk between pilot points.  The initial parameter value of each pilot point is the preferred value.  For preferred difference __(below (a), right)__, there is a radius of influence that connects the pilot point regularization (think correlation length from geostatistics).  \n",
    "\n",
    "\n",
    "<img src=\"intro_to_regularization_files/Fig9.15a_Muffles_pp.png\" style=\"float: center; width:700px;\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Likewise, pilot-point regularization can also be grouped or limited to specific areas.  \n",
    "\n",
    "For example, if the geology of a site suggests distinct units you can just apply the preferred difference to each zone separately:\n",
    "\n",
    "<img src=\"intro_to_regularization_files/Fig9.15b_Kyle_Larry_pp.png\" style=\"float: center; width:800px;\">\n",
    "\n",
    "\n",
    "Here's the caption from [Anderson et al. (2015)](https://www.sciencedirect.com/book/9780120581030/applied-groundwater-modeling) for posterity:  Figure 9.15 Pilot Points. (a) Network of pilot points in a watershed-scale groundwater flow model (left); linkages between pilot points (right) used to calculate Tikhonov regularization constraints for preferred homogeneity (modified from Muffels, 2008). (b) Network of pilot points used to represent two hydraulic conductivity zones where Tikhonov regularization is applied to pilot points within the same zone (modified from Davis and Putnam, 2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But there is more to think about:\n",
    "\n",
    "Just like our observations, our preferred conditions are given a weight.  Typically it is uniform (usually 1) - this is what the PEST utility `addreg1` does. On top of this, typically we have the regularization objective function set up to adjust the weights of the different parameter groups during the course of the parameter estimation (`IREGADJ` variable = 1 in the PEST control file).  See pages 17, 20, and page 34 of [SIR 2010-5169](https:/pubs.usgs.gov/sir/2010/5169/). \n",
    "\n",
    "But, __and this is critical__, these typically end up having somewhat subtle effects; the final say in trade-off between the measurement objective function and the regularization objective function is a *user-specified variable* in the PEST control file called:\n",
    "\n",
    "__<center>PHIMLIM</center>__\n",
    "\n",
    "Many people missed the importance of this variable in the original Doherty (2003) paper that first showed PEST's pilot points and Tikhonov capabilities. This missed importance was addressed in detail in Fienen et al. (2009).  So, for you to do good modeling with these approaches it is critically important that you take this away, so we will state it again in bigger font:  \n",
    "\n",
    "The final say in trade-off between the measurement objective function and the regularization objective function is a *user-specified variable* in the PEST control file called:\n",
    "\n",
    "__<center><span style=font-size:1.5em>PHIMLIM</span></center>__\n",
    "\n",
    "`PHIMLIM` is the \"Target Measurement Objective Function\", which means rather than finding the best fit to the observations, PEST will hit this new `PHIMLIM` level and  *then find the minimum of the regularization objective function* (find the parameters that most closely match the preferred conditions while still keeping the `PHIMLIM` target measurement objective function). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to think of this is that `PHIMLIM` controls the trade-off between the two parts of the righthand side of the equal sign in equation 9.8 above. We can plot this tradeoff as a Pareto front between adhering to soft knowledge (regularization objective function) and getting a better fit (measurement objective function). Here is what a Pareto front looks like for this problem:\n",
    "\n",
    "\n",
    "<img src=\"intro_to_regularization_files/Fig9.17_fit_vs_softknowledge_Pareto.png\" style=\"float: center; width:700px;\">\n",
    "\n",
    "\n",
    "A __key point__ is that many points on this curve could be considered a \"calibrated model\" (i.e. a good fit and reasonable parameters). Which of these possible \"calibrated models\" we choose is based on professional judgement. Subjective, we know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final point:  \n",
    "Here's how `PHIMLIM` affects the optimal parameters:\n",
    "\n",
    "<img src=\"intro_to_regularization_files/Fig9.16_PHIMLIM.png\" style=\"float: center; width:750px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So setting `PHIMLIM` is our primary way to control the degree of fitting, and keep us from *overfitting*.\n",
    "\n",
    "The suggested workflow is to:\n",
    "\n",
    "1) Set `PHIMLIM` very low (e.g., 1.0) and run the parameter estimation. This throws away the soft knowledge and finds the best fit to the observations (minimizes the measurement objective function).  \n",
    "\n",
    "2) Set `PHIMLIM` to something like __10% higher__ than this lowest Phi. Rerun the parameter estimation, evaluate if the parameters are too extreme. If they are, raise `PHIMLIM` again.\n",
    "\n",
    "We'll use this workflow on our pilot point version of Freyberg later. But first, let's talk a little more about the theory and implementation of *prior information* in the PEST datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Singular Value Decomposition\n",
    "\n",
    " > See the \"intro to svd\" notebook for an introduction to Singular Value Decomposition.\n",
    "\n",
    "Tikhonov regularization adds information to the calibration process in order to achieve numerical stability. In contrast, subspace methods achive numerical stability by reducing the dimensionality of the problem, removing and/or combining parameters. When employing SVD in calibration, only parameters and linear combinations of parameters that are sufficiently constrained by measured data are estimated. These parameters are said to reside in the *solution space*. Choosing which parameter combinations to estimate is accomplished via singular value decomposition (SVD). SVD-based parameter estimation fixes intial values for parameters and parameter combinations that are not estimable (reside in the *null space*) and does not adjust them during inversion. (So, once again, make sure initial parameter values are sensible!)  \n",
    "\n",
    "Unlike PEST and PEST_HP, members of the PEST++ suite employ singular value decomposition (or methods closely related to it) **by default** for solution of the inverse problem. Unless otherwise specifed, default options are employed. PEST++GLM offers two numerical libraries for implementing SVD; the default option will usually suffice (see the PEST++ user manual for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to Freyberg\n",
    "\n",
    "Now that we have gone over a bit of theory, let's apply it to calibrating the Freyberg model. We pick up after the \"freyberg pilot point run\" notebook. In that tutorial, we saw that adding many more parameters made it easier to obtaining a good fit with measured data. Unfortunately, we also saw that this led to \"living beyond our means\". We have many more parameters than observations (ill-posed inverse problem), which resulted in overfitting. Parameter values took on \"unrealistic\" values to compensate for structural inadequacies of the model (and parameterization!).\n",
    "\n",
    "Let's see if we can fix this problem with regularization.\n",
    "\n",
    "### Admin\n",
    "Load the Freyberg model and setup up the PEST control file. The next two cells construct the same setups employed during previous tutorials on pilot points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "import psutil\n",
    "import shutil\n",
    "\n",
    "# sys.path.insert(0,os.path.join(\"..\", \"..\", \"dependencies\"))\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd\n",
    "\n",
    "plt.rcParams['font.size'] = 10\n",
    "pyemu.plot_utils.font =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder containing original model files\n",
    "org_d = os.path.join('..', '..', 'models', 'monthly_model_files_1lyr_newstress')\n",
    "# a dir to hold a copy of the org model files\n",
    "tmp_d = os.path.join('freyberg_mf6')\n",
    "if os.path.exists(tmp_d):\n",
    "    shutil.rmtree(tmp_d)\n",
    "shutil.copytree(org_d,tmp_d)\n",
    "# get executables\n",
    "hbd.prep_bins(tmp_d)\n",
    "# get dependency folders\n",
    "hbd.prep_deps(tmp_d)\n",
    "# run our convenience functions to prepare the PEST and model folder\n",
    "hbd.prep_pest(tmp_d)\n",
    "# convenience function that builds a new control file with pilot point parameters for hk\n",
    "hbd.add_ppoints(tmp_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-constructed pst\n",
    "pst = pyemu.Pst(os.path.join(tmp_d,'freyberg_pp.pst'))\n",
    "# run it once for checking\n",
    "pst.control_data.noptmax=0\n",
    "pst.write(os.path.join(tmp_d,'freyberg_pp.pst'))\n",
    "pyemu.os_utils.run('pestpp-glm freyberg_pp.pst', cwd=tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tikhnov regularization as \"prior information\"\n",
    "\n",
    "In pyemu, we can add two forms of regularization:\n",
    "- preferred __value__: we want the parameter values to stay as close to the initial values as possible\n",
    "- preferred __difference__: we prefer the differences in parameter values to be minimized\n",
    "\n",
    "Preferred value is easy to understand, we simply add ``prior_information`` to the control file to enforce this condition.  `pyemu` uses a helper for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(tmp_d,'freyberg_pp.pst'))\n",
    "# check if it ran\n",
    "assert pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the `prior_information` section of the control file like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.prior_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, it is empty. Let's add preferred value regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pyemu helper to apply preferred value regularization on all the parameters\n",
    "pyemu.helpers.zero_order_tikhonov(pst,parbounds=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table of the regularization equations \n",
    "pst.prior_information.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the \"regul_\" prefix added to the parameter group name (check the `obgnme` column) - that is how we tell PEST to track the deviations from preferred conditions separately as a Tikhonov regularization.\n",
    "\n",
    "Ok, that's fine, but should the weight on preferring a hydraulic conductivity (`hk`) value be the same as preferring recharge (`rch`) not to change? Hydraulic conductivity is typically considered to be \"known\" within an order of magnitude; the uncertainty in recharge is typically considered less than that - say plus or minus 50%. Seems like we would want `rch` to change less than `hk`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization weights from parameter prior uncertainty\n",
    "\n",
    "There is a neat trick that `pyemu` gives us: this strength of the preferred value can be inferred from the parameter bounds you specify.  That is, the bounds are used to form the regularization weights; larger bounds = more uncertainty = less weight given to maintaining the initial value during the parameter estimation.  \n",
    "\n",
    "Weight is calculated assuming that the range between parameter bounds reflects 4 standard deviations of the (gaussian) prior parameter probability distribution. Assigned weights are equal to the inverse of standard deviation ($\\sigma$):\n",
    "\n",
    "$$ \\sigma = \\frac{\\textrm{upper bound} - \\textrm{lower bound}}{4}$$\n",
    "\n",
    "$$ weight = \\frac{1}{\\sigma} $$\n",
    "\n",
    "\n",
    "> __Important__. This assumes that the parameter bounds reflect prior parameter uncertainty. So not only should the initial parameter value be assigned carefully, but so should the parameter bounds!\n",
    "\n",
    "Let's try this again using the parameter bounds to specify prior information weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct preferred value reguarization equations and use the bounds to calculate the regularization weight\n",
    "pyemu.helpers.zero_order_tikhonov(pst, parbounds=True)\n",
    "# print out the regularization equations that were constructed\n",
    "pst.prior_information.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are given more strength for keeping recharge near its initial value...good! This way, all else being equal, PEST(++) will \"prefer\" to change `hk` before it changes `rch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what about preferred difference regularization?  \n",
    "\n",
    "Well `pyemu` can do that too.  \n",
    "\n",
    "The simplest form of \"preferred difference\" is homogeneity. We prefer that the \"difference between parameters should equal zero\". This is referred to as \"first-order pearson tikhonov\". But how do we assign weights? How confident are we that this a nearby parameter should have the same value? What about another parameter which is a bit farther away? Geostatistics, that's how.\n",
    "\n",
    "Remember that Covariance matrix `cov` we keep talking about? It expresses the spatial relationship between pilot points (implied by the variogram), so we use it to setup these prior information equations. First we need to make a geostatistical structure to encapsulate the spatial relationships.\n",
    "\n",
    "As we have seen in other tutorials (see the \"intro to geostatistics\", \"intro to pyemu\" and \"freyberg pilot point setup\" notebooks), `pyemu` can prepare a covariance matrix for spatially distributed parameters, based on a geostatistical structure and variogram(s) that reflect our expert knowledge of how we expect parameters to vary. \n",
    "\n",
    "For example, construct a variogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(contribution=1.0, a=2500.0)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=v,nugget=0.0)\n",
    "ax = gs.plot()\n",
    "ax.grid()\n",
    "ax.set_ylim(0,1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to know where the pilot points are.  For this case, we can get this from the pilot point template file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe called df_pp using pyemu helper and the pilot point template file hkpp.dat.tpl\n",
    "df_pp = pyemu.pp_utils.pp_tpl_to_dataframe(os.path.join(tmp_d,\"hkpp.dat.tpl\"))\n",
    "df_pp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a covariance matrix for our pilot point parameters from the geostatistical structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a covariance matrix called cov using pyemu's geostatistics capabilities\n",
    "cov = gs.covariance_matrix(df_pp.x, df_pp.y, df_pp.parnme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a peek at the covariance matrix we just constructed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.to_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now a simple matter of passing this matrix to `pyemu.helpers.first_order_pearson_tikhonov()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pyemu helper to construct preferred difference regularization equations \n",
    "# using the covariance for regularization weight\n",
    "pyemu.helpers.first_order_pearson_tikhonov(pst, cov, reset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the new `prior_information` equations. Note how they now reflect the _difference_ between two parameters should equal zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the new regularization equations out\n",
    "pst.prior_information.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?  \n",
    "\n",
    "We replace the preferred value equations with a bunch of new equations.  These equations each include two parameter names and have different weights - can you guess what the weights are?  The weights are the *pearson correlation coefficients* (CC) between the pilot points (remember those from way back?).  These CC values are calculated from the covariance matrix, which is implied by the geostatistical structure...whew! For example, ``hk_i:2_j:2_zone:1.0`` is \"close\" to ``hk_i:2_j:7_zone:1.0``, so they have a high CC value (equation 1).  Just for fun, go back and change the `a` parameter in the variogram and see how it changes the CC values.\n",
    "\n",
    "> __PRO TIP__:  you can use both preferred value and preferred difference regularization in the same PEST control file, and even on the same parameter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Kahuna - PHIMLIM\n",
    "\n",
    "And, most importantly we need to see what pyemu has for the all-important regularization BIG KNOB, the target objective function - or `phimlim`.  This is THE ONE INPUT that tells PEST how regularization is enforced.  So let's use pyemu to see what our control file has for `phimlim`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the PEST control file defined by pst above, echo out phimlim\n",
    "pst.reg_data.phimlim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is __very low__  for a final parameter estimation.  But recall our workflow from earlier on, this value is just a \"placeholder\" to ignore soft knowledge and only focus on obtaining the best fit.  After the how-low-can-PEST-go run, ``phimlim`` should be set to a larger number, say the number of nonzero-weighted observations (assuming that observation weights are the inverse of standard deviation of noise!).  Here we'll explore the effect of `phimlim` a bit.  \n",
    "\n",
    "Now, we have already undertaken an non-regularized parameter estimation run. Effectively that is what we did in the \"freyberg pilot point run\" notebook. We got a really good fit - much better than is reasonable given the uncertainty (i.e. noise) in our observation data (check the previous notebook to get the value of Phi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many non-zero weighted observations do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnz_obs = pst.nnz_obs\n",
    "nnz_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenience of weighting with the inverse of the measurement uncertainty (which we have done in this case) is that it is easy to know what the ideal Phi should be: it should be equal to the number of nonzero-weighted observations. \n",
    "\n",
    "This of course assumes that all model-to-measurement misfit is due to *measurement* uncertainty. In practice, model error usualy plays a larger role, as we will see in other tutorials. \n",
    "\n",
    "So, let's assign `PHIMLIM` equal to he number of nonzero-weighted observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.reg_data.phimlim = nnz_obs\n",
    "pst.reg_data.phimlim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When changing PHIMLIM, we must also change `PHIMACCEPT`. This is the upper limit of acceptable \"measurement objective function\" values. If PEST(++) achieves `PHIMLIM`, it will then attempt to minimize the _regularization_ objective function, while maintaining the _measurement_ objective function below `PHIMACCEPT`.\n",
    "\n",
    "Ideally, `PHIMACCEPT` should be assigned 5-10% higher than PHIMLIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when phimlim changes so should phimaccept, and is usually 5-10% higher than phimlim\n",
    "pst.reg_data.phimaccept = 1.1 * pst.reg_data.phimlim\n",
    "pst.reg_data.phimaccept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last important detail! Make sure we set the `pestmode` to regularization. This should have happened by default when we used the `pyemu.helpers` to assign prior information equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pst.control_data.pestmode == 'regularization'\n",
    "pst.control_data.pestmode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write and Run\n",
    "\n",
    "Sheesh - this was a long one. Hope you are still with us. Now let's rewrite the PEST control file and run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove some PEST++ optional variables. (We haven't mentioned these before; they control PEST++GLM SVD-Assist options. We won't discuss these here. See tutorials on PEST++GLM in Part 2 of these tutorial notebooks for more information.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options.pop('n_iter_base')\n",
    "pst.pestpp_options.pop('n_iter_super')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the control file again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(tmp_d, 'freyberg_reg.pst'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to specify the number which is adequate for ***your*** machine when assigning a value to the following `num_workers` variable. (If you are unsure how many cores you have, you can use `psutil` to check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psutil.cpu_count(logical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of parallel agents\n",
    "num_workers = 10\n",
    "# set the master directory\n",
    "m_d='master_reg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(tmp_d, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-glm', # the PEST software version we want to run\n",
    "                            'freyberg_reg.pst', # the control file to use with PEST\n",
    "                            num_workers=num_workers, # how many agents to deploy\n",
    "                            worker_root='.', # where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, # the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcomes\n",
    "\n",
    "Reload the control file and check the new Phi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d, 'freyberg_reg.pst'))\n",
    "pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That number looks familiar...what did we assing to PHIMACCEPT again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.reg_data.phimaccept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the Phi progress. Interesting. In this case PEST managed to get a fit better than `PHIMLIM` in the very first iteration (not an usual case). Measurement Phi then increases up to `PHIMACCEPT`. Subsequent iterations were spent reducing regularization Phi until total Phi is lower than PHIMLIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obj = pd.read_csv(os.path.join(m_d,\"freyberg_reg.iobj\"),index_col=0)\n",
    "df_obj.plot(y=['total_phi','measurement_phi','regularization_phi'])\n",
    "plt.ylabel('total_phi');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the fits with measured values? Not perfect...but we aren't looking for a perfect fit here - we are looking for _as good a fit_ as the measured data justifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = pst.plot(kind=\"1to1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the posterior parameter uncertainties for each parameter group. \n",
    "\n",
    "The next cell plots the probability distribution for each parameter in each parameter group. Recall that each pilot point is assigned a unique parameter, so in each plot we are displaying multiple distributions. We are also plotting the parameter upper and lower bounds as vertical dashed black lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_unc():\n",
    "    par = pst.parameter_data\n",
    "    df_paru = pd.read_csv(os.path.join(m_d,\"freyberg_reg.par.usum.csv\"),index_col=0)\n",
    "\n",
    "    fig, axes=plt.subplots(1,len(pst.adj_par_groups),figsize=(12,5))\n",
    "    i=0\n",
    "    for pargp in pst.adj_par_groups:\n",
    "        ax = axes[i]\n",
    "        i+=1\n",
    "        hk_pars = [p for p in pst.par_names if p.startswith(\"hk\")]\n",
    "        pars = par.loc[par.pargp==pargp].parnme.values\n",
    "        df_par = df_paru.loc[pars,:]\n",
    "        ax = pyemu.plot_utils.plot_summary_distributions(df_par,label_post=False, ax=ax)\n",
    "        mn = np.log10(pst.parameter_data.loc[pars[0].lower(),\"parlbnd\"])\n",
    "        mx = np.log10(pst.parameter_data.loc[pars[0].lower(),\"parubnd\"])\n",
    "        ax.set_title(pargp)\n",
    "        ax.plot([mn,mn],ax.get_ylim(),\"k--\")\n",
    "        ax.plot([mx,mx],ax.get_ylim(),\"k--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_unc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot better than before! We are no longer getting lots of parameters at their bounds. Yay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the calibrated parameter K field. First we just need to update the model files with the calibrated parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parrep(os.path.join(m_d, \"freyberg_reg.par\" ))\n",
    "pst.write_input_files(pst_path=m_d)\n",
    "pyemu.geostats.fac2real(os.path.join(m_d,\"hkpp.dat\"),\n",
    "                        factors_file=os.path.join(m_d,\"hkpp.dat.fac\"),\n",
    "                        out_file=os.path.join(m_d,\"freyberg6.npf_k_layer1.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run('python forward_run.py', cwd=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then use `flopy` to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp = pyemu.pp_utils.pp_tpl_to_dataframe(os.path.join(m_d,\"hkpp.dat.tpl\"))\n",
    "sim = flopy.mf6.MFSimulation.load(sim_ws=m_d, verbosity_level=0) #modflow.Modflow.load(fs.MODEL_NAM,model_ws=working_dir,load_only=[])\n",
    "gwf= sim.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "mm = flopy.plot.PlotMapView(model=gwf, ax=ax, layer=0)\n",
    "\n",
    "k = gwf.npf.k.get_data()\n",
    "vmax=40\n",
    "vmin=2\n",
    "ca = mm.plot_array(np.log10(k), masked_values=[1e30],)#vmax=np.log10(vmax), vmin=np.log10(vmin))\n",
    "cb = plt.colorbar(ca, shrink=0.5)\n",
    "cb.ax.set_title('$Log_{10}K$')\n",
    "\n",
    "mm.plot_grid(alpha=0.5)\n",
    "mm.plot_inactive()\n",
    "ax.set_title('$K$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey! What's this?! nearly uniform K?? Oh right...that's what we told PEST we preferred. Therefore parameter values were left alone, unless needed to improve the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about recharge? Remember from the posterior parameter distributions plotted above, we see `rch` parameter values deviated from the prior the most.\n",
    "\n",
    "Plot recharge spatial distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwf.rch.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, although we specified that `hk` parameters were more uncertain than `rch`, information content in the observation data required changes in `rch`. Because `hk` and `rch` are correlated...only one is changed.\n",
    "\n",
    "Is this reasonable? This is where your expert knowledge comes in. If not, perhaps we need to back off on the fit a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as usual, we return to our forecasts. Have we done better this time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast_unc():\n",
    "    figs, axes = pyemu.plot_utils.plot_summary_distributions(os.path.join(m_d,\n",
    "                        \"freyberg_reg.pred.usum.csv\"),subplots=True, )\n",
    "    for ax in axes:\n",
    "        fname = ax.get_title().lower()\n",
    "        ylim = ax.get_ylim()\n",
    "        v = pst.observation_data.loc[fname,\"obsval\"]\n",
    "        ax.plot([v,v],ylim,\"b--\")\n",
    "        ax.set_ylim(0, ylim[-1])\n",
    "    fig.tight_layout()\n",
    "\n",
    "plot_forecast_unc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better...maybe? But still failing. How is this possible?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-to-Measurement Misfit is not just Measurement Noise\n",
    "\n",
    "We specified `PHIMLIM` to ensure that, at best, PEST would achieve a fit _as good_ as __measured__ data quality justified. This assumes that the model is actually able to achieve such a fit! \n",
    "\n",
    "But in practice, measurement noise is __only one__ component of model-to-measurement misfit. The other is __model error__. We are trying to get an ideal fit with an imperfect model. Perhaps a higher PHIMLIM would be a more reliable approach. Given the odd parameter distributions above, that makes sense.\n",
    "\n",
    "But what value to use... how will we know we have eliminated the overfitting problem? Well... in the real-world we will never know. In practice, this is where your professional judgment comes in. The modeller must decide what is a justifiable level of fit given the models' limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(tmp_d, 'freyberg_reg.pst'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.reg_data.phimlim = nnz_obs * 2\n",
    "# when phimlim changes so should phimaccept, and is usually 5-10% higher than phimlim\n",
    "pst.reg_data.phimaccept = 1.1 * pst.reg_data.phimlim\n",
    "#check values\n",
    "pst.reg_data.phimlim, pst.reg_data.phimaccept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run PEST again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 20\n",
    "pst.write(os.path.join(tmp_d, 'freyberg_reg.pst'))\n",
    "\n",
    "pyemu.os_utils.start_workers(tmp_d, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-glm', # the PEST software version we want to run\n",
    "                            'freyberg_reg.pst', # the control file to use with PEST\n",
    "                            num_workers=num_workers, # how many agents to deploy\n",
    "                            worker_root='.', # where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, # the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load it back in again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d, 'freyberg_reg.pst'))\n",
    "pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that parameters have not deviated as much from the intial values. Not surprising, since we set the PHIMLIM so high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_unc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the forecasts.\n",
    "\n",
    "There we go! For the first time we aren't (entirely) failing to capture the truth values in the posterior forecast distributions. (_Although, it is a bit tenuous...are we including enough adjustable parameters in our uncertainty analysis?_)\n",
    "\n",
    "So, what has happened here? Effectively we have traded a good fit with measured data for larger forecast uncertainty. So, even though we aren't fitting the observations as well, we are doing much better from a model forecast reliability stand point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast_unc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
