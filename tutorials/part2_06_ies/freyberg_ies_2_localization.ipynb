{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PESTPP-IES - Localization\n",
    "\n",
    "In the previous tutorial ([\"ies_1_basics\"](../part2_06_ies/freyberg_ies_1_basics.ipynb)) we introduced PESTPP-IES, demonstrated a rudimentary setup and explored some of the outcomes. In the current tutorial we are going to introduce \"localization\".\n",
    "\n",
    "As discussed in the previous tutorial, PESTPP-IES does not calculate derivatives using finite parameter differences. Instead, by running the model using each member of the parameter ensemble, it calculates approximate partial derivatives from cross-covariances between parameter values and model outputs that are calculated using members of the ensemble. This formulation of the inverse problem allows estimation of a virtually unlimited set of parameters, and drastically reduces the computational burden of estimating the relation between parameters and observations.\n",
    "\n",
    "But it is not all sunshine and rainbows.\n",
    "\n",
    "Calculating an empirical cross-covariance between large numbers of parameters and observations, with a limited number of realizations is likely to lead to spurious cross-correlations. Because (1) the relationship between observations and parameters is estimated empirically, and (2) in most cases, the number of realizations will be significantly smaller than the number of parameters, the estimated cross-covariances may contain error.\n",
    "\n",
    "This results in some parameters being adjusted, even if there was no information in the observation dataset to require their adjustment. In other words, artificial relations between parameters and observations can emerge. This can result in the cardinal sin of underestimating of forecast uncertainty - a phenomenon referred to as \"ensemble collapse\". \n",
    "\n",
    "### Localization\n",
    "\n",
    "To deal with these challenges we can employ \"localization\". Localization refers to a strategy in which only \"local\" covariances are allowed to emerge. In essence, a modeller defines a \"local\" neighbourhood around each observation, specifying the parameters which are expected to influence it. Effectively, this creates a series of \"local\" history matching problems using subsets of parameters and observations. Conceptually, localization allows a form of expert knowledge to be expressed in regard to how parameters and observations are or are not related. For example, an observation of groundwater level today cannot be correlated to the recharge which will occur tomorrow (e.g. information cannot flow backwards in time), or groundwater levels cannot inform porosity parameters, etc. \n",
    "\n",
    "#### Localization Matrix\n",
    "PESTPP-IES allows users to provide a localizing matrix to enforce physically plausible parameter-to-observation relations. This matrix must be prepared by the user. Matrix rows are observation names and/or observation group names, and columns are parameter names and/or parameter group names. Elements of the matrix should range between 0.0 and 1.0. A value of 0.0 removes any spurious sensitivities between the relevant observation and parameter. During this tutorial we will demonstrate how to construct such a matrix using `pyEMU` using two localization strategies.\n",
    "\n",
    "#### Automatic Adaptive Localization\n",
    "\n",
    "PESTPP-IES also has an option to automate this process by implementing a form of `automatic adaptive localization`. When employed, during each iteration, PESTPP-IES calculates the empirical correlation coefficient between each parameter and each observation. A \"background\" or \"error\" distribution for this correlation coefficient is also calculated. By comparing (in  statistical sense) these two, statistically significant correlations are identified and retained to construct a localization matrix. This matrix is then fed forward into the parameter adjustment process. Note that automatic adaptive localization can be implemented in tandem with a user supplied localization matrix. In this manner, the automated process is only applied to non-zero elements in the user supplied matrix. We will implement this option during the current tutorial.\n",
    "\n",
    "In practice, automatic localization doesn't resolve the level of localization that can be achieved by a matrix explicitly constructed by the user. However, it is better than no localization at all. In general, implementing some form of localization is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Current Tutorial\n",
    "\n",
    "In the current notebook we are going to pick up after the [\"ies_1_basics\"](../part2_06_ies/freyberg_ies_1_basics.ipynb) tutorial. We setup PEST++IES and ran it. We found that we can achieve great fits with historical data...but that (for some forecasts) the calculated posterior probabilities failed to cover the truth.\n",
    "\n",
    "In this tutorial we are going to take a first stab at fixing that. We are going to implement localization to remove the potential for spurious correlations between observations and parameters incurred by using an \"approximate\" partial derivatives.  \n",
    "\n",
    "### Admin\n",
    "\n",
    "The next couple of cells load necessary dependencies and call a convenience function to prepare the PEST dataset folder for you. Simply press `shift+enter` to run the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "import psutil\n",
    "\n",
    "import sys\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the template directory and copy across model files from the previous tutorial. Make sure you complete the previous tutorial first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the temporary working folder\n",
    "t_d = os.path.join('freyberg6_template')\n",
    "\n",
    "org_t_d = os.path.join(\"master_ies_1\")\n",
    "if not os.path.exists(org_t_d):\n",
    "    raise Exception(\"you need to run the '/freyberg_ies_1_basics.ipynb' notebook\")\n",
    "\n",
    "if os.path.exists(t_d):\n",
    "    shutil.rmtree(t_d)\n",
    "shutil.copytree(org_t_d,t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the PEST control file as a `Pst` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_path = os.path.join(t_d, 'freyberg_mf6.pst')\n",
    "pst = pyemu.Pst(pst_path)\n",
    "assert 'observed' in pst.observation_data.columns,pst.observation_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we are at the right stage to run ies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(os.path.join(t_d, 'freyberg_mf6.3.pcs.csv')), \"you need to run the '/freyberg_ies_1_basics.ipynb' notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick reminder of the PEST++ optional control variables which have been specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEST++IES with no localization\n",
    "\n",
    "Just as a reminder, and so we can compare the results later on, let's load in the results fmor theprevious tutorial and take a look at the (1) timeseries of measured and simulated heads, (2) the forecast probability distributions and (3) compare parameter prior and posterior distributions.\n",
    "\n",
    "\n",
    "As in the previous tutorial, let's write a couple of functions to helps os plot these up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tseries_ensembles(pr_oe, pt_oe,noise, onames=[\"hds\",\"sfr\"]):\n",
    "    pst.try_parse_name_metadata()\n",
    "    obs = pst.observation_data.copy()\n",
    "    obs = obs.loc[obs.oname.apply(lambda x: x in onames)]\n",
    "    obs = obs.loc[obs.obgnme.apply(lambda x: x in pst.nnz_obs_groups),:]\n",
    "    obs.obgnme.unique()\n",
    "\n",
    "    ogs = obs.obgnme.unique()\n",
    "    fig,axes = plt.subplots(len(ogs),1,figsize=(10,2*len(ogs)))\n",
    "    ogs.sort()\n",
    "    for ax,og in zip(axes,ogs):\n",
    "        oobs = obs.loc[obs.obgnme==og,:].copy()\n",
    "        oobs.loc[:,\"time\"] = oobs.time.astype(float)\n",
    "        oobs.sort_values(by=\"time\",inplace=True)\n",
    "        tvals = oobs.time.values\n",
    "        onames = oobs.obsnme.values\n",
    "        [ax.plot(tvals,pr_oe.loc[i,onames].values,\"0.5\",lw=0.5,alpha=0.5) for i in pr_oe.index]\n",
    "        [ax.plot(tvals,pt_oe.loc[i,onames].values,\"b\",lw=0.5,alpha=1) for i in pt_oe.index]\n",
    "        \n",
    "        oobs = oobs.loc[oobs.weight>0,:]\n",
    "        tvals = oobs.time.values\n",
    "        onames = oobs.obsnme.values\n",
    "        [ax.plot(tvals,noise.loc[i,onames].values,\"r\",lw=0.5,alpha=0.5) for i in noise.index]\n",
    "        ax.plot(oobs.time,oobs.obsval,\"r-\",lw=2)\n",
    "        ax.set_title(og,loc=\"left\")\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast_hist_compare(pt_oe,pr_oe, last_pt_oe=None,last_prior=None ):\n",
    "        num_plots = len(pst.forecast_names)\n",
    "        num_cols = 1\n",
    "        if last_pt_oe!=None:\n",
    "            num_cols=2\n",
    "        fig,axes = plt.subplots(num_plots, num_cols, figsize=(5*num_cols,num_plots * 2.5), sharex='row',sharey='row')\n",
    "        for axs,forecast in zip(axes, pst.forecast_names):\n",
    "            # plot first column with currrent outcomes\n",
    "            if num_cols==1:\n",
    "                axs=[axs]\n",
    "            ax = axs[0]\n",
    "            # just for aesthetics\n",
    "            bin_cols = [pt_oe.loc[:,forecast], pr_oe.loc[:,forecast],]\n",
    "            if num_cols>1:\n",
    "                bin_cols.extend([last_pt_oe.loc[:,forecast],last_prior.loc[:,forecast]])\n",
    "            bins=np.histogram(pd.concat(bin_cols),\n",
    "                                         bins=20)[1] #get the bin edges\n",
    "            pr_oe.loc[:,forecast].hist(facecolor=\"0.5\",alpha=0.5, bins=bins, ax=ax)\n",
    "            pt_oe.loc[:,forecast].hist(facecolor=\"b\",alpha=0.5, bins=bins, ax=ax)\n",
    "            ax.set_title(forecast)\n",
    "            fval = pst.observation_data.loc[forecast,\"obsval\"]\n",
    "            ax.plot([fval,fval],ax.get_ylim(),\"r-\")\n",
    "            # plot second column with other outcomes\n",
    "            if num_cols >1:\n",
    "                ax = axs[1]\n",
    "                last_prior.loc[:,forecast].hist(facecolor=\"0.5\",alpha=0.5, bins=bins, ax=ax)\n",
    "                last_pt_oe.loc[:,forecast].hist(facecolor=\"b\",alpha=0.5, bins=bins, ax=ax)\n",
    "                ax.set_title(forecast)\n",
    "                fval = pst.observation_data.loc[forecast,\"obsval\"]\n",
    "                ax.plot([fval,fval],ax.get_ylim(),\"r-\")\n",
    "        # set ax column titles\n",
    "        if num_cols >1:\n",
    "            axes.flatten()[0].text(0.5,1.2,\"Current Attempt\", transform=axes.flatten()[0].transAxes, weight='bold', fontsize=12, horizontalalignment='center')\n",
    "            axes.flatten()[1].text(0.5,1.2,\"Previous Attempt\", transform=axes.flatten()[1].transAxes, weight='bold', fontsize=12, horizontalalignment='center')\n",
    "        fig.tight_layout()\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = flopy.mf6.MFSimulation.load(sim_ws=t_d)\n",
    "ib = sim.get_model().dis.idomain.array[0,:,:]\n",
    "def plot_hk(pr_oe, pt_oe, pst):\n",
    "    obs = pst.observation_data\n",
    "    hkobs = obs.loc[obs.oname==\"hk\",:].copy()\n",
    "    hkobs[\"i\"] = hkobs.i.astype(int)\n",
    "    hkobs[\"j\"] = hkobs.j.astype(int)\n",
    "    fig,axes = plt.subplots(2,3,figsize=(10,10))\n",
    "    prmn,prstd,prmev = np.zeros_like(ib,dtype=float),np.zeros_like(ib,dtype=float),np.zeros_like(ib,dtype=float)\n",
    "    prmn[hkobs.i,hkobs.j] = pr_oe._df.loc[:,hkobs.obsnme].mean()\n",
    "    prstd[hkobs.i,hkobs.j] = pr_oe._df.loc[:,hkobs.obsnme].std()\n",
    "    prmev[hkobs.i,hkobs.j] = pr_oe.loc[\"base\",hkobs.obsnme].values    \n",
    "    ptmn,ptstd,ptmev = np.zeros_like(ib,dtype=float),np.zeros_like(ib,dtype=float),np.zeros_like(ib,dtype=float)\n",
    "    ptmn[hkobs.i,hkobs.j] = pt_oe._df.loc[:,hkobs.obsnme].mean()\n",
    "    ptstd[hkobs.i,hkobs.j] = pt_oe._df.loc[:,hkobs.obsnme].std()\n",
    "    ptmev[hkobs.i,hkobs.j] = pt_oe.loc[\"base\",hkobs.obsnme].values    \n",
    "    for arr in [prmn,prstd,prmev,ptmn,ptstd,ptmev]:\n",
    "        #arr = np.log10(arr)\n",
    "        arr[ib>0] = np.log10(arr[ib>0])\n",
    "        arr[ib==0] = np.nan\n",
    "    prarrs = [prmn,prstd,prmev]\n",
    "    ptarrs = [ptmn,ptstd,ptmev]\n",
    "    titles = [\"mean\",\"stdev\",\"MEV\"]\n",
    "    \n",
    "    for pr,pt,axes,title in zip(prarrs,ptarrs,axes.transpose(),titles):\n",
    "        vmn,vmx = min(np.nanmin(pr),np.nanmin(pt)),max(np.nanmax(pr),np.nanmax(pt))\n",
    "        cb = axes[0].imshow(pr,vmin=vmn,vmax=vmx)\n",
    "        plt.colorbar(cb,ax=axes[0],label=\"$log_{10}\\\\frac{m}{d}$\")\n",
    "        axes[0].set_title(\"prior \"+title)\n",
    "        cb = axes[1].imshow(pt,vmin=vmn,vmax=vmx)\n",
    "        plt.colorbar(cb,ax=axes[1],label=\"$log_{10}\\\\frac{m}{d}$\")\n",
    "        axes[1].set_title(\"posterior \"+title)\n",
    "    \n",
    "    return fig,axes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now that that is out of the way, load the observation ensembles from the prior, the posterior and the measuered+noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_oe = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(org_t_d,\"freyberg_mf6.0.obs.csv\"))\n",
    "pt_oe = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(org_t_d,\"freyberg_mf6.{0}.obs.csv\".format(pst.control_data.noptmax)))\n",
    "noise = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(org_t_d,\"freyberg_mf6.obs+noise.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And _finally_ plot them up. You should be famililar with thes plots fomr the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tseries_ensembles(pr_oe, pt_oe,noise, onames=[\"hds\",\"sfr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you recall, the posterior fails to capture the truth with a good number of realizations for some forecass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_forecast_hist_compare(pt_oe=pt_oe, pr_oe=pr_oe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right then. Here is where things get interesting. Let's take a look at the distribution of _parameters_, comparing their prior and posterior distributions. This will show us where the parameter adjustment process has ..well...adjusted parameters! (And by how much).\n",
    "\n",
    "We will use  `pyemu.plot_utils.ensemble_helper()` to display histograms of parameter groupings from the prior and posterior ensembles. We could display histograms of each parameter group by passing a dictionary of parameter group names (you would probably do so in real-world applications). This will result in a pretty large plot, because we have quite  a few parameter groups. \n",
    "\n",
    "For the purposes of this tutorial, let's instead group parameters a bit more coarsely and just lump all parameters of the same hydraulic property together. The function in the next cell groups parmeters that share the first three letters in the parameter group name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_pdict(pe_pr, pe_pt):\n",
    "        par = pst.parameter_data\n",
    "        pdict = {}\n",
    "        for string in pst.par_groups:\n",
    "                prefix= string[:3] #set first 3 letters as prefix\n",
    "                if prefix=='nel':\n",
    "                        prefix='porosity'\n",
    "                group = pdict.setdefault(prefix, [])\n",
    "                parnames = par.loc[par.pargp==string, 'parnme'].tolist()\n",
    "                group.extend(parnames)\n",
    "        return pdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, read in the parameter ensembles from the initial (prior) and last (posterior) PEST++IES iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_pr = pd.read_csv(os.path.join(org_t_d,\"freyberg_mf6.0.par.csv\"),index_col=0)\n",
    "pe_pt = pd.read_csv(os.path.join(org_t_d,\"freyberg_mf6.{0}.par.csv\".format(pst.control_data.noptmax)),index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary of parameters \"groupings\" using the function we preapared above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdict = group_pdict(pe_pr, pe_pt)\n",
    "# for example:\n",
    "pdict['npf'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot histograms for each of our parameter groupings in the `pdict` dictionary. The grey and blue bars are the prior and posterior parameter distribution, respectively. Where the blue bars have shifted away from the grey bars marks parameters which have been updated during history matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.plot_utils.ensemble_helper({\"0.5\":pe_pr,\"b\":pe_pt},plot_cols=pdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is a pretty coarse check. But it does allow us to pick up on parameters that are changing...but which shouldn't. Take porosity parameters for example (the panel on the second row on the right: `D)porosity`). Our observation data set is only composed of groundwater levels and stream gage measurments. Neither of these types of measurements contain information which should inform porosity. In other words, porosity parameters should be insensitive to history matching. However, PEST++IES has adjsuted them from their prior values. A clear sign of spurrious correlation. And if it's happening for porosity, who's to say it isn't happening for other prameters as well?\n",
    "\n",
    "Right then, let's fix this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Temporal Localization (and common sense)\n",
    "\n",
    "As described at the beginning of the notebook, a user can provide PEST++IES with a localization matrix. This matrix explicilty enforces physically-plausible parameter-to-observation relations. Perhaps more importantly it enforces the non-existence of physically implausible relations. This matrix has rows that are observation names and/or observation group names, and columns that are parameter names and/or parameter group names. Elements of the matrix should range between 0.0 and 1.0. \n",
    "\n",
    "Right then, let's get started and add some localization. The obvious stuff is temporal localization - scenario parameters can't influence historic observations (and the inverse is true) so let's tell PEST++IES about this.  Also, as discussed, should porosity be adjusted at all given the observations we have? (Not for history matching, but yes, it should be adjusted for forecast uncertainty analysis.)\n",
    "\n",
    "This involves several steps:\n",
    " - identify parameter names or parameter group names to specify in the matrix\n",
    " - identify observation names to specify in the matrix\n",
    " - construct a template matrix from the names\n",
    " - assign values to elements of the matrix for each parameter/observation pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section, we are going to use good ole' Python and functionality in `pyEMU` to construct such a matrix. As we constructed our PEST(++) interface using `pyemu.PstFrom` (see the \"pstfrom pest setup\" tutorial), we conveniently have lots of usefull metadata to draw on and help us processs parameters and observations. \n",
    "\n",
    "For starters, let's get the `parameter_data` and `observation_data` sections from the `Pst` control file. We are going to use them for some funky slicing and dicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get par data\n",
    "par = pst.parameter_data\n",
    "par.inst = par.inst.astype(int)\n",
    "# get obs data\n",
    "obs = pst.observation_data\n",
    "obs.time = obs.time.astype(float)\n",
    "# temporal units are different in obs and par:\n",
    "par.inst.unique(), obs.time.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inconveniently, temporal parameters in `par` were recorded with the \"stress period number\" (or `kper`) instead of model time (see the `par.inst` column). But the observations in `obs` were recorded wiht the model time (see the `obs.time` column). \n",
    "\n",
    "So we need to align these. We could go either way, but it is probably more robust to align to model \"time\" instead of \"stress period number\". The next cell updates the `par` parameter data to include a column with model time that corresponds to the time at the end of the stress period at which the parameter comes into existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpar = par.loc[par.parnme.str.contains(\"recharge\"),:]\n",
    "rpar = rpar.loc[rpar.ptype==\"cn\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par.loc[rpar.parnme,\"inst\"] = rpar.parnme.apply(lambda x: int(x.split(\"tcn\")[0].split('_')[-1])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column for each stress period; \n",
    "# we already have spd values associated to parameter names, \n",
    "# so we will use this to associate parameters to observations in time\n",
    "times = obs.time.unique()\n",
    "times.sort()\n",
    "for kper, time in enumerate(times):\n",
    "    par.loc[par.inst==int(kper), 'time'] = time\n",
    "    \n",
    "par.loc[rpar.parnme,[\"inst\",\"time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tyding that up, let's start preparing the parameter names (or parameter group names; PEST++IES accepts either) that we are gong to use as columns in the localization matrix. \n",
    "\n",
    "Let's start off with the easy ones: static parameters. These are parameters which do not vary in time. Let's assume we cannot effectively rule out correlation between them and observations purely based on the time at which the observation occurs. (So things like hydraulic condutivty, storage, SFR conductance, etc.) Let's make a list of parameter group names for these types of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static parameters; these parameters will all be informed by historic obsevration data\n",
    "prefixes = ['npf', 'sto', 'icstrt', 'ghb', 'sfrcondgr','sfrcondcn','sfrgr']\n",
    "static_pargps = [i for i in pst.par_groups if any(i.startswith(s) for s in prefixes)]\n",
    "# start making the list of localization matrix column names (i.e. parameter or parameter group names)\n",
    "# as we dont need to do any further processing for the loc matrix, we can just use parameter group names\n",
    "loc_matrix_cols = static_pargps.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_matrix_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also add the time-invariant forcing parameters to the \"static\" parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pst.parameter_data.pargp.unique()\n",
    "ti_recharge = [g for g in pst.par_groups if \"recharge\" in g and (\"1pp\" in g or \"1gr\" in g)]\n",
    "print(ti_recharge)\n",
    "assert len(ti_recharge) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_matrix_cols.extend(ti_recharge)\n",
    "static_pargps.extend(ti_recharge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we keep going on about porosity not being informed by the data. So let's give it special attention. Let's make a list of parameter groups which should not be adjusted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should we really be adjusting porosity? lets just make a list fo use later on\n",
    "prefixes = ['ne']\n",
    "dont_pargps = [i for i in pst.par_groups if any(i.startswith(s) for s in prefixes)]\n",
    "# keep building up our column name list\n",
    "loc_matrix_cols.extend(dont_pargps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let us make a list of parameter _names_ (not group names!) for parameters which vary over time. Why parameter _names_ and not parameter _group names_? Because some of these (namely the `wel` and `sfrgr` groups) have parameters _within_ a group which pertain to dfferent model times. So we need to drill down to explicitly asign values to specific parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal pars; parameters in the past cannot be informed by observations in the future\n",
    "# so, an observation in stress period 2 cannot inform a rechange parameter in stress period 1, and so on...\n",
    "prefixes = ['wel', 'rch']\n",
    "temp_pargps = [i for i in pst.par_groups if any(i.startswith(s) for s in prefixes) and i not in ti_recharge]\n",
    "temporal_pars = par.loc[par.pargp.apply(lambda x: x in temp_pargps),:].copy()\n",
    "loc_matrix_cols.extend(temporal_pars.parnme.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_matrix_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right'o. So now we have a list of parameter group and paramer names which we are going to use to construct our localization matrix: `loc_matrix_cols`. We also have a couple of sub-lists to help us select specific columns form the matrix after we have constructed it (`static_pargps`, `dont_pargps` and `temporal_pars`).\n",
    "\n",
    "Obviously, we also have the list of observations to use as rows in the matrix - they are simply all the non-zero observations in the `pst` control file: `pst.nnz_obs_names`.\n",
    "\n",
    "Let's get cooking! Generate a `Matrix` using the `pyemu.Matrix` class and then convert it `to_dataframe()` for easy manipulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a Matrix object with the nz_obs names as rows and parameters/par groups as columns:\n",
    "loc = pyemu.Matrix.from_names(pst.nnz_obs_names,loc_matrix_cols).to_dataframe()\n",
    "# just to make sure, set evry cell/element to zero\n",
    "loc.loc[:,:]= 0.0\n",
    "loc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we have the startings of a localization matrix. At this moment, every element is assigned a value of 0.0 (e.g. no parameter-to-observation correlation). We will now go through and assign a value of 1.0 to parameter-observation pairs for which a physically plausible relation might exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now proceed to assign values to elements of the matrix\n",
    "# assign a value of 1 to all rows for the static parameters, \n",
    "# as they may be informed by all obsevrations\n",
    "loc.loc[:,static_pargps] = 1.0\n",
    "# see what that looks like\n",
    "loc.loc[:,static_pargps].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the tricky bit - assigning localization for time-dependent parameters. We are going to say that an observation can _only_ inform parameters that are up to 180 days in the past (e.g. 180 days before the observation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here comes the tricky bit, assigning localization for time-dependent parameters\n",
    "# we are going to say that an observation can _only_ inform parameters that are \n",
    "# up to 180 days into the past; not in the future. \n",
    "#Parameters that are beyond the historic period are not informed by observations\n",
    "nz_obs = pst.observation_data.loc[pst.nnz_obs_names,:]\n",
    "cutoff = 10000 # you can experiement with this number to control the \"system memory\" between forcings and water levels\n",
    "times = nz_obs.time.unique()\n",
    "times.sort()\n",
    "for time in times:\n",
    "    kper_obs_names = nz_obs.loc[nz_obs.time==time].obsnme.tolist()\n",
    "    # get pars from the same kper and up to -180 days backward in time\n",
    "    kper_par_names = temporal_pars.loc[temporal_pars.time.apply(lambda x: x>time-cutoff and x<=time)].parnme.tolist()\n",
    "    # update the loc matrix\n",
    "    loc.loc[kper_obs_names, kper_par_names] = 1.0\n",
    "# see what that looks like:\n",
    "loc.loc[kper_obs_names, kper_par_names].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.imshow(loc.values)\n",
    "ax.set_xticks(np.arange(loc.shape[1]))\n",
    "ax.set_xticklabels(loc.columns.values,rotation=90,fontsize=2)\n",
    "ax.set_yticks(np.arange(loc.shape[0]))\n",
    "_ = ax.set_yticklabels(loc.index.values,fontsize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! We should be good to go. Just a quick to check to see if we messed something up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure havent done something silly\n",
    "assert loc.loc[:,dont_pargps].sum().sum()==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good? Excellent. Let's rebuild the `Matrix` (don't tell Neo) and then write it to an external file named `loc.mat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.Matrix.from_dataframe(loc).to_ascii(os.path.join(t_d,\"loc.mat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost done! We need to tell PEST++IES what file to read. We do so by specifying the file name in the `ies_localizer()` PEST++ control variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"ies_localizer\"] = \"loc.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to re-write the control file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"ies_num_reals\"] = 30 # in theory, with localization we can get by with less reals... feel like #livingdangerously?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d, 'freyberg_mf6.pst'),version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, good to go. As usual, make sure to specify the number of workers that your machine can cope with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = psutil.cpu_count(logical=False) #update this according to your resources\n",
    "m_d = os.path.join('master_ies_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(t_d, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-ies', #the PEST software version we want to run\n",
    "                            'freyberg_mf6.pst', # the control file to use with PEST\n",
    "                            num_workers=num_workers, #how many agents to deploy\n",
    "                            worker_root='.', #where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, #the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Localization Outcomes\n",
    "\n",
    "By now you should be familiar with the next few plots. Let's blast through our plots of timeseries, forecast histograms and parameter distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_pr_tloc = pd.read_csv(os.path.join(m_d,\"freyberg_mf6.0.par.csv\"),index_col=0)\n",
    "pe_pt_tloc = pd.read_csv(os.path.join(m_d,\"freyberg_mf6.{0}.par.csv\".format(pst.control_data.noptmax)),index_col=0)\n",
    "\n",
    "pdict = group_pdict(pe_pr_tloc, pe_pt_tloc)\n",
    "fig = pyemu.plot_utils.ensemble_helper({\"0.5\":pe_pr_tloc,\"b\":pe_pt_tloc},plot_cols=pdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the parameter changes. Hey whadya know! That looks a bit more reasonable, doesn't it? Porosity parameters no longer change from the prior to the posterior. Variance for temporal parameters has also changed. Excellent. At least we've removed _some_ potential for underestimating forecast uncertainty. Next check what this has done for history matching and, more importantly, the forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok!  Now we see that the porosity parameters are unchanged - just like we wanted!  Do you think this will effect the travel time forecast???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in the new posterior observation ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_oe_tloc = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(m_d,\"freyberg_mf6.{0}.obs.csv\".format(pst.control_data.noptmax)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tseries_ensembles(pr_oe, pt_oe_tloc,noise, onames=[\"hds\",\"sfr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still getting acceptable fits with historical data (not strictly as low a phi as the unlocalized case, but c'mon - who wouldnt accept that level of fit?!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_hk(pr_oe,pt_oe,pst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happend with our ever important forecasts? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_forecast_hist_compare(pt_oe=pt_oe_tloc, pr_oe=pr_oe, last_pt_oe=pt_oe, last_prior=pr_oe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, much wider posterior distributions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Based Localization\n",
    "\n",
    "This is industrial strength localization that combines the temporal localization from before with a distance-based cutoff between each spatially-distributed parameter type and each spatially-discrete observation.  In this way, we are defining a \"window\" around each observation and only parameters that are within this window are allowed to be conditioned from said observation.  It's painful to setup and subjective (since a circular window around each observation is a coarse approximation) but in practice, it seems to yield robust forecast estimates.\n",
    "\n",
    "For the first time now, we will be using a fully-localized solve, meaning each parameter is upgraded independently.  This means PEST++IES has to run through the upgrade calculations once for each parameter - this can be very slow.  Currently, PESTPP-IES can multithread these calculations but the optimal number of threads is very problem specific.  Through testing, 3 threads seems to be a good choice for this problem (the PEST++IES log file records the time it takes to solve groups of 1000 pars for each lambda so you can test for your problem too)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few cells we are going to make use of `flopy` and some of the metadata that `pyemu.PstFrom` recorded when constructing our PEST(++) setup to calculate distances between parmaters and observations. We will do this only for groundwater level observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load simulation\n",
    "sim = flopy.mf6.MFSimulation.load(sim_ws=t_d, verbosity_level=0)\n",
    "# load flow model\n",
    "gwf = sim.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our head observation names conveniently contain the layer, row and column number. This allows us to use `flopy` to obtain their `x` and `y` coordinate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by getting the locations of observation sites\n",
    "# we will only do this for head obsevrations; other obs types in our PEST dataset arent applicable\n",
    "hobs = nz_obs.loc[nz_obs.oname.isin(['hds','hdsvd','hdstd'])].copy()\n",
    "hobs.loc[:,'i'] = hobs.obgnme.apply(lambda x: int(x.split('-')[-2]))\n",
    "hobs.loc[:,'j'] = hobs.obgnme.apply(lambda x: int(x.split('-')[-1]))\n",
    "# get x,y for cell of each obs\n",
    "hobs.loc[:,'x'] = hobs.apply(lambda x: gwf.modelgrid.xcellcenters[x.i,x.j], axis=1)\n",
    "hobs.loc[:,'y'] = hobs.apply(lambda x: gwf.modelgrid.ycellcenters[x.i,x.j], axis=1)\n",
    "# group them for use later in identifying unique locations\n",
    "hobs.loc[:,\"xy\"] = hobs.apply(lambda x: \"{0}_{1}\".format(x.x,x.y),axis=1)\n",
    "hobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the `x` and `y` coordinate of all grid and pilot point based parameters recorded in the `Pst` `parameter_data` section. Convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par.x=par.x.astype(float)\n",
    "par.y=par.y.astype(float)\n",
    "par.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a `Matrix` for _all_ adjustable parameter _names_ and non-zero observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_loc = pyemu.Matrix.from_names(pst.nnz_obs_names,pst.adj_par_names).to_dataframe()\n",
    "# set all elements to 1.0 to make sure all observation-parameter pairs are \"active\"\n",
    "spatial_loc.values[:,:] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the tricky bit. We are going to go through each observation location and assign 0.0 to rows that correspond to _spatially distributed_ parameters that are further away than a specified cutoff distance (`loc_dist`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cutoff distance\n",
    "loc_dist = 5000.0 # arbitrary!\n",
    "# prepare a set of adjustable parameter names\n",
    "sadj = set(pst.adj_par_names)\n",
    "#select only spatial params to avoid applying to layer-wide multiplier parameters\n",
    "spatial_par = par.loc[par.x.notnull()].copy()\n",
    "# group obs by location\n",
    "xy_groups = hobs.groupby('xy').groups\n",
    "print('Number of observation sites:',len(xy_groups))\n",
    "# loop through each observation site and \"blank out\" correlation with parameters which are too far away\n",
    "for xy,onames in xy_groups.items():\n",
    "    # get the obs site x and y coords\n",
    "    oname = onames[0]\n",
    "    xx,yy = hobs.loc[oname,['x','y']]\n",
    "    # calculate distance from obs to parameters\n",
    "    spatial_par.loc[:,'dist'] = spatial_par.apply(lambda x: (x.x - xx)**2 + (x.y - yy)**2,axis=1).apply(np.sqrt)\n",
    "    # select pars that are too far from obs\n",
    "    too_far = spatial_par.loc[spatial_par.dist > loc_dist,\"parnme\"]\n",
    "    # keep only adjustable pars\n",
    "    too_far = too_far.loc[too_far.apply(lambda x: x in sadj)]\n",
    "    # assign zero to loc matrix for parameters that are too far from obs\n",
    "    spatial_loc.loc[onames, too_far] = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to update that with the temporal localization we prepared earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the the loc matrix with temporal localization that we prepared previously\n",
    "temporal_parnames = temporal_pars.parnme.tolist()\n",
    "spatial_loc.loc[loc.index, temporal_parnames] = loc.loc[:, temporal_parnames]\n",
    "# make sure the porosity pars are not; go down to parname level to make sure \n",
    "dont_pars = par.loc[par.pargp.apply(lambda x: x in dont_pargps),\"parnme\"].tolist()\n",
    "spatial_loc.loc[:,dont_pars] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always good to throw in some checks to make sure we aren't missing something:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_loc.loc[:, [i for i in spatial_loc.columns if ':ne' in i]].sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right then. Rebuild the `Matrix` from the dataframe, write it to an external file and update the relevant PEST++ option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.Matrix.from_dataframe(spatial_loc).to_coo(os.path.join(t_d,\"spatial_loc.jcb\"))\n",
    "pst.pestpp_options[\"ies_localizer\"] = \"spatial_loc.jcb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final consideration. \n",
    "\n",
    "Through localization, a complex parameter estimation problem can be turned into a series of independent parameter estimation problems. If large numbers of parameters are being adjusted, the parameter upgrade calculation process for a given lambda will require as many truncated SVD solves as there are adjustable parameters. This can require considerable numerical effort. To overcome this problem, the localized upgrade solution process in PESTPP-IES has been multithreaded; this is possible in circumstances such as these where each local solve is independent of every other local solve. The use of multiple threads is invoked through the `ies_num_threads()` control variable. It should be noted that the optimal number of threads to use is  problem-specific. Furthermore, it should not exceed the number of physical cores of the host machine on which the PESTPP-IES master instance is running.\n",
    "\n",
    "However, the fully localized solve is still sssslllloooooowwwwwww.  So if you have heaps of parameters (>30,000 say) it may actually be faster to use more realizations rather than use localization in terms of wall time - more realizations will over come the issues related to spurious correlation simply by having more samples to calculate the empirical derivatives with...but this depends on the runtime of the forward model as well. As usual, the answer is: \"It depends\" - haha!\n",
    "\n",
    "Just to make this notebook experience more enjoyable, lets limit the number of lambdas being tested (so that we only have to solve the fully localized solution once per iteration...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"ies_lambda_mults\"] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the control file and deploy PEST++IES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d,\"freyberg_mf6.pst\"),version=2)\n",
    "m_d = os.path.join('master_ies_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(t_d, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-ies', #the PEST software version we want to run\n",
    "                            'freyberg_mf6.pst', # the control file to use with PEST\n",
    "                            num_workers=num_workers, #how many agents to deploy\n",
    "                            worker_root='.', #where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, #the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Localization Outcomes\n",
    "\n",
    "You know the drill by now. Let's look at our parameter and observation distributions. \n",
    "\n",
    "Starting with the parameters. Perhaps a bit less change. Hard to tell at this scale. What about the observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_pr_sloc = pd.read_csv(os.path.join(m_d,\"freyberg_mf6.0.par.csv\"),index_col=0)\n",
    "pe_pt_sloc = pd.read_csv(os.path.join(m_d,\"freyberg_mf6.{0}.par.csv\".format(pst.control_data.noptmax)),index_col=0)\n",
    "\n",
    "pdict = group_pdict(pe_pr_sloc, pe_pt_sloc)\n",
    "fig = pyemu.plot_utils.ensemble_helper({\"0.5\":pe_pr_sloc,\"b\":pe_pt_sloc},plot_cols=pdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the new posterior ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_oe_sloc = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(m_d,\"freyberg_mf6.{0}.obs.csv\".format(pst.control_data.noptmax)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, fits are than with temporal localization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tseries_ensembles(pr_oe, pt_oe_sloc,noise, onames=[\"hds\",\"sfr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_hk(pr_oe,pt_oe_sloc,pst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the ever important foreacasts. Again, a bit more variance in the null-space dependent forecasts (i.e. particle travel time) and a bit less variance in the more solution-space dependent forecasts #winning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_forecast_hist_compare(pt_oe=pt_oe_sloc, pr_oe=pr_oe, last_pt_oe=pt_oe_tloc, last_prior=pr_oe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PESTPP-IES with Automatic Adaptive Localization\n",
    "\n",
    "PESTPP-IES includes functionality for automatic localization.  In practice, this form of localization doesn't resolve the level of localization that more rigorous explicit localization that you get through a localization matrix.  However, its better than no localization at all. \n",
    "\n",
    "A localization matrix supplied by the user can be used in combination with automatic adaptive localization (autoadaloc). When doing so, autoadaloc process is restricted to to the allowed parameter-to-observation relations in the user specified localization matrix. The automated process will only ever adjust values in the localization matrix downwards (i.e. decrease the correlation coefficients).\n",
    "\n",
    "And, just like par-by-par distance based localization above, we  need to solve the upgrade equations once for each parameter...this can take quite a bit of CPU time. Multithreading is a must.\n",
    "\n",
    "How do we implement it? Easy peasy. Just activate the `ies_autoadaloc()` PEST++ option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options.pop(\"ies_localizer\") #should you wish to try autoadaloc on its own, simply drop the loc matrix\n",
    "# or use the temporal localizer and let the AAD process work within those rules:\n",
    "pst.pestpp_options[\"ies_localizer\"] = \"loc.mat\"\n",
    "pst.pestpp_options[\"ies_autoadaloc\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these upgrade calcs take a while, so let's only do one lambda\n",
    "pst.pestpp_options[\"ies_lambda_mults\"] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one control value for `autoadaloc` is `ies_autoadaloc_sigma_distance` which is the statistical difference background or error correlation estimate and the current correlation coefficient.  Any correlation coefficient that is less than the error mean plus/minus error standard deviation times `ies_autoadaloc_sigma_distance` is treated as a non-significant correlation and is localized out. So large `ies_autoadaloc_sigma_distance` values result in stronger localization.  The default value is 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"ies_autoadaloc_sigma_dist\"] = 1\n",
    "pst.pestpp_options[\"ies_num_threads\"] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool beans. We are good to go. Just re-write the control file and let PEST++IES loose. Now, keep in mind that, even though we are using only a few hundred model runs, solving the parameter upgrade equations for a high-dimensional problem can be quite expensive. So this will take  bit longer than previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d,\"freyberg_mf6.pst\"),version=2)\n",
    "m_d = os.path.join('master_ies_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(t_d, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-ies', #the PEST software version we want to run\n",
    "                            'freyberg_mf6.pst', # the control file to use with PEST\n",
    "                            num_workers=num_workers, #how many agents to deploy\n",
    "                            worker_root='.', #where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, #the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoAdaLoc Outcomes\n",
    "\n",
    "Right, let's go straight for our forecasts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_oe_autoloc = pyemu.ObservationEnsemble.from_csv(pst=pst,filename=os.path.join(m_d,\"freyberg_mf6.{0}.obs.csv\".format(pst.control_data.noptmax)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_hk(pr_oe,pt_oe_autoloc,pst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_forecast_hist_compare(pt_oe=pt_oe_autoloc, \n",
    "                                pr_oe=pr_oe, \n",
    "                                last_pt_oe=pt_oe_sloc, \n",
    "                                last_prior=pr_oe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby!\n",
    "\n",
    "Thus far we have implemented localization, a strategy to tackle spurious parameter-to-observation correlation. In doing so we reduce the potential for \"ensemble collapse\", a fancy term that means an \"underestimate of forecast uncertainty caused by artificial parameter-to-observation relations\". This solves history-matching induced through using ensemble based methods, but it does not solve a (the?) core issue - trying to \"perfectly\" fit data with an imperfect model will induce bias. \n",
    "\n",
    "Now, as we have seen, for some forecasts this is not a huge problem (these are data-driven forecasts, which are well informed by available observation data). For others, it is (these are the forecasts which are influenced by parameter combinations in the null space, that are not informed by observation data). But when undertaking modelling in the real world, we will rarely know where our forecast lies on that spectrum (probably somewhere in the middle...).  So, better safe than sorry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
