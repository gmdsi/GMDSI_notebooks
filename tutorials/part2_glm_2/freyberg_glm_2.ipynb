{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highly Parameterized Inversion\n",
    "\n",
    "The current tutorial continues where the \"freyberg glm part1\" notebook left off. The \"freyberg intro to model\" and \"freyberg pstfrom pest setup\" provide details on the model and PEST dataset. The \"freyberg glm 1\" notebooks introduced changes to the PEST setup that are relevant to the current tutorial. You do not need to know the details of all these noteboks to follow along with general points of the current tutorial - but it helps! \n",
    "\n",
    "In this tutorial we will add the final tweaks to a PEST dataset and then calibrate our model using PEST++GLM. Much like PEST, PEST++GLM undertakes highly parameterized inversion. However, it streamlines alot of the user-input process. It also automates FOSM and FOSM-based Monte Carlo uncertainty analyses. This drastically reduces requirements for user input, making workflows easier to implement. However, it also increases the number of moving pieces that a user must be familiar with (no free lunch!).\n",
    "\n",
    "Here we will discuss some PEST++GLM specific options and their implications, calibrate the model and then explore outputs - all using a programatic interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Admin\n",
    "\n",
    "Start off with the usual loading of dependencies and preparing model and PEST files. We will be continuing to work with the MODFLOW6 modified-Freyberg model (see \"freyberg intro to model\" notebook), and the high-dimensional PEST dataset prepared in the \"freyberg glm 1\" notebook. For the purposes of this notebook, you do not require familiarity with previous notebooks (but it helps...). \n",
    "\n",
    "Simply run the next few cells by pressing `shift+enter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import numpy as np\n",
    "import pyemu\n",
    "import flopy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt;\n",
    "import shutil \n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# import pre-prepared convenience functions\n",
    "import herebedragons as hbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the path to the PEST dataset template folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the temporary working folder\n",
    "t_d = os.path.join('freyberg6_template')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy across pre-prepared model and PEST files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(t_d):\n",
    "        shutil.rmtree(t_d)\n",
    "org_t_d = os.path.join(\"..\",\"part2_glm_1\",t_d)\n",
    "if not os.path.exists(org_t_d):\n",
    "    hbd.dir_cleancopy(org_d=os.path.join('..','..', 'models','freyberg_glm_1'), \n",
    "                new_d=t_d)\n",
    "    # get and unzip the prior covariance JCB file, prepared in \"freyberg pstfrom\" tutorial\n",
    "    hbd.unzip(os.path.join('..','..','models','prior_cov.zip'), os.path.join(t_d))\n",
    "else:\n",
    "    print(\"using files at \",org_t_d)\n",
    "    shutil.copytree(org_t_d,t_d)\n",
    "pt1_jco_path = os.path.join(\"..\",\"part2_glm_1\",\"master_glm_1\",\"freyberg_pp.jcb\")\n",
    "if os.path.exists(pt1_jco_path):\n",
    "    print(\"using jacobian: \",pt1_jco_path)\n",
    "    shutil.copy2(pt1_jco_path,os.path.join(t_d,\"freyberg_pp.jcb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right then, let's load in our PEST control file.\n",
    "\n",
    "Just as a reminder, this is the control fle prepared in the \"freyber glm 1\" notebook:\n",
    " - We have reduced our very high dimensional PEST dataset down from 10s of thousands to several hundreds of parameters (#sad).\n",
    " - We continue to have lots of spatially distributed parameters (pilot points), which we expect to have a degree of spatial (and in some cases, temporal) correlation. \n",
    " - We have weighted observations to reflect the inverse of measurement noise standard deviation. \n",
    " - We have already calculated a Jacobian matrix, with parameter sensitivities based on intial parameter values. It is stored in a file named \"freyberg_pp.jcb\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(t_d, 'freyberg_pp.pst'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of observations: {pst.nnz_obs} \\nnumber of parameters: {pst.npar_adj}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Regularisation\n",
    "\n",
    "We have more unkown parameters than observations. Thus, we have an ill-posed inverse problem. The mathematical term for the process through which a unique solution is sought for a nonunique inverse problem is “regularisation”. The goal of regularised inversion is to seek a unique parameter field that results in a suitable fit between model outputs and field measurements, whilst minimizing the potential for wrongness in model predictions. That is, out of all the ways to fit a calibration dataset, regularized inversion seeks the parameter set of minimum error variance.\n",
    "\n",
    "There are two main approaches used to stabilize this problem: (1) adding soft knowledge and/or (2) reducing the problem dimensionality. These methods can be used by themselves, but are most commonly applied together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Tikhonov Regularisation (e.g. \"soft knowledge\")\n",
    "\n",
    "One way to seek a parameter field of minimum error variance is to seek a parameter field that allows the model to fit the calibration dataset, but whose values are also as close as possible to a set of “preferred parameter values”. Ideally, preferred parameter values should also be initial parameter values as listed in the “parameter data” section of the PEST control file. These preferred parameter values are normally close to the centre of the prior parameter probability distribution. At the same time, scales and patterns of departures from the preferred parameter set that are required for model outputs to fit a calibration dataset should be achieved in ways that make “geological sense”.\n",
    "\n",
    "PEST provides a user with a great deal of flexibility in how Tikhonov constraints can be introduced to an inversion process. The easiest way is to do this is through the use of prior information equations. When prior information equations are employed, Tikhonov constraints are expressed through preferred values that are assigned to linear relationships between parameters. (Equality is the simplest type of linear relationship.) Weights must be assigned to these equations. As is described in PEST documentation, when PEST is run in “regularisation” mode, it makes internal adjustments to the weights that are assigned to any observations or prior information equations that belong to special observation groups that are referred to as “regularisation groups”. \n",
    "\n",
    "\n",
    "#### 2.2. Promoting \"Geologicaly Reasonable\" Patterns\n",
    "\n",
    "PEST (and PEST_HP) provide the option to replace prior information equation weights with covariance matrices. When prior information equations embody Tikhonov constraints, they can be used to ensure that patterns of parameter heterogeneity that emerge from the inversion process are geologically reasonable. (See the GMDSI Tutorials for examples of how to accompish this with PEST_HP: https://gmdsi.org/blog/calibration-a-simple-model/)\n",
    "\n",
    "PEST++GLM (which we will be using here) does things a bit differently. You *can* specify prior information equations and assign relevant weights and run PEST++GLM in \"regularisation\" mode. (See the \"intro to pyemu\" notebook for an example of how to specify prior information equations.) However, PEST++GLM does not accept covariance matrices for the purposes of specifying preferred parameter heterogeneity.\n",
    "\n",
    "Alternatively, PEST++GLM offers users the option of using prior parameter covariance matrix based regularisation directly in the parameter upgrade calculation process - referred to as Regularized-Gauss-Levenburg-Marquardt. Instead of using the contribution of regularisation to the objective function to avoid \"unrealistic\" parameter patterns (as is done in PEST), this approach tries to maintain \"realistic\" patterns directly when calculating parameter upgrades. \n",
    "\n",
    "As we will show further on, this is implemented by specifying a prior parameter covariance matrix and the relevant `pest++` options. If this option is enabled, then prior information equation based regularisation cannot also be employed. Regularisation is controlled by the max singular values and eigen threshold control variables in the SVD section.\n",
    "\n",
    "Pragmatically, the latter approach is the easier one to implement within a PEST++ workflow. A modeller will likley have already prepared a prior parameter covariance marix (we have uncertianty on our mind after all...). Thus, only a few lines need to be added to the PEST control file. It also removes the need to determine and dynamically adjust prior information weights, making the process numerically more efficient. \n",
    "\n",
    "The Regularized-Gauss-Levenburg-Marquardt option is activated with the `glm_normal_form(prior)` pest++ option and by providing a prior parameter covariance matrix in `parcov()`.\n",
    "\n",
    "Let's start by preparing this covariance matrix. Recall that we created a prior covariance matrix in the \"freyberg pstfrom pest setup\" tutorial and saved it in a file named \"prior_cov.jcb\". That matrix contains values for *all* the parameters. First, we need to reduce it down to only the adjustable parameters in our current \"pilot points\" control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the orginal high-dimensional covariance matrix\n",
    "cov = pyemu.Cov.from_binary(os.path.join(t_d,\"prior_cov.jcb\"))\n",
    "\n",
    "# reduce it down to the currently adjustable parameters\n",
    "cov = cov.get(pst.adj_par_names)\n",
    "\n",
    "# write the reduced covariance matrix to an external file\n",
    "cov.to_ascii(os.path.join(t_d,\"glm_prior.cov\"))\n",
    "\n",
    "x = cov.x.copy()\n",
    "x[np.abs(x)<1.0e-4] = np.nan\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the covariance matrix is ready, let's add the pest++ control variables to the `pst` control file.\n",
    "\n",
    "Start by providing the control variable specifying the filename for the prior parameter covariance matrix. This matrix is used for both regularisation and during FOSM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the prior parameter covariance matrix file\n",
    "pst.pestpp_options[\"parcov\"] = \"glm_prior.cov\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then specify the option to employ Regularized-Gauss-Levenburg-Marquardt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate the regularized-GLM solution\n",
    "pst.pestpp_options[\"glm_normal_form\"] = \"prior\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. SVD\n",
    "\n",
    "Tikhonov regularisation adds information to the calibration process in order to achieve numerical stability. In contrast, subspace methods do so by reducing the dimensionality of the problem, removing and/or combining prameters. When employing SVD in calibration, only parameters and linear combinations of parameters that are suficiently constrained by measured data are estimated. These parameters are said to reside in the *solution space*. Chossing which parameter combinations to estimate is accomplished via singular value decomposition (SVD). SVD-based parameter estimation fixes intial values for parameters/parameter combinations that are not estimable (reside in the *null space*) and does not adjust them during inversion. (So make sure initial parameter values are sensible!)  \n",
    "\n",
    "Unlike PEST, by default members of the PEST++ suite employ singular value decomposition (or methods closely related to it) for solution of the inverse problem. Unless otherwise specifed, default options are employed. PEST++GLm offers two numerical libraries for implementing SVD; the default option will usually suffice (see the manual for details).\n",
    "\n",
    "Two variables affect how SVD is deployed: MAXSING and EIGTHRESH. These are recorded in the `* singular value decomposition` section of the PEST control file. By default, MAXSING is equal to the number of adjustable parameters and EIGHTRESH is 1.0E-7. \n",
    "\n",
    "When employing Regularized-Gauss-Levenburg-Marquardt (i.e. if the `glm_normal_form(prior)` option is specified, as we have done here), the values of MAXSING and EIGTHRESH control the degree of regularisation. \n",
    "\n",
    "For the purposes of this tutorial, we will use default values. However, if desired, these variables could be assigned thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAXSING\n",
    "pst.svd_data.maxsing = pst.npar_adj\n",
    "\n",
    "#EIGTHRESH\n",
    "pst.svd_data.eigthresh = 1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. SVD-assist\n",
    "\n",
    "Use of PEST’s “SVD-assist” methodology can promulgate significant increases in the numerical efficiency of highly parameterized inversion. In implementing this methodology, PEST estimates the values of so-called “super parameters” in place of the parameters themselves. \n",
    "\n",
    "Estimating super parameters can reduce the computational cost of highly parameterized inversion considerably. However, a Jacobian matrix based on the full parameter set must be calculated before the super parameter inversion process.\n",
    "\n",
    "We have already done so in the \"freyberg glm 1\" tutorial. We can now make use of the pre-calculated Jacobian matrix to save time. We specify this with the `base_jacobian` pest++ option. Note that this is **not** an SVD-Assist specific option. It can be used any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the Jacobian file with a different name\n",
    "shutil.copy2(os.path.join(t_d,\"freyberg_pp.jcb\"),\n",
    "             os.path.join(t_d,\"freyberg_reuse.jcb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify file to use as the base jacobian\n",
    "pst.pestpp_options[\"base_jacobian\"] = \"freyberg_reuse.jcb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the large computational savings gained from SVD-assisted inversion come with a number of costs. Chief among these is that, for a nonlinear model, the partitioning of parameter space into solution and null spaces based on singular decomposition of a full Jacobian matrix calculated on the basis of initial parameter values may not remain valid as parameter values change. Hence, as the super parameter inversion process progresses, it may become necessary to recalculate a full Jacobian matrix so that super parameters can be re-defined. \n",
    "\n",
    "For moderately to highly nonlinear models, super parameter redefinition may be required every few iterations. With intermittent super parameter redefinition, model run savings accrued through SVD-assisted inversion may still be considerable; however, they will not be as great as for a linear model where re-computation of a full Jacobian matrix is never required.\n",
    "\n",
    "In PEST++GLM, the `n_iter_base` and `n_iter_super` pestpp options determine the number of sequential base and super parameter iterations, respectively. Choosing values for these variables will be case specific and may require some trial and error. Relatively linear problems may work well a single base iteration. Non liner problems will likley benefit from fewer super iterations, interspersed with base iterations. Ideal values may also depend on the maximum number of super parameters (discussed further on).\n",
    "\n",
    "For our case we have found that a single base parameter iteration, followed by a few super iterations is sufficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convenience, specify a maximum number of iterations. Avoids wasting cpu-time for the purposes of the tutorial; in the real-world you will probably use more than this\n",
    "pst.control_data.noptmax = 2\n",
    "\n",
    "# specify the number of base parameter iterations; \n",
    "# by passing a value of -1, PEST++GLM will calculate a single iteration and then switch to super parameter iterations\n",
    "pst.pestpp_options[\"n_iter_base\"] = -1\n",
    "\n",
    "# specify the number of subsequent super parameter iterations; set equal to NOPTMAX\n",
    "pst.pestpp_options[\"n_iter_super\"] = pst.control_data.noptmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of super parameters determines the number of model runs undertaken during a super parameter iteration. The number of super parameters to estimate can be set using either or both of the `max_n_super()` and ­`super_eigthresh()` pest++ control variable.  The upper limit of super parameters to form is controlled by `max_n_super()`. Ideally, this value should be as high as possible. In practice, it may have to reflect available computational resources. (See the PEST Manual Part 1 for a pragmatic discussion of how to choose the number of super parameters.) \n",
    "\n",
    "For our case, through a bit of trial-and-error we know reasonable results can be achieved with a maximum of super parameters specified in the following cell. That's already a decent amount of savings in terms of run-time! Now, for each iteration instead of running the model several hundred times (i.e. the number of adjustable base parameters; see `pst.npar_adj`) we only need to run the model a few tens of times. An order of magnitude less!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"max_n_super\"] = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.5. FOSM\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEST++GLM makes FOSM parameter and predictive uncertainty analysis a breeze. \n",
    "\n",
    "At the end of each iteration PEST++GLM implements FOSM analysis, as long as the `uncertainty()` control variable is set to `true` (it is by default). If present, it ignores regularisation and prior information equations (not present in our case). By default, prior parameter uncertainties are calculated from parameter bounds. Alternatively, a user can supply a prior parameter covariance matrix with the `parcov()` variable; this permits inclusion of prior parameter covariances. Recall that we have already specified this option: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options['parcov']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEST++GLM calculates a posterior parameter covariance matrix at the end of each iteration. Each covariance matrix is recorded in an extenral file. PEST++GLM also provides summaries of prior and posterior parameter uncertainties (means, standard deviations and bounds) for each iteration.\n",
    "\n",
    "If any observations are listed as forecasts (in the `forecast()` variable), PEST++GLM will also undertake predicitve uncertainty analysis. By default, if no forecasts are provided, PEST++GLM will assume all zero-weighted observations are forecasts - so it is usually a good idea to specify forecasts explicitly (if you have many many zero-weighted obsevrations FOSM can cost some computation time). Recall that we specifed several observations as forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options['forecasts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOSM implemented by PEST++GLM assumes that the standard deviation of measurement noise associated with each observation is proportional current observation residual. This accounts for the model's ability to reproduce an obsevration. Effectively, it assumes that the residual is a measure of measurement noise + model error. Thus, observation weights used during FOSM are calcualted as the inverse of the residual. Note that this \"residual weight\" never increases weights beyond those which are specified in the control file! The assumption is that weighs in the control file represent the inverse of measurment noise standard deviation - and it would be illogical to decrease noise beyond this level. \n",
    "\n",
    "It is important to keep this in mind. If observation weights in the control file do **not** represent measurement noise, then it may be preferable to not use PEST++GLM to undertake FOSM during parameter estimation. In our case, weights represent the inverse of measurment standard deviations - so we are all good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. FOSM-informed Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEST++GLM also has the ability to undertake nonlinear Monte Carlo uncertainty analysis. FOSM-based posterior Monte Carlo (also called Bayes-linear Monte Carlo) is implemented by drawing samples of parameters from the posterior parameter distribution (described by the posterior parameter covariance matrix and assuming best-fit parameter values as the mean). Each of these parameter realisation is then simulated. As long as forecasts are included as observations in the control file, then the Monte Carlo process provides an ensemble of forecast values. With enough samples of foreacast the posterior predictive uncertainty can be described. In principle, using FOSM-based Monte Carlo to evaluate forecast uncertainty relaxes the assumption of linearity between and foreacsts and parameters - making it more robust.\n",
    "\n",
    "Activating this option is as easy as adding the `glm_num_reals()` option to the control file and specifying the number of Monte Carlo realisation to sample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"glm_num_reals\"] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Extra Utility Options\n",
    "\n",
    "There are numerous \"utility\" options availabel in PEST++GLM. The user manual provides descriptions of all of them. The following two specify options for halting PEST++GLM due to model-run failure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider a model to have failed if it takes 5 times the average model run time\n",
    "pst.pestpp_options[\"overdue_giveup_fac\"] = 5.0\n",
    "# attemp to repeat a failed model a maximum of 3 times; if it still fails, give up on that model run\n",
    "pst.pestpp_options[\"max_run_fail\"] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Re-write Control File and Run PEST++GLM\n",
    "\n",
    "Re-write the control file with the updated pestpp options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write PST \n",
    "case = 'freyberg_pp'\n",
    "pst.write(os.path.join(t_d,f\"{case}.pst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, deploy PEST++GLM in parallel.\n",
    "\n",
    "To speed up the process, you will want to distribute the workload across as many parallel agents as possible. Normally, you will want to use the same number of agents (or less) as you have available CPU cores. Most personal computers (i.e. desktops or laptops) these days have between 4 and 10 cores. Servers or HPCs may have many more cores than this. Another limitation to keep in mind is the read/write speed of your machines disk (e.g. your hard drive). PEST and the model software are going to be reading and writting lots of files. This often slows things down if agents are competing for the same resources to read/write to disk.\n",
    "\n",
    "The first thing we will do is specify the number of agents we are going to use.\n",
    "\n",
    "# Attention!\n",
    "\n",
    "You must specify the number which is adequate for ***your*** machine! Make sure to assign an appropriate value for the following `num_workers` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run glm in parallel\n",
    "num_workers = 10\n",
    "m_d = os.path.join('master_glm_2')\n",
    "pyemu.os_utils.start_workers(t_d,\"pestpp-glm\",f\"{case}.pst\",\n",
    "                           num_workers=num_workers,\n",
    "                           worker_root=\".\",\n",
    "                           master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see PEST++'s progress, switch to the command line window from which you launched this notebook. Wait until it has completed. This may take several minutes. Feel free to go make a cup of coffee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Postprocess\n",
    "\n",
    "During inversion PEST++ records a lot of usefull information in external files. We shall not go through each of these here (see the PEST++ user manual for detailed descriptions). The following are some common outputs a modeller is likely to inspect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A usefull way to track PEST's performance is to look at the evolution of Phi throughout the inversion. This is recorded in a file with the extension `*.iobj`  (e.g. `freyperg_pp.iobj`). You can read this file whilst PEST++ is working if you like. PEST++ will update the file after every iteration, thus it provides an easy way to keep an eye on the inversion progress.\n",
    "\n",
    "The file has a tabular format, making it easy to read as a `Pandas` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe \"df_obj\" that shows the contents of the pst file casename with the extension .iobj\n",
    "# .iobj = PEST++ output file that has the objective function by iteration \n",
    "df_obj = pd.read_csv(os.path.join(m_d, \"freyberg_pp.iobj\"),index_col=0)\n",
    "# echo out the dataframe\n",
    "df_obj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick and dirty plot to see the evolution of Phi per iteration. Ideally Phi should decrease for each sucessive iteration. If this plot bounces up and down it is often a sign of trouble. When using SVD-Assist, this often occurs if the inverse problem is highy nonlinear. In such cases it may be worth experimenting with fewer sucessive super parameter iterations and/or a greater number of super parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out the dataframe that was shown as a table above\n",
    "df_obj.loc[:,[\"total_phi\",\"model_runs_completed\"]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Residuals\n",
    "\n",
    "We may also wish to compare the measured versus simulated observation values obtained using the \"best-fit\" parameter set.\n",
    "\n",
    "PEST++ stores obsevration residuals in a `*.rei` file. When instantiating a `Pst` class from an existing control file, `pyemu` will attemp to read a corresponding `*.rei` file. Data from the rei file is stored in the `Pst.res` attribute as a `Pandas` `DataFrame`. This makes it easy to access and postprocess. We can also read in residuals after instatinating a `Pst` object by using the `Pst.set_res()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.set_res(os.path.join(m_d, 'freyberg_pp.rei'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then display 1-to-1 and residual plots for each of the non-zero weighted observation groups. This is a quick way to explore how well the model is able to replicate measured data.\n",
    "\n",
    "These plots you'll see often.  The left plot is a \"1:1\" plot that has simulated values on the x-axis and measured values on the y-axis; a perfect fit would be all circles on the black diagonal line.  \n",
    "\n",
    "The right plot has the residual (y-axis) compared to the observation magnitude (x-axis).  The closer the circle is to the black line the better the fit.  The mean residual is shown as a red line. Ideally this red line should plot on y=0. \n",
    "\n",
    "Scroll down through the plot below. How well does the model replicate historical data? Within the range of \"measurement error\"? Seems good overall!\n",
    "\n",
    "What about the residuals? are they evenly distributed around zero? No? Oh dear. This is a sign of bias. It means there is somethign wrong with the model or with the assumptions used for history matching. (In fact, because this is a synthetic model, we know what the cause is: under-parameterisation. Recall all the grid and pilot point parameters that we set as \"fixed\" during the \"glm part 1\" tutorial.) By looking for a \"good\" fit with our poor model, we may be inducing bias in our predictions. Perhaps we would be better served by accepting a worse fit...let's look at our forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pyemu's plot utilities to plot 1:1 line and the residuals as fxn of observation magnitude\n",
    "pyemu.plot_utils.res_1to1(pst);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Posterior Monte Carlo \n",
    "\n",
    "Because we included FOSM-based Monte Carlo options in the control file, when PEST++GLM concluded the inversion, it subsequently simulated an ensemble of parameters sampled from the linearized posterior parameter probability distribution. Observations from these simulations are recorded in the file named `freyberg_pp.post.obsen.csv`.\n",
    "Let's read this file and instantiate a `pyemu.ObservationEnsemble` object to make it easy to plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df=pd.read_csv(os.path.join(m_d,\"freyberg_pp.post.obsen.csv\"),index_col=0)\n",
    "oe = pyemu.ObservationEnsemble.from_dataframe(pst=pst,df=df)\n",
    "\n",
    "oe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a histogram of Phi achieved by this ensemble. Some have a good fit (low Phi). Others not so much. (The more linear the problem, the more likley that more will have a good fit.) So should we use all of them? Probably not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe.phi_vector.sort_values().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, as observations were weighted with the inverse of the standard deviation of measurement noise, we should accept a Phi equal to the number of nonzero observations. In practice, because of model error, we rarely reach the ideal value of Phi. For the purposes of this tutorial, we are going to arbitrarily take the 30 best realisations and use these as our \"posterior ensemble\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_pt = oe.loc[oe.phi_vector.sort_values().index[:30],:] #just take the 30 lowest phi realizations\n",
    "fig,ax = plt.subplots(1,1)\n",
    "oe.phi_vector.sort_values().hist(ax=ax,fc='g',alpha=0.5)\n",
    "oe_pt.phi_vector.sort_values().hist(ax=ax,fc=\"b\",alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our observations are time series. Let's take a look at how well time series from the ensemble of outputs match those of measured values. This allows us to get a quick look at where the ensmbles may not be capturing model behaviour well. \n",
    "\n",
    "Run the following cell to generate the plots. Each plot displays an obsevration time series. The red line are the measured values. The simulated values from each posterior realisation are displayed as blue lines.\n",
    "\n",
    "Overall it doesn't look too bad, but we do see some more indications of potential problems. The posterior ensemble fails to cover some of the measured data. Noticeably, measured values (red line) tend to be near the limits of the posterior ensemble - they should be at the centre. This may be due to bias (see residual plots from earlier on) and/or under-estimation of uncertainty (too few realisations? too few parameters?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_obs = pst.observation_data.loc[pst.nnz_obs_names,:].copy()\n",
    "\n",
    "for nz_group in pst.nnz_obs_groups:\n",
    "    # select obs values\n",
    "    nz_obs_group = nz_obs.loc[nz_obs.obgnme==nz_group,:]\n",
    "    nz_obs_group.time = nz_obs_group.time.astype(float)\n",
    "    nz_obs_group.sort_values('time', inplace=True)\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    # plot measured values\n",
    "    ax.plot(nz_obs_group.time,nz_obs_group.obsval,\"r-\")\n",
    "    # plot simulated values from post ensemble\n",
    "    [ax.plot(nz_obs_group.time,oe_pt.loc[r,nz_obs_group.obsnme],color=\"b\",lw=0.1,alpha=0.5) for r in oe_pt.index]\n",
    "\n",
    "    ax.set_title(nz_group)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. The Minimum Error Variance Parameter Field\n",
    "\n",
    "We have inspected the models' ability to replicate measured data. It is also a good idea to inspect how \"sensible\" are the obtained parameter values. A common easy check is to visualy inspect the spatial distirbution of hydrualic property parameters. One can often find unexpected insight from how parameter patterns emerge during calibration. \n",
    "\n",
    "First start by updating the control file with the calibrated parameter values using the `Pst.parrep()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parrep(parfile=os.path.join(m_d, 'freyberg_pp.par'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then write the parameter values to the model input files and run the model once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updates the model input files with parameter values\n",
    "pst.write_input_files(pst_path=m_d)\n",
    "\n",
    "# run the model forward run; this applies all the multipler paarameters, executes MODFLOW6 and MODPATH7 and then postprocess observations\n",
    "pyemu.os_utils.run('python forward_run.py', cwd=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `flopy` to load the model and plot some fo the parameters. For example, inspecting horizontal K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load simulation\n",
    "sim = flopy.mf6.MFSimulation.load(sim_ws=m_d, verbosity_level=0)\n",
    "# load flow model\n",
    "gwf = sim.get_model()\n",
    "gwf.npf.k.plot(colorbar=True)\n",
    "#gwf.sto.ss.plot()\n",
    "#gwf.sto.sy.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks a bit \"blotchy\", but not *too* bad...perhaps not particularily realistic, hey? In part, this is due to using the regularized Gauss Levenburg Marquardt option which tends to not look as \"smooth\" as using regularisation explicitly. \n",
    "\n",
    "What is more concerning are the localized areas of extreme parameter values, which may indicate parameters taking on compensatory roles - the plots of residuals and ensembles above hint at this as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quick check is to look for parameters which are at their bounds. This is often a sign of an incorrect model setup or poor parameterisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idealy, this should return an empty list\n",
    "pst.get_adj_pars_at_bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooo - that is pretty bad. Many parameters at their bounds. It does provide a possible explanation for the blotchiness in the spatial distribution of K - local values of K are taking on surrogate roles to compensate for parameters at their bounds. A clear sign of either a strucutral problem with the model or poor parameterisation. \n",
    "\n",
    "Ideally, a modeller would now go back and try and correct this. For this tutorial, we will not do so. We *know* why this happend because we engineered it. (However, if you have the inclination, why not try and see if you can come up with a solution? hint: in the \"glm part1\" notebook we induced under parameterisation, in particular for recharge parameters.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Forecast Uncertainty\n",
    "\n",
    "So far we have looked at the fit with measured data. But what we are realy intersted in are the forecasts. We instructed PEST++GLM to undertake both FOSM and FOSM-based Monte Carlo uncertainty analysis.\n",
    "\n",
    "We have alread loaded the ensemble of forecasts from the Monte Carlo analysis (`oe_pt`). Let's also load and plot the FOSM forecast results along side of the ensemble results. FOSM forecast results are recorded in the file named `freyberg_pp.pred.usum.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.read_csv(os.path.join(m_d,\"freyberg_pp.pred.usum.csv\"),index_col=0)\n",
    "f_df.index = f_df.index.map(str.lower)\n",
    "f_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make some plots of the forecast uncertainties obtained from FOSM (e.g. linear analysis) and from FOSM-based Monte Carlo (e.g. nonlinear analyis). This let's us investigate the validity of the assumed linear relation between parameters and forecasts (the forecast sensitivity vectors). If it holds up, then the FOSM posterior should cover the Monte Carlo posterior (note that we used very few realisations for Monte Carlo, so in our case this assumption is a bit shaky). \n",
    "\n",
    "We will superimpose the two posteriors on the *prior* forecast probability distribution to illustrate the uncertainty reduction gained from history matching. Because we are using a synthetic case, we know the true forecast values. So let's also plot these. If all went well, both posteriors should cover the truth.\n",
    "\n",
    "Run the next cell to generate plots for each of the forecasts referenced in the control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "fnames = pst.pestpp_options[\"forecasts\"].split(\",\")\n",
    "for forecast in fnames:\n",
    "    ax = plt.subplot(111)\n",
    "    # plot Monte Carlo posterior\n",
    "    oe_pt.loc[:,forecast].hist(ax=ax,color=\"b\",alpha=0.5,density=True)\n",
    "    # plot truth\n",
    "    ax.plot([obs.loc[forecast,\"obsval\"],obs.loc[forecast,\"obsval\"]],ax.get_ylim(),\"r\")\n",
    "    # plot FOSM prior\n",
    "    axt = plt.twinx()\n",
    "    x,y = pyemu.plot_utils.gaussian_distribution(f_df.loc[forecast,\"prior_mean\"],f_df.loc[forecast,\"prior_stdev\"])\n",
    "    axt.fill_between(x,0,y,facecolor=\"0.5\",alpha=0.25)\n",
    "    # plot FOSM posterior\n",
    "    x,y = pyemu.plot_utils.gaussian_distribution(f_df.loc[forecast,\"post_mean\"],f_df.loc[forecast,\"post_stdev\"])\n",
    "    axt.fill_between(x,0,y,facecolor=\"b\",alpha=0.25)\n",
    "    axt.set_ylim(0,axt.get_ylim()[1])\n",
    "    axt.set_yticks([])\n",
    "    #ax.set_xlim(oe_pt.loc[:,forecast].min() * .1,oe_pt.loc[:,forecast].max() * 1.5)\n",
    "    #axt.set_xlim(oe_pt.loc[:,forecast].min() * .10,oe_pt.loc[:,forecast].max() * 1.5)\n",
    "    ax.set_title(forecast)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, good news and bad news. \n",
    "\n",
    "The good news: the FOSM posterior (blue-shaded area) overlaps the Monte Carlo posterior (blue columns) for all forecasts. Good, the assumed linear parameter-forecast relation in the FOSM calculations seems to hold up - yeah! \n",
    "\n",
    "The bad news: some forecasts are not covered by either posterior. Dear oh dear - uncertainty analysis has failed! Why is this? Is it because of an imperfect model (often yes, but not in this case because our model is the same as our reality)? Is it because we are using too few parameters (in this case, yes, this is most likley the cause)? \n",
    "\n",
    "Now, here we have the luxury of knowing the \"truth\". In the real-world, we do not! We would not know that our forecast was wrong.\n",
    "\n",
    "So how do we reduce the potential for misrepresenting forecast uncertainty? We return to this issue in the \"freyberg pestpp-ies\" and the \"model error concepts\" notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xx. Final Remarks\n",
    "\n",
    "In this tutorial we demonstrated the steps for deploying PEST++GLM for parallelized highly-parameterized inversion using `pyemu`. We discussed some common settings and options available in PEST++GLM and employed functionality available in `pyemu` to explore the outcomes of inversion. We noted some indicators of bias and/or structural error, however other tutorials delve into these topics in greater detail. \n",
    "\n",
    "We encourage readers to use this notebook to explore the effects of alternative PEST++GLM settings. The model is relatively fast and enables experimentation. Why not explore the implications of not using SVD-Assist? Or different observation weighting schemes? Or more/other parameters? What are the costs/benefits?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
