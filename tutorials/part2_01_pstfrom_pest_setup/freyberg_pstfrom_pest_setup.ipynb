{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the PEST(++) interface around the modified Freyberg model\n",
    "\n",
    "In this notebook, we will construct a complex model-independent (nonintrusive) interface around an existing `MODFLOW6` model using `pyEMU`. We assume that the reader is at least partially familiar with PEST(++) file formats and working philosophy. \n",
    "\n",
    "The modified Freyberg groundwater flow model has been constructed and is described in a previous notebook from this series (see [\"intro freyberg model\"](../part0_02_intro_to_freyberg_model/intro_freyberg_model.ipynb)). We will construct the entire PEST(++) interface from scratch here. This setup will be built upon in subsequent tutorials. \n",
    "\n",
    "We will rely heavily on the `pyemu.PstFrom` class. Although here we employ it with a `MODFLOW6` model, `PstFrom` is designed to be general and software-independent (mostly). Some features are only available for `MODFLOW` models (e.g. `SpatialReference`).\n",
    "\n",
    "The `PstFrom` class automates the construction of high-dimensional PEST(++) interfaces with all the bells and whistles. It provides easy-to-use functions to process model input and output files into PEST(++) datasets. It can assist with setting up spatiotemporally varying parameters. It handles the generation of geostatistical prior covariance matrices and ensembles. It automates writing a \"model run\" script. It provides tools to add custom pre- and post-processing functions to this script. It makes adding tweaks and fixes to the PEST(++) interface a breeze. All of this from the comfort of your favorite Python IDE.\n",
    "\n",
    "You may also wish to go through the [\"intro to pyemu\"](../part0_intro_to_pyemu/intro_to_pyemu.ipynb) and [\"pstfrom sneakpeak\"](../part1_02_pest_setup/pstfrom_sneakpeak.ipynb) notebooks to get an overview of `pyemu` and a slimmed down `PstFrom` example.\n",
    "\n",
    "During this tutorial we are going to construct a PEST dataset. Amongst other things, we will demonstrate:\n",
    " - how to add observations & parameters from model output & input files;\n",
    " - how to add pre- and post-processing functions to the \"model run\" script;\n",
    " - how to generate geostatistical structures for spatially and temporally correlated parameters;\n",
    " - how to edit parameter/observation data sections;\n",
    " - how to generate a prior parameter covariance matrix and prior parameter ensemble;\n",
    "\n",
    "First, let's get our model files and sort out some admin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Admin & Organize Folders\n",
    "First some admin. Load the dependencies and organize model folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import platform\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "# for this course we use locally stored version of pyemu and flopy to avoid version conflicts\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "# herebedragons.py contains a series of utility functions that help us manage the tutorial repository\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be calling a few external programs throughout this tutorial. Namely, MODFLOW 6 and PESTPP-GLM. For the purposes of the tutorial(s), we have included executables in the tutorial repository. They are in the `bin_new` folder, organized by operating system and will be programmatically copied into the working dirs as needed. \n",
    "\n",
    "Some may prefer that executables be located in a folder that is cited in your computer's PATH environment variable. Doing so allows you to run them from a command prompt open to any other folder without having to include the full path to these executables in the command to run them. \n",
    "\n",
    "However, in situations where someone has several active projects and each may use different versions of compiled binary codes, this may not be practical. In such cases, we can simply place the executables in the folder from which they will be executed.  So, let's copy the necessary executables into our working folder using a simple helper function:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy the original model folder into a new working directory, just to ensure we don't mess up the base files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder containing original model files\n",
    "org_d = os.path.join('..', '..', 'models', 'monthly_model_files_1lyr_newstress')\n",
    "\n",
    "# a dir to hold a copy of the org model files\n",
    "tmp_d = os.path.join('freyberg_mf6')\n",
    "\n",
    "if os.path.exists(tmp_d):\n",
    "    shutil.rmtree(tmp_d)\n",
    "shutil.copytree(org_d,tmp_d)\n",
    "\n",
    "# get executables\n",
    "hbd.prep_bins(tmp_d)\n",
    "# get dependency folders\n",
    "hbd.prep_deps(tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you inspect the model folder, you will see that all the `MODFLOW6` model files have been written \"externally\". This is key for working with the `PstFrom` class (or with PEST(++) in general, really). Essentially, all pertinent model inputs have been written as independent files in either array or list format. This makes it easier for us to programmatically access and re-write the values in these files.\n",
    "\n",
    "Array files contain a data type (usually floating points). List files will have a few columns that contain index information and then columns of floating point values (they have a tabular format; think `.csv` files or DataFrames). The `PstFrom` class provides methods for processing these file types into a PEST(++) dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need just a tiny bit of info about the spatial discretization of the model - this is needed to work out separation distances between parameters to build a geostatistical prior covariance matrix later.\n",
    "\n",
    "Here we will load the flopy sim and model instance just to help us define some quantities later - flopy is ***not required*** to use the `PstFrom` class. ***Neither is MODFLOW***. However, at the time of writing, support for `SpatialReference` to spatially locate parameters is limited to structured grid models.\n",
    "\n",
    "Load the simulation. Run it once to make sure it works and to ***make sure that model output files are in the folder***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load simulation\n",
    "sim = flopy.mf6.MFSimulation.load(sim_ws=tmp_d)\n",
    "# load flow model\n",
    "gwf = sim.get_model()\n",
    "\n",
    "# run the model once to make sure it works\n",
    "pyemu.os_utils.run(\"mf6\",cwd=tmp_d)\n",
    "# run modpath7\n",
    "pyemu.os_utils.run(r'mp7 freyberg_mp.mpsim', cwd=tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Reference\n",
    "Now we can instantiate a `SpatialReference`. This will later be passed to `PstFrom` to assist with spatially locating parameters (e.g. pilot points and/or cell-by-cell parameters).  You can also use the flopy `modelgrid` class instance that is attached to the simulation, but `SpatialReference` is cleaner and faster for structured grids..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = pyemu.helpers.SpatialReference.from_namfile(\n",
    "        os.path.join(tmp_d, \"freyberg6.nam\"),\n",
    "        delr=gwf.dis.delr.array, delc=gwf.dis.delc.array)\n",
    "sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate PstFrom\n",
    "\n",
    "Now we can start to construct the PEST(++) interface by instantiating a `PstFrom` class instance. There are a few things that we need to specify up front:\n",
    "\n",
    " - the folder in which we currently have model files (e.g. `tmp_d`). PstFrom will copy all the files from this directory into a new \"template\" folder.\n",
    " - **template folder**: this is a folder in which the PEST dataset will be constructed - this folder will hold the model files plus all of the files needed to run PEST(++). This folder/dataset will form the template for subsequent deployment of PEST(++).\n",
    " - **longnames**: for backwards compatibility with PEST and PEST_HP (i.e. non-PEST++ versions), which have upper limits to parameter/observation names (PEST++ does not). Setting this value to False is only recommended if required. \n",
    " - Whether the model is `zero based` or not.\n",
    " - (optional) the **spatial reference**, as previously discussed. This is only required if using `pyEMU` to define parameter spatial correlation. Alternatively, you can define these yourself or use utilities available in the PEST-suite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a template directory (i.e. the PstFrom working folder)\n",
    "template_ws = os.path.join(\"freyberg6_template\")\n",
    "start_datetime=\"1-1-2008\"\n",
    "# instantiate PstFrom\n",
    "pf = pyemu.utils.PstFrom(original_d=tmp_d, # where the model is stored\n",
    "                            new_d=template_ws, # the PEST template folder\n",
    "                            remove_existing=True, # ensures a clean start\n",
    "                            longnames=True, # set False if using PEST/PEST_HP\n",
    "                            spatial_reference=sr, # the spatial reference we generated earlier\n",
    "                            zero_based=False, # does the MODEL use zero based indices? For example, MODFLOW does NOT\n",
    "                            start_datetime=start_datetime, # required when specifying temporal correlation between parameters\n",
    "                            echo=False) # to stop PstFrom from writting lots of infromation to the notebook; experiment by setting it as True to see the difference; usefull for troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(template_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that when `PstFrom` is instantiated, it starts by copying the `original_d` to the `new_d`. Sweet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "We now have a `PstFrom` instance assigned to the variable `pf`. For now it is only an empty container to which we can start adding \"observations\", \"parameters\" and other bits and bobs.\n",
    "\n",
    "Let's start with observations because they are easier. `MODFLOW6` makes life even easier by recording observations in nicely organized .csv files. Isn't that a peach!\n",
    "\n",
    "#### Freyberg Recap\n",
    "As you may recall from the \"*intro to Freyberg*\" tutorial, the model is configured to record time series of head at observation wells, and flux at three locations along the river. These are recorded in external .csv files named `heads.csv` and `sfr.csv`, respectively. You should be able to see these files in the model folder.\n",
    "\n",
    "Recall that each .csv houses records of observation time-series. Outputs are recorded for each simulated stress-period. The model starts with a single steady-state stress-period, followed by 24 monthly transient stress-periods. The steady-state and first 12 transient stress-periods simulate the history-matching period. The last 12 transient stress periods simulate future conditions (i.e. the prediction period)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the output csv file names\n",
    "for i in gwf.obs:\n",
    "    print(i.output.obs_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the 'heads.csv' file. First load it as a DataFrame to take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(template_ws,\"heads.csv\"),index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are many columns, one for each observation site. Conveniently, * *cough* * they are named according to the cell layer, row and column. \n",
    "\n",
    "The values in the *.csv* file were generated by running the model. (***IMPORTANT!***) However, `PstFrom` assumes that values in this file are the *target* observation values, and they will be used to populate the PEST(++) dataset.  This lets the user quickly verify that the `PstFrom` process reproduces the same model output files - an important thing to test!\n",
    "\n",
    "Now, you can and should change the observation values later on for the quantities that correspond to actual observation data.  This is the standard workflow when using `PstFrom` because it allows users to separate the PEST interface setup from the always-important process of setting observation values and weights. We address this part of the workflow in a separate tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Observations\n",
    "\n",
    "First, we will use the `PstFrom.add_observations()` method to add observations to our `pf` object. This method can use ***list-type*** files, where the data are organized in column/tabular format with one or more index columns and one or more data columns.  This method can also use ***array-type*** files, where the data are organized in a 2-D array structure (we will see this one later...)\n",
    "\n",
    "We are going to tell `pf` which columns of this file contain observations. Values in these columns will be assigned to *observation values*.\n",
    "\n",
    "We can also inform it if there is an index column (or columns). Values in this column will be included in the *observation names*. \n",
    "\n",
    "We could also specify which rows to include as observations. But observations are free...so why not keep them all! \n",
    "\n",
    "Let's add observations from `heads.csv`. The first column of this file records the time at which the value is simulated. Let's use that as the index column (this becomes useful later on to post-process results). We want all other columns as observation values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df = pf.add_observations(\"heads.csv\", # the model output file to read\n",
    "                            insfile=\"heads.csv.ins\", # optional, the instruction file name\n",
    "                            index_cols=\"time\", # column header to use as index; can also use column number (zero-based) instead of the header name\n",
    "                            use_cols=list(df.columns.values), # names of columns that include observation values; can also use column number (zero-based) instead of the header name\n",
    "                            prefix=\"hds\") # prefix to all observation names; choose something logical and easy to find. We use it later on to select observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect what we just created. \n",
    "\n",
    "We can see that the `.add_observations()` method returned a dataframe with lots of useful info: \n",
    "\n",
    " - the observation names that were formed (see `obsnme` column); note that these include lots of useful metadata like the column name, index value and so on;\n",
    " - the values that were read from `heads.csv` (see `obsval` column); \n",
    " - some generic weights and group names; note that observations are grouped according to the column of the model output .csv. Alternatively, we could have specified a list of observation group names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, no PEST *control file* has been created, we have simply prepared to add these observations to the control file later. Everything is still only stored in memory. However, a PEST *instruction* file has been created in the template folder (`template_ws`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".ins\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blimey, wasn't that easy? Automatically monitoring thousands of model output quantities as observations into a PEST dataset becomes a breeze!\n",
    "\n",
    "Let's quickly do the same thing for the SFR observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(template_ws, \"sfr.csv\"), index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the observations to pf\n",
    "sfr_df = pf.add_observations(\"sfr.csv\", # the model output file to read\n",
    "                            insfile=\"sfr.csv.ins\", # optional, the instruction file name\n",
    "                            index_cols=\"time\", # column header to use as index; can also use column number (zero-based) instead of the header name\n",
    "                            use_cols=list(df.columns.values), # names of columns that include observation values; can also use column number (zero-based) instead of the header name\n",
    "                            prefix=\"sfr\") # prefix to all observation names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to add observations of particle travel time and status. Unfortunately the file written by MODPATH7 is not easily ingestible by `PstFrom`. So we are going to need to \"manually\" construct an instruction file. We could add that now with the `PstFrom.add_observations_from_ins()` method, but we will wait and add these after constructing the `Pst` object - soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "The `PstFrom.add_parameters()` method reads model input files and adds parameters to the PEST(++) dataset. Parameterization can be configured in several ways. \n",
    "\n",
    " - model input files can be in array or list format;\n",
    " - parameters can be setup as different \"types\". Each value in model input files can (1) each be a separate parameter (\"grid\" scale parameters), (2) be grouped into \"zones\" or (3) all be treated as a single parameter (\"constant\" type). Alternatively, (4) parameters can be assigned to pilot points, from which individual parameter values are subsequently interpolated. `PstFrom` adds the relevant pre-processing steps to assign parameter values directly into the \"model run\" script.\n",
    " - parameter values can be setup as \"direct\", \"multiplier\" or \"addend\". This means the \"parameter value\" which PEST(++) sees can be (1) the same value the model sees, (2) a multiplier on the value in the existing/original model input file, or (3) a value which is added to the value in the existing/original model input file. This is very nifty and allows for some pretty advanced parameterization schemes by allowing mixtures of different types of parameters. `PstFrom` is designed to preferentially use parameters setup as multipliers (that is the default parameter type). This lets us preserve the existing model inputs and treat them as the mean of the prior parameter distribution. Once again, relevant preprocessing scripts are automatically added to the \"model run\" script (discussed later) so that the multiplicative and additive parameterization process is not something the user has to worry about.\n",
    "\n",
    "\n",
    "#### Freyberg Recap\n",
    "\n",
    "As discussed, all model inputs are stored in external files. Some are arrays. Others are lists. Recall that our model has 1 layer. It is transient. Hydraulic properties (Kh, Kv, Ss, Sy) vary in space. Recharge varies over both space and time. We have GHBs, SFR and WEL boundary conditions. GHB parameters are constant over time, but vary spatially. SFR inflow varies over time. Pumping rates of individual wells are uncertain in space and and time.\n",
    "\n",
    "All of these have some degree of spatial and/or temporal correlation.\n",
    "\n",
    "#### Geostatistical Structures\n",
    "\n",
    "Parameter correlation plays a role in (1) regularization when giving preference to the emergence of patterns of spatial heterogeneity and (2) when specifying the prior parameter probability distribution (which is what regularization is enforcing!). Since we are all sophisticated and recognize the importance of expressing spatial and temporal uncertainty (e.g. heterogeneity) in the model inputs (and the corresponding spatial correlation in those uncertain inputs), let's use geostatistics to express uncertainty. To do that we need to define \"geostatistical structures\". \n",
    "\n",
    "For the sake of this tutorial, let's assume that heterogeneity in all spatially distributed parameters share the same statistical characteristics. Likewise for temporally varying parameters. We will therefore only construct two geostatistical structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponential variogram for spatially varying parameters\n",
    "v_space = pyemu.geostats.ExpVario(contribution=1.0, #sill\n",
    "                                    a=1000, # range of correlation; length units of the model. In our case 'meters'\n",
    "                                    anisotropy=1.0, # name says it all\n",
    "                                    bearing=0.0 # angle in degrees East of North corresponding to anisotropy ellipse\n",
    "                                    )\n",
    "\n",
    "# geostatistical structure for spatially varying parameters\n",
    "grid_gs = pyemu.geostats.GeoStruct(variograms=v_space, transform='log') \n",
    "\n",
    "# plot the gs if you like:\n",
    "_ = grid_gs.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponential variogram for time-varying parameters\n",
    "v_time = pyemu.geostats.ExpVario(contribution=1.0, #s ill\n",
    "                                    a=60, # range of correlation; length time units (days)\n",
    "                                    anisotropy=1.0, # do not change for 1-D time\n",
    "                                    bearing=0.0 # do not change for 1-D time\n",
    "                                    )\n",
    "\n",
    "# geostatistical structure for time-varying parameters\n",
    "temporal_gs = pyemu.geostats.GeoStruct(variograms=v_time, transform='none') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Parameters\n",
    "\n",
    "Let's start by adding parameters of hydraulic properties that vary in space (but not time) and which are housed in array-type files (e.g. Kh, Kv, Ss, Sy). We will start by demonstrating step-by-step for Kh.\n",
    "\n",
    "First, find all the external array files that contain Kh values. In our case, these are the files with \"npf_k_\" in the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"npf_k_\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up multiple spatial scales of parameters for Kh. To do this we will use three of the parameter \"types\" described above. The coarse scale will be a `constant` single value for each array. The medium scale will use `pilot points`. The finest scale will use parameters as the `grid` scale (a unique parameter for each model cell!).\n",
    "\n",
    "Each scale of parameters will work with the others as multipliers with the existing Kh arrays. (This all happens at runtime as part of the \"model run\" script.) Think of the scales as dials that PEST(++) can turn to improve the fit. The \"coarse\" scale is one big dial that allows PEST to move everything at once - that is, change the mean of the entire Kh array. The \"medium\" dials are few (but not too many) that allow PEST to adjust broad areas, but not making everything move. The \"fine\" scales are lots of small dials that allow PEST(++) to have very detailed control, tweaking parameter values within very small areas. \n",
    "\n",
    "However, because we are working with parameter `multipliers`, we will need to specify two sets of parameter bounds: \n",
    " - `upper_bound` and `lower_bound` are the standard control file bounds (the bounds on the parameters that PEST sees), while\n",
    " - `ult_ubound` and `ult_lbound` are bounds that are applied at runtime to the resulting (multiplied out) model input array that MODFLOW reads. \n",
    " \n",
    "Since we are using sets of multipliers, it is important to make sure we keep the resulting model input arrays within the range of realistic values.\n",
    "\n",
    "#### Array Files\n",
    "\n",
    "We will first demonstrate step-by-step for `freyberg6.npf_k_layer1.txt`. We will start with grid scale parameters. These are multipliers assigned to each individual value in the array.\n",
    "\n",
    "We start by getting the `idomain` array. As our model has inactive cells, this helps us avoid adding unnecessary parameters. It is also required later when generating pilot points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the IDOMAIN array; in our case we only have one layer\n",
    "ib = gwf.dis.idomain.array[0]\n",
    "plt.imshow(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'freyberg6.npf_k_layer1.txt'\n",
    "\n",
    "# grid (fine) scale parameters\n",
    "df_gr = pf.add_parameters(f,\n",
    "                zone_array=ib, # as we have inactive model cells, we can avoid assigning these as parameters\n",
    "                par_type=\"grid\", # specify the type, these will be unique parameters for each cell\n",
    "                geostruct=grid_gs, # the gestatisical structure for spatial correlation \n",
    "                par_name_base=f.split('.')[1].replace(\"_\",\"\")+\"gr\", # specify a parameter name base that allows us to easily identify the filename and parameter type. \"_gr\" for \"grid\", and so forth.\n",
    "                pargp=f.split('.')[1].replace(\"_\",\"\")+\"gr\", # likewise for the parameter group name\n",
    "                lower_bound=0.2, upper_bound=5.0, # parameter lower and upper bound\n",
    "                ult_ubound=100, ult_lbound=0.01 # The ultimate bounds for multiplied model input values. Here we are stating that, after accounting for all multipliers, Kh cannot exceed these values. Very important with multipliers\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As when adding observations,  `pf.add_parameters()` returns a dataframe. Take a look. You may recognize a lot of the information that appears in a PEST `*parameter data` section. All of this is still only housed in memory for now. We will write the PEST control file later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `add_parameters()` call also wrote a template file that PEST(++) will use to populate the multiplier array at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember!  no PEST control file has been made yet. `PstFrom` is simply preparing to make a control file later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add pilot point (medium scale) multiplier parameters to the same model input file. These multipliers are assigned to pilot points, which are subsequently interpolated to values in the array.\n",
    "\n",
    "You can add pilot points in two ways:\n",
    "\n",
    "1. `PstFrom` can generate them for you on a regular grid or \n",
    "2. you can supply `PstFrom` with existing pilot point location information in the form of a dataframe or a point-coverage shapefile. \n",
    "\n",
    "When you change `par_type` to \"pilotpoints\", by default, a regular grid of pilot points is set up using a default `pp_space` value of 10 (which is every 10th row and column). You can change this spacing by passing an integer to `pp_space` (as demonstrated below). \n",
    "\n",
    "Alternatively you can specify a filename or dataframe with pilot point locations. If you supply `pp_space` as a `str` it is assumed to be a filename. The extension is the guide: \".csv\" for dataframe, \".shp\" for shapefile (point-type). Anything else and the file is assumed to be a pilot points file type. The dataframe (or .csv file) must have \"name\", \"x\", and \"y\" as columns - it can have more, but must have those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pilot point (medium) scale parameters\n",
    "df_pp = pf.add_parameters(f,\n",
    "                    zone_array=ib,\n",
    "                    par_type=\"pilotpoints\",\n",
    "                    geostruct=grid_gs,\n",
    "                    par_name_base=f.split('.')[1].replace(\"_\",\"\")+\"pp\",\n",
    "                    pargp=f.split('.')[1].replace(\"_\",\"\")+\"pp\",\n",
    "                    lower_bound=0.2,upper_bound=5.0,\n",
    "                    ult_ubound=100, ult_lbound=0.01,\n",
    "                    pp_space=5) # `PstFrom` will generate a unifrom grid of pilot points in every 4th row and column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(4,6))\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.pcolormesh(sr.xcentergrid, sr.ycentergrid,ib)\n",
    "ax.scatter(df_pp.x,df_pp.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, add the constant (coarse) parameter multiplier. This is a single multiplier value applied to all values in the array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant (coarse) scale parameters\n",
    "df_cst = pf.add_parameters(f,\n",
    "                    zone_array=ib,\n",
    "                    par_type=\"constant\",\n",
    "                    geostruct=grid_gs,\n",
    "                    par_name_base=f.split('.')[1].replace(\"_\",\"\")+\"cn\",\n",
    "                    pargp=f.split('.')[1].replace(\"_\",\"\")+\"cn\",\n",
    "                    lower_bound=0.2,upper_bound=5.0,\n",
    "                    ult_ubound=100, ult_lbound=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see three template files have been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to navigate to the `template_ws` and inspect these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be repeating this for each parameter type, so let's write a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mult_pars(f, lb=0.2, ub=5.0, ulb=0.01, uub=100, add_coarse=True):\n",
    "    if isinstance(f,str):\n",
    "        base = f.split(\".\")[1].replace(\"_\",\"\")\n",
    "    else:\n",
    "        base = f[0].split(\".\")[1]\n",
    "    # grid (fine) scale parameters\n",
    "    pf.add_parameters(f,\n",
    "                    zone_array=ib,\n",
    "                    par_type=\"grid\", # specify the type, these will be unique parameters for each cell\n",
    "                    geostruct=grid_gs, # the geostatisical structure for spatial correlation \n",
    "                    par_name_base=base+\"gr\", # specify a parameter name base that allows us to easily identify the filename and parameter type. \"_gr\" for \"grid\", and so forth.\n",
    "                    pargp=base+\"gr\", # likewise for the parameter group name\n",
    "                    lower_bound=lb, upper_bound=ub, # parameter lower and upper bound\n",
    "                    ult_ubound=uub, ult_lbound=ulb # The ultimate bounds for multiplied model input values. Here we are stating that, after accounting for all multipliers, Kh cannot exceed these values. Very important with multipliers\n",
    "                    )\n",
    "                    \n",
    "    # pilot point (medium) scale parameters\n",
    "    pf.add_parameters(f,\n",
    "                        zone_array=ib,\n",
    "                        par_type=\"pilotpoints\",\n",
    "                        geostruct=grid_gs,\n",
    "                        par_name_base=base+\"pp\",\n",
    "                        pargp=base+\"pp\",\n",
    "                        lower_bound=lb, upper_bound=ub,\n",
    "                        ult_ubound=uub, ult_lbound=ulb,\n",
    "                        pp_space=5) # `PstFrom` will generate a unifrom grid of pilot points in every 4th row and column\n",
    "    if add_coarse==True:\n",
    "        # constant (coarse) scale parameters\n",
    "        pf.add_parameters(f,\n",
    "                            zone_array=ib,\n",
    "                            par_type=\"constant\",\n",
    "                            geostruct=grid_gs,\n",
    "                            par_name_base=base+\"cn\",\n",
    "                            pargp=base+\"cn\",\n",
    "                            lower_bound=lb, upper_bound=ub,\n",
    "                            ult_ubound=uub, ult_lbound=ulb)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's speed through the other array parameter files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Kv\n",
    "tag = \"npf_k33\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "for f in files:\n",
    "    add_mult_pars(f, lb=0.2, ub=5.0, ulb=1e-4, uub=1e3)\n",
    "\n",
    "# for Ss\n",
    "tag = \"sto_ss\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "# only for layer 2 and 3; we aren't monsters\n",
    "for f in files[1:]: \n",
    "    add_mult_pars(f, lb=0.2, ub=5.0, ulb=1e-7, uub=1e-3)\n",
    "\n",
    "# For Sy\n",
    "tag = \"sto_sy\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "# only for layer 1\n",
    "f = files[0]\n",
    "add_mult_pars(f, lb=0.2, ub=5.0, ulb=0.01, uub=0.4)\n",
    "\n",
    "# For porosity\n",
    "tag = \"ne_\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "for f in files: \n",
    "    add_mult_pars(f, lb=0.2, ub=5.0, ulb=0.01, uub=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([f for f in os.listdir(template_ws) if f.endswith(\".tpl\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom!  We just conquered property parameterization in a big way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial and Temporal Correlation\n",
    "\n",
    "Now, you may be thinking \"shouldn't recharge have temporal correlation as well?\". \n",
    "\n",
    "Damn straight it should. Now, this requires a little trickery because native handling in spatiotemporal correlation is hard to do.  So what we are going to do is split this correlation into two setups of multiplier parameters.  One set of parameters will be constant in space but vary (and be correlated) in time.  The other set of multiplier parameters will be constant in time but vary (and be correlated) in space.  Since both of these sets of parameters are multipliers, we implicitly represent the concept that recharge is uncertain and correlated in both space and time.  Easy as pie!\n",
    "\n",
    "First we need to construct a container of stress period datetimes. (This relies on specifying the start_datetime argument when instantiating `PstFrom`.) These datetime values will specify the position of parameters on the time-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up a container of stress period start datetimes - this will\n",
    "# be used to specify the datetime of each multipler parameter\n",
    "\n",
    "dts = pd.to_datetime(start_datetime) + pd.to_timedelta(np.cumsum(sim.tdis.perioddata.array[\"perlen\"]),unit='d')\n",
    "\n",
    "dts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use the same parameter group name (`pargp`) and same geostruct, `PstFrom` will treat parameters set up across different calls to `add_parameters()` as correlated - ***WARNING*** do not try to express spatial and temporal correlation together - as discussed above, #badtimes.  In this case, we want to express temporal correlation in the recharge multiplier parameters that are \"constant\" type in space so that there is one recharge multiplier parameter for each stress period that shares a parameter group name across different calls to `add_parameters`. So, we use the same parameter group names for each stress period data file, and specify the `datetime` and `geostruct` arguments.\n",
    "\n",
    "Including temporal correlation introduces an additional challenge. Interpolation between points that share a common coordinate creates all types of trouble. We are going to have many parameters during each stress period (a single point on the time-axis). To get around this challenge we need to be a bit sneaky.\n",
    "\n",
    "\n",
    "First, we will apply the multiple *spatial* scales of parameter multipliers (`constant`, `pilot point` and `grid`) as we did for hydraulic properties.  We do this for all recharge files at once, which tells `PstFrom` to broadcast (e.g. share) the same parameters for all of those files: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Recharge; \n",
    "tag = \"rch_recharge\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "sp = [int(f.split(\".\")[1].split('_')[-1]) for f in files]\n",
    "d = {s:f for s,f in zip(sp,files)}\n",
    "sp.sort()\n",
    "files = [d[s] for s in sp]\n",
    "print(files)\n",
    "# the spatial multiplier parameters; just use the same function\n",
    "add_mult_pars(files, lb=0.2, ub=5.0, ulb=0, uub=1e-3, add_coarse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([f for f in os.listdir(template_ws) if f.endswith(\".tpl\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will assign an additional `constant` multiplier parameter for each recharge stress-period file (so, a single multiplier for all recharge parameters for each stress period). We will specify temporal correlation for these `constant` multipliers. These will all have the same parameter group name, as discussed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:   \n",
    "    # multiplier that includes temporal correlation\n",
    "    # get the stress period number from the file name\n",
    "    kper = int(f.split('.')[1].split('_')[-1]) - 1  \n",
    "    # add the constant parameters (with temporal correlation)\n",
    "    pf.add_parameters(filenames=f,\n",
    "                    zone_array=ib,\n",
    "                    par_type=\"constant\",\n",
    "                    par_name_base=f.split('.')[1]+\"tcn\",\n",
    "                    pargp=f.split('.')[1]+\"tcn\",\n",
    "                    lower_bound=0.5, upper_bound=1.5,\n",
    "                    ult_ubound=1e-3, ult_lbound=0,\n",
    "                    datetime=dts[kper], # this places the parameter value on the \"time axis\"\n",
    "                    geostruct=temporal_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Files\n",
    "\n",
    "Adding parameters from list-type files follows similar principles. As with observation files, they must be tabular. Certain columns are specified as index columns and are used to populate parameter names, as well as provide the parameters' spatial location. Other columns are specified as containing parameter values. \n",
    "\n",
    "Parameters can be `grid` or `constant`. As before, values can be assigned `directly`, as `multipliers` or as `additives`.\n",
    "\n",
    "We will demonstrate for the boundary-condition input files. \n",
    "\n",
    "Starting off with GHBs. Let's inspect the folder. As you can see, there is a single input file (here we assume GHB parameters do not vary over time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"ghb_stress_period_data\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these boundaries are likely to be very influential, we want to include a robust representation of their uncertainty - both head and conductance and at multiple scales.  \n",
    "\n",
    "Let's parameterize both GHB conductance and head:\n",
    "\n",
    " - For conductance, we shall use two scales of `multiplier` parameters (`constant` and `grid`).\n",
    "\n",
    " - For heads, multipliers are not ideal. Instead we will use `additive` parameters. Again, with a coarse and fine scale.\n",
    "\n",
    " **ATTENTION!** \n",
    " \n",
    " Additive parameters by default get assigned an initial parameter value of zero. This can be problematic later on when computing the derivatives. Be sure to either apply a parameter offset, or use \"absolute\" increment types in the parameter group section (we will implement the latter option further on in the current tutorial.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"ghb_stress_period_data\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "\n",
    "for f in files:\n",
    "    # constant and grid scale multiplier conductance parameters\n",
    "    name = 'ghbcond'\n",
    "    pf.add_parameters(f,\n",
    "                        par_type=\"grid\",\n",
    "                        geostruct=grid_gs,\n",
    "                        par_name_base=name+\"gr\",\n",
    "                        pargp=name+\"gr\",\n",
    "                        index_cols=[0,1,2], #column containing lay,row,col\n",
    "                        use_cols=[4], #column containing conductance values\n",
    "                        lower_bound=0.1,upper_bound=10.0)\n",
    "    pf.add_parameters(f,\n",
    "                        par_type=\"constant\",\n",
    "                        geostruct=grid_gs,\n",
    "                        par_name_base=name+\"cn\",\n",
    "                        pargp=name+\"cn\",\n",
    "                        index_cols=[0,1,2],\n",
    "                        use_cols=[4],  \n",
    "                        lower_bound=0.1,upper_bound=10.0,\n",
    "                        ult_lbound=0.01, ult_ubound=100) #absolute limits\n",
    "\n",
    "    # constant and grid scale additive head parameters\n",
    "    name = 'ghbhead'\n",
    "    pf.add_parameters(f,\n",
    "                        par_type=\"grid\",\n",
    "                        geostruct=grid_gs,\n",
    "                        par_name_base=name+\"gr\",\n",
    "                        pargp=name+\"gr\",\n",
    "                        index_cols=[0,1,2],\n",
    "                        use_cols=[3],   # column containing head values\n",
    "                        lower_bound=-2.0,upper_bound=2.0,\n",
    "                        par_style=\"a\", # specify additive parameter\n",
    "                        transform=\"none\", # specify not log-transform\n",
    "                        ult_lbound=32.5, ult_ubound=42) #absolute limits; make sure head is never lower than the bottom of layer1\n",
    "    pf.add_parameters(f,\n",
    "                        par_type=\"constant\",\n",
    "                        geostruct=grid_gs,\n",
    "                        par_name_base=name+\"cn\",\n",
    "                        pargp=name+\"cn\",\n",
    "                        index_cols=[0,1,2],\n",
    "                        use_cols=[3],\n",
    "                        lower_bound=-2.0,upper_bound=2.0, \n",
    "                        par_style=\"a\", \n",
    "                        transform=\"none\",\n",
    "                        ult_lbound=32.5, ult_ubound=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy peasy.\n",
    "\n",
    "Now, this will make some people uncomfortable, but how well do we really ever know historic water use flux rates in space and in time? hmmm, not really! And just a little uncertainty in historic water use can result in large changes in simulated water levels...So let's add parameters to represent that uncertainty in the model inputs.\n",
    "\n",
    "For wells it may not (or it may...) make sense to include spatial correlation. Here we will assume temporal correlation - its reasonable that pumping rates today will be similar to pumping rates yesterday. \n",
    "\n",
    "Pumping rates for different stress periods are in separate files. We will call `.add_parameters()` for each file. But we want to specify correlation between parameters in different files. As explained above for recharge, we do this with the parameter group name.\n",
    "\n",
    "OK, let's get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, including temporal correlation introduces an additional challenge. We use the same approach described for recharge parameters:\n",
    "\n",
    " - First, we will assign a `constant` multiplier parameter for each WEL stress-period file (so, a single multiplier for all well pumping rates for each stress period). We will specify temporal correlation for these `constant` multipliers.\n",
    "\n",
    " - Then, we will also have `grid` type multiplier parameters for each WEL stress period file (so, multipliers for individual well pumping rate during each stress period). These will not include (temporal) correlation. (We could in principle include spatial correlation here if we wanted to; but let's not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(template_ws) if \"wel_stress_period_data\" in f and f.endswith(\".txt\")]\n",
    "sp = [int(f.split(\".\")[1].split('_')[-1]) for f in files]\n",
    "d = {s:f for s,f in zip(sp,files)}\n",
    "sp.sort()\n",
    "files = [d[s] for s in sp]\n",
    "\n",
    "for f in files:\n",
    "    # get the stress period number from the file name\n",
    "    kper = int(f.split('.')[1].split('_')[-1]) - 1  \n",
    "    \n",
    "    # add the constant parameters (with temporal correlation)\n",
    "    pf.add_parameters(filenames=f,\n",
    "                        index_cols=[0,1,2], # columns that specify cell location\n",
    "                        use_cols=[3],       # columns with parameter values\n",
    "                        par_type=\"constant\",    # each well will be adjustable\n",
    "                        par_name_base=\"welcst\",\n",
    "                        pargp=\"welcst\", \n",
    "                        upper_bound = 4, lower_bound=0.25,\n",
    "                        datetime=dts[kper], # this places the parameter value on the \"time axis\"\n",
    "                        geostruct=temporal_gs)\n",
    "    \n",
    "    # add the grid parameters; each individual well\n",
    "    pf.add_parameters(filenames=f,\n",
    "                        index_cols=[0,1,2], # columns that specify cell location \n",
    "                        use_cols=[3],       # columns with parameter values\n",
    "                        par_type=\"grid\",    # each well will be adjustable\n",
    "                        par_name_base=\"welgrd\",\n",
    "                        pargp=\"welgrd\", \n",
    "                        upper_bound = 4, lower_bound=0.25,\n",
    "                        datetime=dts[kper]) # this places the parameter value on the \"time axis\"\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, our favorite (not!) boundary-condition: SFR.\n",
    "\n",
    "Let's parameterize conductance (time-invariant) and inflow (time-variant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFR conductance\n",
    "tag = \"sfr_packagedata\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "assert len(files) == 1 # There can be only one! It is tradition. Jokes.\n",
    "print(files)\n",
    "\n",
    "f = files[0]\n",
    "# constant and grid scale multiplier conductance parameters\n",
    "name = \"sfrcond\"\n",
    "pf.add_parameters(f,\n",
    "                par_type=\"grid\",\n",
    "                geostruct=grid_gs,\n",
    "                par_name_base=name+\"gr\",\n",
    "                pargp=name+\"gr\",\n",
    "                index_cols=[0,2,3],\n",
    "                use_cols=[9],\n",
    "                lower_bound=0.1,upper_bound=10.0)\n",
    "pf.add_parameters(f,\n",
    "                par_type=\"constant\",\n",
    "                geostruct=grid_gs,\n",
    "                par_name_base=name+\"cn\",\n",
    "                pargp=name+\"cn\",\n",
    "                index_cols=[0,2,3],\n",
    "                use_cols=[9],\n",
    "                lower_bound=0.1,upper_bound=10.0,\n",
    "                ult_lbound=0.001, ult_ubound=100) # absolute limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFR inflow\n",
    "files = [f for f in os.listdir(template_ws) if \"sfr_perioddata\" in f and f.endswith(\".txt\")]\n",
    "sp = [int(f.split(\".\")[1].split('_')[-1]) for f in files]\n",
    "d = {s:f for s,f in zip(sp,files)}\n",
    "sp.sort()\n",
    "files = [d[s] for s in sp]\n",
    "print(files)\n",
    "for f in files:\n",
    "    # get the stress period number from the file name\n",
    "    kper = int(f.split('.')[1].split('_')[-1]) - 1  \n",
    "    # add the parameters\n",
    "    pf.add_parameters(filenames=f,\n",
    "                        index_cols=[0], # reach number\n",
    "                        use_cols=[2],   # columns with parameter values\n",
    "                        par_type=\"grid\",    \n",
    "                        par_name_base=\"sfrgr\",\n",
    "                        pargp=\"sfrgr\", \n",
    "                        upper_bound = 10, lower_bound=0.1, # don't need ult_bounds because it is a single multiplier\n",
    "                        datetime=dts[kper], # this places the parameter value on the \"time axis\"\n",
    "                        geostruct=temporal_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damn!  we just parameterized many recognized sources of model input uncertainty at several spatial and temporal scales.  And we expressed spatial and temporal correlation in those parameters.  One last set of parameters that we will need later for sequential data assimilation - initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(template_ws) if \"ic_strt\" in f and f.endswith(\".txt\")]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    base = f.split(\".\")[1].replace(\"_\",\"\")\n",
    "    df = pf.add_parameters(f,par_type=\"grid\",par_style=\"d\",\n",
    "                      pargp=base,par_name_base=base,upper_bound=50,\n",
    "                     lower_bound=15,zone_array=ib,transform=\"none\")\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Forward Run Script\n",
    "\n",
    "OK! So, we almost have all the base building blocks for a PEST(++) dataset. We have some (1) observations and some (2) parameters. We are still missing (3) the \"forward run\" script. Recall that in the PEST world, the \"model\" is not just the numerical model (e.g. MODFLOW). Instead it is a composite of the numerical model (or models) and pre- and post-processing steps, encapsulated in a \"forward run\" script which can be called from the command line. This command line instruction is what PEST(++) sees as \"the model\". During execution, PEST(++) writes values to parameter files, runs \"the model\", and then reads values from the observation files.\n",
    "\n",
    "`PstFrom` automates the generation of such a script when constructing the PEST control file. The script is written to file named `forward_run.py`. It is written in Python (this is not a PEST(++) requirement, merely a convenience...we are working in Python after all...). \n",
    "\n",
    "How about we see that in action? Magic time! Let's create the PEST control file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pf.build_pst()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! Done. (Well almost.) Check the folder. You should see a new .pst file and the `forward_run.py` file. By default, the .pst file is named after the original model folder name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".py\") or f.endswith(\".pst\") ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get to the `pst` object later on (see also the \"intro to pyemu\" tutorial notebook). For now, let's focus on the `forward_run.py` script. It is printed out below.\n",
    "\n",
    "This script does a bunch of things:\n",
    " - it loads necessary dependencies\n",
    " - it removes model output files to avoid the possibility of files from a previous model run being read by mistake;\n",
    " - it runs pre-processing steps (see `pyemu.helpers.apply_list_and_array_pars()`;\n",
    " - it executes system commands (usually running the simulator, i.e. MODFLOW). (*This is still missing. We will demonstrate next.*)\n",
    " - it executes post-processing steps; (*for now there aren't any*)\n",
    " - ...it washes the dishes (sorry, no it doesn't...this feature is still in development)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(line.rstrip()) for line in open(os.path.join(template_ws,\"forward_run.py\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty amazing. But as we just saw, we still need to add commands to actually run the model.\n",
    "\n",
    "`PstFrom` allows you to pass a list of system commands which will be executed in sequence. It also has methods for including Python functions that run before or after the system commands. These make pre-/post-processing a piece of cake. In fact, we have already started to add to it. Remember all of the multiplier and additive parameters we setup? These all require pre-processing steps to convert the PEST-generated multipliers into model input values. `PstFrom` will automatically add these functions to the `forward_run.py` script. Nifty, hey?\n",
    "\n",
    "Next we will demonstrate how to specify the system commands and add Python functions as processing steps.\n",
    "\n",
    "#### Sys Commands\n",
    "\n",
    "Let's start by adding a command line instruction. These are stored as a list in `PstFrom.mod_sys_cmds`, which is currently empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.mod_sys_cmds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a MODFLOW6 model from the command line, you can simply execute `mf6` in the model folder. So, we can add this command by appending it to the list. (Do this only once! Every time you append 'mf6' results in an additional call to MODFLOW6, meaning the model would be run multiple times.)\n",
    "\n",
    "`PstFrom` will add a line to `forward_run.py` w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.mod_sys_cmds.append(\"mf6\") # do this only once\n",
    "pf.mod_sys_cmds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to run MODPATH7, so we need to add that to the list of system commands. In this case we also need to specify the modpath sim file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.mod_sys_cmds.append(\"mp7 freyberg_mp.mpsim\") # do this only once\n",
    "pf.mod_sys_cmds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's rebuild the Pst control file and check out the changes to the `forward_run.py` script.\n",
    "\n",
    "You should see that `pyemu.os_utils.run(r'mf6')` has been added after the pre-processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pf.build_pst()\n",
    "\n",
    "_ = [print(line.rstrip()) for line in open(os.path.join(template_ws,\"forward_run.py\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra pre- and post-processing functions\n",
    "\n",
    "You will also certainly need to include some additional processing steps.  These are supported through the `PstFrom.pre_py_cmds` and `PstFrom.post_py_cmds`, which are lists for pre and post model run python commands and `PstFrom.pre_sys_cmds` and `PstFrom.post_sys_cmds`, which are lists for pre and post model run system commands (these are wrapped in `pyemu.os_utils.run()`.  \n",
    "\n",
    "But what if your additional steps are actually an entire python function? Well, we got that too! `PstFrom.add_py_function()`. This method allows you to get functions from another (pre-prepared) python source file and add them to the `forward_run.py` script. We will demonstrate this to post-process model observations after each run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see this py-sauce in action: we are going to add a little post-processing function to extract the final simulated water level for all model cells for the last stress period from the MF6 binary headsave file and save them to ASCII format so that PEST(++) can read them with instruction files.  And, while we are at it, let's also extract the global water budget info from the MF6 listing file and store it in dataframes - these are usually good numbers to watch!  We will need the simulated water level arrays later for sequential data assimilation (wouldn't it be nice if MF6 supported the writing of ASCII format head arrays?).  Anyway, this function is stored in the \"helpers.py\" script which you can find in the tutorial folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.add_py_function(\"helpers.py\",\"extract_hds_arrays_and_list_dfs()\",is_pre_cmd=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That last argument - `is_pre_cmd` tells `PstFrom` if the python function should be treated as a pre-processor or a post-processor. So we have added that post-processor, but we still need to set up pest observations for those ASCII head arrays.  Let's do that by first calling that function to operate once within the `template_ws` to generate the arrays and then we can add them with `add_observations()`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "helpers.test_extract_hds_arrays(template_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(template_ws) if f.startswith(\"hdslay\")]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    pf.add_observations(f,prefix=f.split(\".\")[0],obsgp=f.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [\"inc.csv\",\"cum.csv\"]:\n",
    "    df = pd.read_csv(os.path.join(template_ws,f),index_col=0)\n",
    "    pf.add_observations(f,index_cols=[\"totim\"],use_cols=list(df.columns.values),\n",
    "                        prefix=f.split('.')[0],obsgp=f.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crushed it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secondary Observations\n",
    "\n",
    "Often it is useful to include \"secondary model outcomes\" as observations. These can be important components in a history-matching dataset to tease out specific aspects of system behaviour (e.g. head differences between aquifer layers to inform vertical permeabilities). Or they may be simple summaries of modelled outputs which are of interest for a prediction (e.g. minimum simulated head over a given period).\n",
    "\n",
    "If you inspect the tutorial folder you will find a file named `helpers.py`. This is a python source file which we have prepared for you. (Open it to see how it is organized.) This is not standard `pyemu` functionality, but we provide it here as an example of incorporating custom code for observation processing. The sky's the limit! It contains a function named `process_secondary_obs()`. This function reads the model output .csv files, processes them and writes a series of new observation .csv files. These new files contain the temporal-differences between head and SFR observations. The new .csv files are named `heads.tdiff.csv`and `sfr.tdiff.csv`, respectively.\n",
    "\n",
    "First, let's load the function here and run it so you can see what happens. (And to make sure that the observation files are in the template folder!) \n",
    "\n",
    "Run the next cell, then inspect the template folder. You should see several new csv files. These are the new secondary observations calculated by the post-processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the helper function\n",
    "helpers.process_secondary_obs(ws=template_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now let's add this function to the `forward_run.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.add_py_function(\"helpers.py\", # the file which contains the function\n",
    "                    \"process_secondary_obs(ws='.')\", # the function, making sure to specify any arguments it may requrie\n",
    "                    is_pre_cmd=False) # whether it runs before the model system command, or after. In this case, after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, boom! Bob's your uncle. As easy as that.\n",
    "\n",
    "Now, of course we want to add these observations to `PstFrom` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(os.path.join(template_ws, \"sfr.tdiff.csv\"), index_col=0)\n",
    "_ = pf.add_observations(\"sfr.tdiff.csv\", # the model output file to read\n",
    "                            insfile=\"sfr.tdiff.csv.ins\", # optional, the instruction file name\n",
    "                            index_cols=\"time\", # column header to use as index; can also use column number (zero-based) instead of the header name\n",
    "                            use_cols=list(df.columns.values), # names of columns that include observation values; can also use column number (zero-based) instead of the header name\n",
    "                            prefix=\"sfrtd\") # prefix to all observation \n",
    "                            \n",
    "df = pd.read_csv(os.path.join(template_ws, \"heads.tdiff.csv\"), index_col=0)\n",
    "_ = pf.add_observations(\"heads.tdiff.csv\", # the model output file to read\n",
    "                            insfile=\"heads.tdiff.csv.ins\", # optional, the instruction file name\n",
    "                            index_cols=\"time\", # column header to use as index; can also use column number (zero-based) instead of the header name\n",
    "                            use_cols=list(df.columns.values), # names of columns that include observation values; can also use column number (zero-based) instead of the header name\n",
    "                            prefix=\"hdstd\") # prefix to all observation names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to re-build the Pst control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pf.build_pst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(line.rstrip()) for line in open(os.path.join(template_ws,\"forward_run.py\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that `extract_hds_array_and_list_dfs()` has been added to the forward run script and it is being called after MF6 runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Building the Control File\n",
    "\n",
    "At this point, we can do some additional modifications that would typically be done that are problem-specific.  Here we can tweak the setup, specifying things such as observation weights, parameter bounds, transforms, control data, etc.\n",
    "\n",
    "Note that any modifications made after calling `PstFrom.build_pst()` will only exist in memory - you need to call `pf.pst.write()` to record these changes to the control file on disk.  Also note that if you call `PstFrom.build_pst()` after making some changes, these changes will be lost.  \n",
    "\n",
    "For the current case, the main thing we haven't addressed are the observations from custom *.ins files,  observation weights, parameter group INCTYP's and forecasts.\n",
    "\n",
    "We will do so now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Observations from INS files\n",
    "\n",
    "Recall that we wish to include observations of particle end time and status. As mentioned earlier, MP7 output files are not in a nicely organized tabular format - so we need to construct a custom instruction file. We will do this now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a really simple instruction file to read the MODPATH end point file\n",
    "out_file = \"freyberg_mp.mpend\"\n",
    "ins_file = out_file + \".ins\"\n",
    "with open(os.path.join(template_ws, ins_file),'w') as f:\n",
    "    f.write(\"pif ~\\n\")\n",
    "    f.write(\"l7 w w w w !part_status! w w !part_time!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add these observations to the `Pst`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.add_observations(ins_file=os.path.join(template_ws, ins_file),\n",
    "                    out_file=os.path.join(template_ws, out_file),\n",
    "                            pst_path='.')\n",
    "\n",
    "# and then check what changed                            \n",
    "obs = pst.observation_data\n",
    "obs.loc[obs.obsnme=='part_status', 'obgnme'] = 'part'\n",
    "obs.loc[obs.obsnme=='part_time', 'obgnme'] = 'part'\n",
    "\n",
    "obs.iloc[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters with Zero as Intial Value\n",
    "\n",
    "Recall that we assigned additive parameters to the GHB heads. Our initial parameter values for these parameter types were set as 0 (zero). This creates a wee bit of trouble when calculating derivatives. There are a couple of ways we could get around it. One way is to add an \"offset\" to the parameter initial values and to the parameter bounds. Another is to use \"absolute\" increment types (INCTYP). See the PEST manual or PEST++ user guides for descriptions of increment types. \n",
    "\n",
    "We will apply both here. \n",
    "\n",
    "We will assign INCTYP as 'absolute'. We will leave DERINC as 0.01 (the default). It is a reasonable value in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_pargps = [i for i in pst.adj_par_groups if 'head' in i]\n",
    "head_pargps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_groups.loc[head_pargps, 'inctyp'] = 'absolute'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the \"offset\" to parameter data entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "par_names = par.loc[par.parval1==0].parnme\n",
    "\n",
    "par.loc[par_names].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = -10\n",
    "par.loc[par_names, 'offset'] = offset\n",
    "par.loc[par_names, ['parval1', 'parlbnd', 'parubnd']] -= offset\n",
    "\n",
    "par.loc[par_names].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasts\n",
    "\n",
    "For most models there is a forecast/prediction that someone needs. Rather than waiting until the end of the project, the forecast should be entered into your thinking and workflow __right at the beginning__.  Here we do this explicitly by monitoring the forecasts as \"observations\" in the control file.  This way, for every PEST(++) analysis we do, we can watch what is happening to the forecasts - #winning\n",
    "\n",
    "The optional PEST++ `++forecasts` control variable allows us to provide the names of one or more observations featured in the \"observation data\" section of the PEST control file; these are treated as predictions in FOSM predictive uncertainty analysis by PESTPP-GLM. It is also a convenient way to keep track of \"forecast\" observations (makes post-processing a wee bit easier later on).\n",
    "\n",
    "Recall that, for our synthetic case we are interested in forecasting:\n",
    "\n",
    " - groundwater level in the upper layer at row 9 and column 1 (site named \"trgw-0-9-1\") in stress period 22 (time=640);\n",
    " - the \"tailwater\" surface-water/groundwater exchange during stress period 13 (time=367); and\n",
    " - the \"headwater\" surface-water/groundwater exchange at stress period 22 (time=640).\n",
    " - the particle travel time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts =[\n",
    "            'oname:sfr_otype:lst_usecol:tailwater_time:4383.5',\n",
    "            'oname:sfr_otype:lst_usecol:headwater_time:4383.5',\n",
    "            'oname:hds_otype:lst_usecol:trgw-0-9-1_time:4383.5',\n",
    "            'part_time'\n",
    "            ]\n",
    "\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fobs = obs.loc[forecasts,:]\n",
    "fobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just set this optional PEST++ argument because it will trigger certain automatic behavior later in PESTPP-GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options['forecasts'] = forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite the Control File!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to re-**write** the PEST control file. But beware, if you re-**build** the `Pst`, all these changes will be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(template_ws, 'freyberg_mf6.pst'), version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that was pretty epic. We now have a (very) high-dimensional PEST interface that includes secondary observations, as well as forecasts, ready to roll. \n",
    "\n",
    "If you inspect the folder, you will see PEST control file and all the necessary instruction and template files. Note that if there are more than 10k parameters, version 2 of the PEST control file will be [written by default](https://pyemu.readthedocs.io/en/develop/autoapi/pyemu/pst/index.html#pyemu.pst.Pst.write). \n",
    "\n",
    "Shall we check that it works? Let's run PEST once (i.e. with NOPTMAX=0). Now, by default, noptmax is set to zero. But just to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so when we run PEST it will call the model once and then stop. If the next cell is successful, then everything is working. Check the folder, you should see PEST output files. (We will go into these and how to process PEST outcomes in subsequent tutorials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run('pestpp-glm freyberg_mf6.pst', cwd=template_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we assigned observation values generated from the \"base model run\"? If we set up everything correctly, this means that PEST should have obtained residuals very close to zero. As mentioned, this is a good way to check for problems early on.\n",
    "\n",
    "Let's check the Phi recorded in the *.iobj file (could also check the *.rec or *.rei files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file\n",
    "iobj = pd.read_csv(os.path.join(template_ws, 'freyberg_mf6.iobj'))\n",
    "\n",
    "# check value in phi column\n",
    "iobj.total_phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Sweet! Zero. All is well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Parameter Covariance Matrix\n",
    "\n",
    "One the major reasons `PstFrom` was built is to help with building the Prior - both covariance matrix and ensemble - with geostatistical correlation.  Remember all that business above related to geostatistical structures and correlations?  This is where it pays off.\n",
    "\n",
    "Let's see how this works.  For cases with fewer than about 30,000 parameters, we can actually generate and visualize the prior parameter covariance matrix.  If you have more parameters, this matrix may not fit in memory.  But, not to worry, `PstFrom` has some trickery to help generate the geostatistical prior ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the prior covariance matrix and store it as a compressed binary file (otherwise it can get huge!)\n",
    "# depending on your machine, this may take a while...\n",
    "if pf.pst.npar < 35000:  #if you have more than about 35K pars, the cov matrix becomes hard to handle\n",
    "    cov = pf.build_prior(fmt='coo', filename=os.path.join(template_ws,\"prior_cov.jcb\"))\n",
    "    # and take a peek at a slice of the matrix\n",
    "    try: \n",
    "        x = cov.x.copy()\n",
    "        x[x==0] = np.NaN\n",
    "        plt.imshow(x[:1000,:1000])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "snap!  That big block must be a grid-scale parameter group..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.row_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now generate a prior parameter ensemble. This step is relevant for using PESTPP-IES in subsequent tutorials. Note: you do not have to call `build_prior()` before calling `draw()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pf.draw(num_reals=250, use_specsim=True) # draw parameters from the prior distribution\n",
    "pe.enforce() # enforces parameter bounds\n",
    "pe.to_binary(os.path.join(template_ws,\"prior_pe.jcb\")) # writes the parameter ensemble to binary file\n",
    "assert pe.shape[1] == pst.npar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test-run one of these geostatistical realizations (always a good idea!).  We do this by replacing the `parval1` values in the control with a row from `pe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data.loc[:,\"parval1\"] = pe.loc[pe.index[0],pst.par_names].values\n",
    "pst.parameter_data.parval1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 0\n",
    "pst.write(os.path.join(template_ws,\"test.pst\"),version=2)\n",
    "pyemu.os_utils.run(\"pestpp-glm test.pst\",cwd=template_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, that's it! The PEST-interface is set up, tested and we have our prior prepared. We should be good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Understanding Multiplier-Parameters\n",
    "\n",
    "Now the multiplier files in the `template_ws/mult` folder and the MF6 input files in the `template_ws` folder contain the values corresponding to this realization, so we can visualize the multiplier parameter process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(template_ws,\"mult2model_info.csv\"))\n",
    "kh1_df = df.loc[df.model_file.str.contains(\"npf_k_layer1\"),:]\n",
    "kh1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_arr = np.loadtxt(os.path.join(template_ws,kh1_df.org_file.iloc[0]))\n",
    "inp_arr = np.loadtxt(os.path.join(template_ws,kh1_df.model_file.iloc[0]))\n",
    "mlt_arrs = [np.loadtxt(os.path.join(template_ws,afile)) for afile in kh1_df.mlt_file]\n",
    "arrs = [org_arr]\n",
    "arrs.extend(mlt_arrs)\n",
    "arrs.append(inp_arr)\n",
    "names = [\"org\"]\n",
    "names.extend([mf.split('.')[0].split('_')[-1] for mf in kh1_df.mlt_file])\n",
    "names.append(\"MF6 input\")\n",
    "fig,axes = plt.subplots(1,kh1_df.shape[0]+2,figsize=(5*kh1_df.shape[0]+2,5))\n",
    "for i,ax in enumerate(axes.flatten()):\n",
    "    arr = np.log10(arrs[i])\n",
    "    arr[ib==0] = np.NaN\n",
    "    cb = ax.imshow(arr)\n",
    "    plt.colorbar(cb,ax=ax)\n",
    "    ax.set_title(names[i],loc=\"left\")\n",
    "plt.tight_layout()    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
