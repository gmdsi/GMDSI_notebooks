{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History Match Freyberg using K\n",
    "\n",
    "In this notebook we will start to ease our way into using PEST++ for history-matching and uncertainty analysis. We will start by revisting the Freyberg model and the PEST control file. Both of these are pre-prepared and provided for you. We will calibrate the model by adjusting a single hydraulic property (hydraulic conductivity) and then look at typical summary statstics and plots that describe our degree of fit. \n",
    "\n",
    "We will also start to gently introduce the use of `pyEMU` and programaticaly interfacing with PEST and PEST outputs. \n",
    "\n",
    "### Admin\n",
    "\n",
    "We have provided some pre-cooked PEST dataset files, wrapped around the modified Freyberg model. This is the same dataset introduced in the \"freyberg pest setup\" notebook. \n",
    "\n",
    "The functions in the next cell import required dependencies and prepare a folder for you. This folder contains the model files and a preliminary PEST setup. Run the cells, then inspect the new folder named \"freyberg_k\" which has been created in your tutorial directory. (Just press `shift+enter` to run the cells). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "import shutil\n",
    "\n",
    "sys.path.insert(0,os.path.join(\"..\", \"..\", \"dependencies\"))\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd\n",
    "\n",
    "plt.rcParams['font.size'] = 10\n",
    "pyemu.plot_utils.font =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder containing original model files\n",
    "org_d = os.path.join('..', '..', 'models', 'monthly_model_files_1lyr_newstress')\n",
    "\n",
    "# a dir to hold a copy of the org model files\n",
    "tmp_d = os.path.join('freyberg_mf6')\n",
    "\n",
    "if os.path.exists(tmp_d):\n",
    "    shutil.rmtree(tmp_d)\n",
    "shutil.copytree(org_d,tmp_d)\n",
    "\n",
    "\n",
    "# get executables\n",
    "hbd.prep_bins(tmp_d)\n",
    "# get dependency folders\n",
    "hbd.prep_deps(tmp_d)\n",
    "# run our convenience functions to prepare the PEST and model folder\n",
    "hbd.prep_pest(tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder - the modified-Freyberg model\n",
    "Just a quick reminder of what the model looks like and what we are doing. \n",
    "\n",
    "It is a one-layer model. A river runs north-south, represented with the SFR package (green cells in the figure). On the southern border there is a GHB (cyan cells). No-flow cells are shown in black. Pumping wells are shown with red cells. \n",
    "\n",
    "Time-series of measured heads are available at the locations marked with black X's. River flux is also measured at three locations (headwater, tailwater and gage; not displayed).\n",
    "\n",
    "The simulation starts with a steady state stress period, followed by twelve transient stress periods. These represent the historic period, for which measured data are available.\n",
    "\n",
    "A subsequent twelve transient stress periods representing a period in the future. Modelling is undertaken to assess selected forecasts during the simulated period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbd.plot_freyberg(tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PEST Control File\n",
    "\n",
    "Open the new folder named `freyberg_k` and find the file named `freyberg.pst`. Open it in a text editor. You may recognize it from the previous tutorial.\n",
    "\n",
    "In the tutorial folder there is a PDF file named \"Annotated_PEST_control_file_SIR20105169.pdf\". Open it up and use it to guide you through reading the PEST control file and answering the following questions:\n",
    "\n",
    "1. How many parameters are we running? \n",
    "2. How many are adjustable? \n",
    "3. How many types of observations are included?\n",
    "4. How many forecasts? What types?\n",
    "5. How many template (tpl) files do we have?\n",
    "6. How many instruction (ins) files do we have? \n",
    "\n",
    "Here's an annotated top of the PEST control file. Check the variables highlighted in yellow to answer the above questions:\n",
    "\n",
    "<img src=\"freyberg_k_files\\2010-5169_annotated_Appendix1_PST_file.png\" style=\"float: center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we did in the previous tutorial, let's double check these files to make sure the PEST dataset does not have any errors. Run TEMPCHEK, INSCHEK and PESTCHEK on the template, instruction and control files respectively.\n",
    "\n",
    "To speed things up, this time, instead of running them in a separate terminal window we can run them directly from this notebook using `pyemu`. Execute the next code block then look at the terminal window from which you launched this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pyemu to run a command line, run tempchek on the tpl files listed in the control file\n",
    "pyemu.os_utils.run(\"tempchek freyberg6.npf_k_layer1.txt.tpl\", # the instruction passed to the command line\n",
    "                    cwd=tmp_d)                            # the directory in which the command is executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do it yourself for the other TPL files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyemu.os_utils.run(\"tempchek ....\", cwd=tmp_d)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the instruction files with `INSCHEK`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pyemu to run a command line, run inschek on the ins files listed in the control file\n",
    "pyemu.os_utils.run(\"inschek heads.csv.ins heads.csv\", cwd=tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do it yourself for the other INS files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyemu.os_utils.run(\"tempchek ....\", cwd=tmp_d)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, check the control file with `PESTCHEK`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pyemu to run a command line, run pestchek on the pst file \n",
    "pyemu.os_utils.run(\"pestchek freyberg.pst\", cwd=tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PEST\n",
    "\n",
    "Okay, let's run this thing. \n",
    "\n",
    "Because we call a program from within the Jupyter Notebook you have to look at the terminal window that you used to start the notebook to see the screen report of the run.  So, when executing this next block look at your terminal window to see the run.  It will say \"Simulation complete...\" when finished.\n",
    "\n",
    "> Note: And/or wait until the standard out reports a \"0\" below this next block (=when the run is finished) before going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pyemu to run a command line, run pestpp on the pst file defined on the import\n",
    "pyemu.os_utils.run(\"pestpp-glm freyberg.pst\", cwd=tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``PEST++`` only ran the model one time - why?\n",
    "\n",
    "Yeah, that's right, the NOPTMAX=0 thing again.  We had that set to zero because we want to check the plumbing before burning the silicon. Did everything run (i.e., did you see \"Simulation Complete...\" in your terminal window?)?  Like before, you *could* change NOPTMAX to 20 in a text editor.  But, `pyemu` can do it for you with the next block.  \n",
    "\n",
    "> Note: see the \"intro to pyemu\" tutorial notebook for an overview of `pyemu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the path ot the pst control file\n",
    "pstfile = os.path.join(tmp_d,'freyberg.pst')\n",
    "\n",
    "# pymu stores all things related to a PEST control file in the Pst class. \n",
    "# We can instantiate a Pst object by reading an existing control file \n",
    "pst = pyemu.Pst(pstfile)\n",
    "\n",
    "# We can access and modfiy variables in the \"* control data\" section using the Pst.control_data attribute\n",
    "# have pyemu change PEST's NOPTMAX variable to 20\n",
    "pst.control_data.noptmax = 20\n",
    "\n",
    "# changes so far are only stored in memory, they have not been written to the .pst control file yet\n",
    "# Easy enough to accomplish - write out a new pst control file\n",
    "pst.write(pstfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `freyberg.pst` file again in a text editor. Verify that NOPTMAX has been changed to 20?\n",
    "\n",
    "Great, let's try that again!\n",
    "\n",
    "Just like before  you have to look at the terminal window that you used to start the notebook to see the screen report of the run.  So, when executing this next block look at your terminal window to see the run.  It will say \"pestpp-glm analysis complete...\" when finished. It should take a couple of minutes.\n",
    "\n",
    "Or wait until the standard out  reports a \"0\" below this next block (=when the run is finished) before going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pyemu to run a command line, run pest++ on the pst file defined above\n",
    "pyemu.os_utils.run(\"pestpp-glm freyberg.pst\", cwd=tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Results\n",
    "\n",
    "PEST writes lots of usefull information to the `*.rec` file. It also outputs a series of other useful files. What outputs are recorded depends on which version of PEST or PEST++ is being used. Here we will use PEST++GLM. The following section will demonstrate usefull information that can be found in some of the outputs. Throughout subsequent tutorials we will address others.\n",
    "\n",
    "#### Objective Function\n",
    "First let's look at the measurement objective function (Phi), which is calculated using the sum of squared weighted residuals.  First we'll look at a table, then plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe \"df_obj\" that shows the contents of the pst file casename with the extension .iobj\n",
    "# .iobj = PEST++ output file that has the objective function by iteration \n",
    "df_obj = pd.read_csv(os.path.join(tmp_d, \"freyberg.iobj\"),index_col=0)\n",
    "# echo out the dataframe\n",
    "df_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So thats usefull. If we make a plot (see next cell), it becomes evident that there are diminshing returns after a certain point (for this case!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out the dataframe that was shown as a table above\n",
    "df_obj.loc[:,[\"total_phi\",\"model_runs_completed\"]].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Termination Criteria** \n",
    "\n",
    "But hold on a second! We told PEST to try 20 parameter estimation upgrades but it stopped at 8!  What gives?!?\n",
    "\n",
    ">hint: search the `.rec` file for OPTIMIZATION COMPLETE\n",
    "\n",
    "PEST and PEST++ will quit the parameter estimation process if one of these 4 conditions is met:\n",
    "\n",
    "1. The maximum number of interations specified in NOPTMAX is reached\n",
    "2. The fit is not getting any better based on a user-supplied closure\n",
    "3. The parameters are not changing based on a user-supplied closure\n",
    "4. The user halted the run, usually with a ctrl-c  (happens quite frequently)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Phi** \n",
    "\n",
    "Look at the FINAL OPTIMISATION RESULTS in the terminal where PEST++ ran (you can also find it near the bottom of the `.rec` file).  \n",
    "\n",
    "Which target group(s) matter?  How was splitting the contributions to PHI accomplished in the PEST control file?\n",
    "\n",
    "For this problem, recall our objective function is calculated using this equation:\n",
    "\n",
    "<img src=\"freyberg_k_files\\SOSWR_eq_AW&H2015.png\" style=\"float: center\">\n",
    "\n",
    "where Phi is the \"sum of squared weighted residuals\" that we look to minimize, $w_{hi}$ is the weight for the ith head observation; $h_m$ is the measured (observed) head target; $h_s$ is the simulated head; and $n$ is the number of observations.  \n",
    "\n",
    "If we use only heads for calibration, then PHI only reflects the sum of squared weighted residuals for the observed-simulated head targets. \n",
    "\n",
    "So! We have two types of observations (heads and flux) each in their respecive observation groups (hds and flux)...but only heads are contributing to the objective function. This is because all \"flux\" observations have been assigned zero weight (see the `* observation data` section). They are in the control file, but they aren't doing anything for the time-being. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residuals\n",
    "\n",
    "Let's evaulate our fit using the observed-simulated residuals.\n",
    "\n",
    "PEST++ stores obsevration residuals in a `*.rei` file. In the working folder you will find a file named `freyberg.rei`. Open it in a text editor. Here you will find a table with observation names, their measured and simulated values, the differneces between them (e.g. the residuals) and weights assigned in the PEST control file. \n",
    "\n",
    "When instantiating a `Pst` object from an existing control file, `pyemu` will attemp to read a corresponding `*.rei` file. Data from the rei file is stored in the `Pst.res` attribute as a `Pandas` `DataFrame`. This makes it easy to access and postprocess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define res_nz to equal a table of residuals for all observations with non-zero weight\n",
    "# pst.res stores information that is in the .rei file (see the freyberg.rei file in the tutorial folder)\n",
    "# pst.nnz_obs_names returns a list of all the \"non-zero weighted observation names\"\n",
    "res_nz = pst.res.loc[pst.nnz_obs_names,:]\n",
    "# display res_nz to understand what we are doing\n",
    "res_nz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could plot these results up using common libraries. Or, use `pyemu`s built in plotting methods (see next cell).\n",
    "\n",
    "Not too shabby!  Thanks PEST++.\n",
    "\n",
    "These plots you'll see a lot.  The left plot is a \"1:1\" plot that has simulated on the x-axis and observed on the y-axis; a perfect fit would be all circles on the black diagonal line.  The right plot has the residual (y-axis) compared to the observation magnitude (x-axis).  The closer the circle is to the black line the better the fit.  The mean residual is shown as a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pyemu's plot utilities to plot 1:1 line and the residuals as fxn of observation magnitude\n",
    "pyemu.plot_utils.res_1to1(pst);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we had a lot of other observations listed in the PEST control file.  What do they look like?\n",
    "\n",
    "> **Note**: We have used a naming convention for our observations. Each observation name starts with the site name (e.g. \"gage-1\"), followed by \" : \" and then the simulation time in days (e.g. \"1.0\"). So, \"gage_1:1.0\" refers to the observation at \"gage-1\" after \"1.0\" days (in this case, at the end of the first steady state stress period).\n",
    "\n",
    "How well did the model do at fitting river flux? Recall that only head observations have non-negative weights, so PEST was only interested in getting a good fit with heads, ignoring all other observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pst.res.measured, pst.res.modelled)\n",
    "plt.xlabel('measured')\n",
    "plt.ylabel('modelled')\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Uncertainty Reduction\n",
    "\n",
    "Now, let's look at what calibration has done for uncertainty reduction. First, let's look the change in uncertainty for horizontal hydraulic conductivity (`hk`) parameters.\n",
    "\n",
    "PEST++GLM conveniently records parameter and forecast uncertainty summaries in CSV files (we will get to what this means and how it is done in another tutorial). Parameter uncertainty is recorded in the file named `freyberg.par.usum.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dataframe that has uses the PEST++ output file freyberg_un.par.usum.csv\n",
    "# freyberg_un.par.usum.csv is comma-delimited file that has the uncertainty summary for the parameters\n",
    "df_paru = pd.read_csv(os.path.join(tmp_d, \"freyberg.par.usum.csv\"),index_col=0)\n",
    "\n",
    "# echo out this dataframe \n",
    "df_paru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that because we log transformed the `hk` parameters the uncertainty results are reported as logarithms in the dataframe above.  What you'll see in the MODFLOW input file is the non-log transformed `hk` value (e.g. $10^{0.69897} = 5.0$  for the prior mean).\n",
    "\n",
    "A quick way to evaluate the ***reduction in uncertainty*** is to compare `prior_stdev` (e.g. standard deviation of the prior, or standard deviation before calibration) to `post_stdev` (e.g. standard deviation of the posterior, or standard deviation after caibration).  The amount that `post_stdev` is less than `pre_stdev` reflects the magnitude of the uncertainty reduction\n",
    "\n",
    "Now let's plot it using `pyemu`'s plot utility.\n",
    "\n",
    "The dotted gray line represents the \"prior\" information as expressed by the parameter bounds in the PEST control file. (In this case, the differences between parameter bounds are taken to represent the probability distribution range.) The shaded area is the uncertainty after the calibration.\n",
    "\n",
    "Wow! Amazing, almost no posterior uncertainty...ermm...yeah, no. That ain't right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a set of plots called ax to have the information of our dataframe df_paru above\n",
    "ax = pyemu.plot_utils.plot_summary_distributions(df_paru, label_post=True, figsize=(7,5))\n",
    "# Plot it with a label \n",
    "ax.set_xlabel(\"$log_{10}(\\\\frac{L}{T})$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at changes in model forecast uncertainty, first as a table then as a plot.  These are *observations* now instead of parameters like above. PEST++GLM has recorded these in the file named `freyberg.pred.usum.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dataframe that has uses the PEST++ output file freyberg.pred.usum.csv\n",
    "# freyberg.pred.usum.csv is comma-delimited file that has the uncertainty summary for the predictions \n",
    "df_predu = pd.read_csv(os.path.join(tmp_d, \"freyberg.pred.usum.csv\"),index_col=0)\n",
    "# echo out the dataframe\n",
    "df_predu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same deal as above: a quick way to evaluate the ***reduction in uncertainty*** is to compare `prior_stdev` (=standard deviation of the prior=standard deviation before calibration) to `post_stdev` (=standard deviation of the posterior = standard deviation after caibration).  The amount that `post_stdev` is less than pre_stdev reflects the magnitude of the uncertainty reduction.\n",
    "\n",
    "As we can see in the plot below, prediction uncertainty is reduced for all forecasts. Some by quite a lot! Our calibration must have been amazing (#sarcasm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pyemu plotting utility to plot up the forecasts\n",
    "figs, axes = pyemu.plot_utils.plot_summary_distributions(df_predu,subplots=True)\n",
    "figs[0].tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing prior to posterior standatd deviations we can check how well calibration reduced forecast uncertainties (see bar plot in the next cell; larger value is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predu.loc[:,\"percent_reduction\"] = 100.0 * (1.0 - (df_predu.post_stdev / df_predu.prior_stdev))\n",
    "df_predu.percent_reduction.plot.bar()\n",
    "plt.ylabel('% uncertainty reduction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Wow! Calibration really helped huh? So we can call it a day and bill the client? Awesome, thanks very much and have a nice day!\n",
    "\n",
    "Well, no...\n",
    "\n",
    "Just because we can, let's look again at forecast uncertainty with the \"truth\". In the next cell we plot the forecast probability distributions again, but this time we have included the \"true\" outcome as well (the vertical black dashed line). Recal that here we know the \"truth\" because we (the authors) created reality; in the real-world we do not have this luxury."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs, axes = pyemu.plot_utils.plot_summary_distributions(df_predu,subplots=True)\n",
    "for ax in axes:\n",
    "    fname = ax.get_title().lower()\n",
    "    ylim = ax.get_ylim()\n",
    "    v = pst.observation_data.loc[fname,\"obsval\"]\n",
    "    ax.plot([v,v],ylim,\"k--\")\n",
    "    ax.set_ylim(0,ylim[-1])\n",
    "figs[0].tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dear oh dear....none of the forecasts are bracketed by the posterior distribution! This means that \n",
    "\n",
    "## __uncertainty analysis has failed!__\n",
    "\n",
    "In some cases the prior (the dashed grey line) encompasses the \"truth\" but the posterior (the blue shaded area) does not. Therefore calibration actualy made our forecasts less reliable. Why is that? How can improving a model's ability to represent the past make it *worse* at representing the future? \n",
    "\n",
    "Find out in the next episode! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
