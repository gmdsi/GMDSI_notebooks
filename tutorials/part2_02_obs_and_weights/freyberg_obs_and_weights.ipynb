{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulating the Objective Function and The Dark Art of Weighting\n",
    "\n",
    "The objective function expresses model-to-measurement misfit for use in the solution of an inverse problem through the *weighted squared difference between measured and simulated observations*. There is no formula for a universally \"best approach\" to formulate an objective function. However, the universal underlying principle is to ensure that as much information as possible is transferred to the parameters of a model, in as \"safe\" a way as possible (Doherty, 2015). \n",
    "\n",
    "**Observation Types**\n",
    "\n",
    "In most history-matching contexts a “multicomponent” objective function is recommended. Each component of this objective function is calculated on the basis of different groups of observations or of the same group of observations processed in different ways. In a nutshell, this means as many (useful!) types of observation as are available should be included in the parameter-estimation process. This does not **just** mean different \"measurement types\"! It also means teasing out components *within* a given measurement type. These \"secondary\" components often contain information that is otherwise lost or overshadowed. \n",
    "\n",
    "**Observation Grouping** \n",
    "\n",
    "Using a multicomponent approach can extract as much information from an observation dataset as possible and transfer this information to estimated parameters. When constructing a PEST dataset, it is often useful (and convenient) to group observations by type. This makes it easier to customize objective function design and track the flow of information from data to parameters (and subsequently to predictions). Ideally, each observation grouping should illuminate and constrain parameter estimation related to a separate aspect of the system being modelled (Doherty and Hunt, 2010). For example, absolute values of heads may inform parameters that control horizontal flow patterns, whilst vertical differences between heads in different aquifers may inform parameters that control vertical flow patterns. \n",
    "\n",
    "**Observation Weighting**\n",
    "\n",
    "A user must decide how to weight observations before estimating parameters with PEST(++). In some cases, it is prudent to strictly weight observations based on the inverse of the standard deviation of measurement noise. Observations with higher credibility should, without a doubt, be given more weight than those with lower credibility. However, in many history-matching contexts, model defects are just as important as noise in inducing model-to-measurement misfit as field measurements. Some types of model outputs are more affected by model imperfections than others. (Notably, the effects of imperfections on model output _differences_ are frequently less than their effects on raw model outputs.) In some cases, accommodating the \"structural\" nature of model-to-measurement misfit (i.e. the model is inherently better at fitting some measurements than others) is preferable rather than attempting to strictly respect the measurement credibility. \n",
    "\n",
    "The PEST Book (Doherty, 2015) and the USGS published report \"A Guide to Using PEST for Groundwater-Model Calibration\" (Doherty et al. 2010) go into detail on formulating an objective function and discuss common issues with certain data-types. \n",
    "\n",
    "**References and Recommended Reading:**\n",
    ">  - Doherty, J., (2015). Calibration and Uncertainty Analysis for Complex Environmental Models. Watermark Numerical Computing, Brisbane, Australia. ISBN: 978-0-9943786-0-6.\n",
    ">  - <div class=\"csl-entry\">White, J. T., Doherty, J. E., &#38; Hughes, J. D. (2014). Quantifying the predictive consequences of model error with linear subspace analysis. <i>Water Resources Research</i>, <i>50</i>(2), 1152–1173. https://doi.org/10.1002/2013WR014767</div>\n",
    ">  - <div class=\"csl-entry\">Doherty, J., &#38; Hunt, R. (2010). <i>Approaches to Highly Parameterized Inversion: A Guide to Using PEST for Groundwater-Model Calibration: U.S. Geological Survey Scientific Investigations Report 2010–5169</i>. https://doi.org/https://doi.org/10.3133/sir20105169</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Recap: the modified-Freyberg PEST dataset\n",
    "\n",
    "The modified Freyberg model is introduced in another tutorial notebook (see \"intro to freyberg model\"). The current notebook picks up following the [\"pstfrom pest setup\"](../part2_01_pstfrom_pest_setup/freyberg_pstfrom_pest_setup.ipynb) notebook, in which a high-dimensional PEST dataset was constructed using `pyemu.PstFrom`. You may also wish to go through the [\"intro to pyemu\"](../part0_intro_to_pyemu/intro_to_pyemu.ipynb) and [\"pstfrom sneakpeak\"](../part1_02_pest_setup/pstfrom_sneakpeak.ipynb) notebooks beforehand.\n",
    "\n",
    "We will now address assigning observation target values from \"measured\" data and observation weighting prior to history matching. \n",
    "\n",
    "Recall from the [\"pstfrom pest setup\"](../part2_01_pstfrom_pest_setup/freyberg_pstfrom_pest_setup.ipynb) notebook that we included several observation types in the history matching dataset, namely:\n",
    " - head time series\n",
    " - river gage time series\n",
    " - temporal differences between both heads and gage time series\n",
    "\n",
    "We also included many observations of model outputs for which we do not have measured data. We kept these to make it easier to keep track of model outputs (this becomes a necessity when working with ensembles). We also included observations of \"forecasts\", i.e. model outputs of management interest.\n",
    "\n",
    "The next couple of cells load necessary dependencies and call a convenience function to prepare the PEST dataset folder for you. This is the same dataset that was constructed during the [\"pstfrom pest setup\"](../part2_01_pstfrom_pest_setup/freyberg_pstfrom_pest_setup.ipynb) tutorial. \n",
    "\n",
    "Simply press `shift+enter` to run the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "import matplotlib\n",
    "\n",
    "import sys\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the temporary working folder\n",
    "t_d = os.path.join('freyberg6_template')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the conveninece function to get the pre-preprepared PEST dataset;\n",
    "# this is the same dataset constructed in the \"pstfrom\" tutorial\n",
    "if os.path.exists(t_d):\n",
    "        shutil.rmtree(t_d)\n",
    "org_t_d = os.path.join(\"..\",\"part2_01_pstfrom_pest_setup\",t_d)\n",
    "assert os.path.exists(org_t_d),\"you need to run the '/part2_01_pstfrom_pest_setup/freyberg_pstfrom_pest_setup.ipynb' notebook\"\n",
    "print(\"using files at \",org_t_d)\n",
    "shutil.copytree(org_t_d,t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the `Pst` control file we constructed during the \"pstfrom\" tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_file = \"freyberg_mf6.pst\"\n",
    "pst = pyemu.Pst(os.path.join(t_d, pst_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we constructed the PEST dataset (in the \"pstfrom pest setup\" tutorial) we simply identified what model outputs we wanted PEST to \"observe\". In doing so, `pyemu.PstFrom` assigned observation target values that it found in the existing model output files (which conveniently allowed us to test whether our PEST setup was working correctly). All observation weights were assigned a default value of 1.0. \n",
    "\n",
    "As a reminder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we need to do several things:\n",
    " - replace observation target values (the `obsval` column) with corresponding values from \"measured data\";\n",
    " - assign meaningful weights to history-matching target observations (the `weight` column);\n",
    " - assign zero weight to observations that should not affect history matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off with the basics. First set all weights to zero. We will then go through and assign meaningful weights only to relevant target observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for nonzero weights\n",
    "obs.weight.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign all weight zero\n",
    "obs.loc[:, 'weight'] = 0\n",
    "\n",
    "# check for non zero weights\n",
    "obs.weight.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measured Data\n",
    "\n",
    "In most data-assimilation contexts you will have some relevant measured data (e.g. water levels, river flow rates, etc.) which correspond to simulated model outputs. These will probably not coincide exactly with your model outputs. Are the wells at the same coordinate as the center of the model cell? Do measurement times line up nicely with model output times? Doubt it. And if they do, are single measurements that match model output times biased? And so on... \n",
    "\n",
    "A modeller needs to ensure that the observation values assigned in the PEST control file are aligned with simulated model outputs. This will usually require some case-specific preprocessing. Here we are going to demonstrate __an example__ - but remember, every case is different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's access our dataset of \"measured\" observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_csv = os.path.join('..', '..', 'models', 'daily_freyberg_mf6_truth',\"obs_data.csv\")\n",
    "assert os.path.exists(obs_csv)\n",
    "obs_data = pd.read_csv(obs_csv)\n",
    "obs_data.set_index('site', inplace=True)\n",
    "obs_data.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have measured data at daily intervals. But our model simulates monthly stress periods. So what observation value do we use? \n",
    "\n",
    "One option is to simply sample measured values from the data closest to our simulated output. The next cell does this, with a few checks along the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just pick the nearest to the stress period end\n",
    "model_times = pst.observation_data.time.dropna().astype(float).unique()\n",
    "# get the list of observation names for which we have data\n",
    "obs_sites =  obs_data.index.unique().tolist()\n",
    "\n",
    "# restructure the observation data \n",
    "es_obs_data = []\n",
    "for site in obs_sites:\n",
    "    site_obs_data = obs_data.loc[site,:].copy()\n",
    "    if isinstance(site_obs_data, pd.Series):\n",
    "        site_obs_data.loc[\"site\"] = site_obs_data.index.values\n",
    "    elif isinstance(site_obs_data, pd.DataFrame):\n",
    "        site_obs_data.loc[:,\"site\"] = site_obs_data.index.values\n",
    "        site_obs_data.index = site_obs_data.time\n",
    "        site_obs_data = site_obs_data.reindex(model_times,method=\"nearest\")\n",
    "\n",
    "    if site_obs_data.shape != site_obs_data.dropna().shape:\n",
    "        print(\"broke\",site)\n",
    "    es_obs_data.append(site_obs_data)\n",
    "es_obs_data = pd.concat(es_obs_data,axis=0,ignore_index=True)\n",
    "es_obs_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right then... let's plot our down-sampled measurement data and compare it to the original high-frequency time series.\n",
    "\n",
    "The next cell generates plots for each time series of measured data. Blue lines are the original high-frequency data. The marked red line is the down-sampled data. What do you think? Does sampling to the \"closest date\" capture the behaviour of the time series? Doesn't look too good... It does not seem to capture the general trend very well.\n",
    "\n",
    "Let's try something else instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in obs_sites:\n",
    "    #print(site)\n",
    "    site_obs_data = obs_data.loc[site,:]\n",
    "    es_site_obs_data = es_obs_data.loc[es_obs_data.site==site,:].copy()\n",
    "    es_site_obs_data.sort_values(by=\"time\",inplace=True)\n",
    "    #print(site,site_obs_data.shape)\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    ax.plot(site_obs_data.time,site_obs_data.value,\"b-\",lw=0.5)\n",
    "    #ax.plot(es_site_obs_data.datetime,es_site_obs_data.value,'r-',lw=2)\n",
    "    ax.plot(es_site_obs_data.time,es_site_obs_data.value,'r-',lw=1,marker='.',ms=10)\n",
    "    ax.set_title(site)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's try using a moving average instead. Effectively this is applying a low-pass filter to the time-series, smoothing out some of the spiky noise. \n",
    "\n",
    "The next cell resamples the data and then plots it. Measured data sampled using a low-pass filter is shown by the marked green line. What do you think? Better? It certainly does a better job at capturing the trends in the original data! Let's go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_obs_data = {}\n",
    "for site in obs_sites:\n",
    "    #print(site)\n",
    "    site_obs_data = obs_data.loc[site,:].copy()\n",
    "    if isinstance(site_obs_data, pd.Series):\n",
    "        site_obs_data.loc[\"site\"] = site_obs_data.index.values\n",
    "    if isinstance(site_obs_data, pd.DataFrame):\n",
    "        site_obs_data.loc[:,\"site\"] = site_obs_data.index.values\n",
    "        site_obs_data.index = site_obs_data.time\n",
    "        sm = site_obs_data.value.rolling(window=20,center=True,min_periods=1).mean()\n",
    "        sm_site_obs_data = sm.reindex(model_times,method=\"nearest\")\n",
    "    #ess_obs_data.append(pd.DataFrame9sm_site_obs_data)\n",
    "    ess_obs_data[site] = sm_site_obs_data\n",
    "    \n",
    "    es_site_obs_data = es_obs_data.loc[es_obs_data.site==site,:].copy()\n",
    "    es_site_obs_data.sort_values(by=\"time\",inplace=True)\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    ax.plot(site_obs_data.time,site_obs_data.value,\"b-\",lw=0.25)\n",
    "    ax.plot(es_site_obs_data.time,es_site_obs_data.value,'r-',lw=1,marker='.',ms=10)\n",
    "    ax.plot(sm_site_obs_data.index,sm_site_obs_data.values,'g-',lw=0.5,marker='.',ms=10)\n",
    "    ax.set_title(site)\n",
    "plt.show()\n",
    "ess_obs_data = pd.DataFrame(ess_obs_data)\n",
    "ess_obs_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Target Observation Values in the Control File\n",
    "\n",
    "Right then - so, these are our smooth-sampled observation values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_obs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are confronted with the task of getting these _processed_ measured observation values into the `Pst` control file. Once again, how you do this will end up being somewhat case-specific and will depend on how your obsveration names were constructed. For example, in our case we can use the following function (we made it a function because we are going to repeat it a few times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pst_obsvals(obs_names, obs_data):\n",
    "    \"\"\"obs_names: list of selected obs names\n",
    "       obs_data: dataframe with obs values to use in pst\"\"\"\n",
    "    # for checking\n",
    "    org_nnzobs = pst.nnz_obs\n",
    "    # get list of times for obs name sufixes\n",
    "    time_str = obs_data.index.map(lambda x: f\"time:{x}\").values\n",
    "    # empty list to keep track of misssing observation names\n",
    "    missing=[]\n",
    "    for col in obs_data.columns:\n",
    "        # get obs list suffix for each column of data\n",
    "        obs_sufix = col.lower()+\"_\"+time_str\n",
    "        for string, oval, time in zip(obs_sufix,obs_data.loc[:,col].values, obs_data.index.values):\n",
    "                \n",
    "                if not any(string in obsnme for obsnme in obs_names):\n",
    "                    if string.startswith(\"trgw-2\"):\n",
    "                        pass\n",
    "                    else:\n",
    "                        missing.append(string)\n",
    "                # if not, then update the pst.observation_data\n",
    "                else:\n",
    "                    # get a list of obsnames\n",
    "                    obsnme = [ks for ks in obs_names if string in ks] \n",
    "                    assert len(obsnme) == 1,string\n",
    "                    obsnme = obsnme[0]\n",
    "                    # assign the obsvals\n",
    "                    obs.loc[obsnme,\"obsval\"] = oval\n",
    "                    # assign a generic weight\n",
    "                    if time > 3652.5 and time <=4018.5:\n",
    "                        obs.loc[obsnme,\"weight\"] = 1.0\n",
    "    # checks\n",
    "    #if (pst.nnz_obs-org_nnzobs)!=0:\n",
    "    #    assert (pst.nnz_obs-org_nnzobs)==obs_data.count().sum()\n",
    "    if len(missing)==0:\n",
    "        print('All good.')\n",
    "        print('Number of new nonzero obs:' ,pst.nnz_obs-org_nnzobs) \n",
    "        print('Number of nonzero obs:' ,pst.nnz_obs)  \n",
    "    else:\n",
    "        raise ValueError('The following obs are missing:\\n',missing)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nnz_obs_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subselection of observaton names; this is because several groups share the same obs name suffix\n",
    "obs_names = obs.loc[obs.oname.isin(['hds', 'sfr']), 'obsnme']\n",
    "\n",
    "# run the function\n",
    "update_pst_obsvals(obs_names, ess_obs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nnz_obs_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.oname.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that has sorted out the absolute observation groups. But remember the 'sfrtd' and 'hdstd' observation groups? Yeah thats right, we also added in a bunch of other \"secondary observations\" (the time difference between observations) as well as postprocessing functions to get them from model outputs. We need to get target values for these observations into our control file as well!\n",
    "\n",
    "Let's start by calculating the secondary values from the absolute measured values. In our case, the easiest is to populate the model output files with measured values and then call our postprocessing function.\n",
    "\n",
    "Let's first read in the SFR model output file, just so we can see what is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_sfr = pd.read_csv(os.path.join(t_d,\"sfr.csv\"),\n",
    "                    index_col=0)\n",
    "\n",
    "obs_sfr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now update the model output csv files with the smooth-sampled measured values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_obs_csv(obs_csv):\n",
    "    obsdf = pd.read_csv(obs_csv, index_col=0)\n",
    "    check = obsdf.copy()\n",
    "    # update values in relevant cols\n",
    "    for col in ess_obs_data.columns:\n",
    "        if col in obsdf.columns:\n",
    "            obsdf.loc[:,col] = ess_obs_data.loc[:,col]\n",
    "    # keep only measured data columns; helps for vdiff and tdiff obs later on\n",
    "    #obsdf = obsdf.loc[:,[col for col in ess_obs_data.columns if col in obsdf.columns]]\n",
    "    # rewrite the model output file\n",
    "    obsdf.to_csv(obs_csv)\n",
    "    # check \n",
    "    obsdf = pd.read_csv(obs_csv, index_col=0)\n",
    "    assert (obsdf.index==check.index).all()\n",
    "    return obsdf\n",
    "\n",
    "# update the SFR obs csv\n",
    "obs_srf = update_obs_csv(os.path.join(t_d,\"sfr.csv\"))\n",
    "# update the heads obs csv\n",
    "obs_hds = update_obs_csv(os.path.join(t_d,\"heads.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK... now we can run the postprocessing function to update the \"tdiff\" model output csv's. Copy across the `helpers.py` we used during the `PstFrom` tutorial. Then import it and run the `process_secondary_obs()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy2(os.path.join('..','part2_01_pstfrom_pest_setup', 'helpers.py'),\n",
    "            os.path.join('helpers.py'))\n",
    "\n",
    "import helpers\n",
    "helpers.process_secondary_obs(ws=t_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the oname column in the pst.observation_data provides a useful way to select observations in this case\n",
    "obs.oname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_nnzobs = pst.nnz_obs\n",
    "    #if (pst.nnz_obs-org_nnzobs)!=0:\n",
    "    #    assert (pst.nnz_obs-org_nnzobs)==obs_data.count().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of nonzero obs:', pst.nnz_obs)\n",
    "\n",
    "diff_obsdict = {'sfrtd': \"sfr.tdiff.csv\", \n",
    "                'hdstd': \"heads.tdiff.csv\",\n",
    "                }\n",
    "\n",
    "for keys, value in diff_obsdict.items():\n",
    "    print(keys)\n",
    "    # get subset of obs names\n",
    "    obs_names = obs.loc[obs.oname.isin([keys]), 'obsnme']\n",
    "    # get df\n",
    "    obs_csv = pd.read_csv(os.path.join(t_d,value),index_col=0)\n",
    "    # specify cols to use; make use of info recorded in pst.observation_data to only select cols with measured data\n",
    "    usecols = list(set((map(str.upper, obs.loc[pst.nnz_obs_names,'usecol'].unique()))) & set(obs_csv.columns.tolist()))\n",
    "    obs_csv = obs_csv.loc[:, usecols]\n",
    "    # for checking\n",
    "    org_nnz_obs_names = pst.nnz_obs_names\n",
    "    # run the function\n",
    "    update_pst_obsvals(obs_names,\n",
    "                        obs_csv)\n",
    "    # verify num of new nnz obs\n",
    "    print(pst.nnz_obs)\n",
    "    print(len(org_nnz_obs_names))\n",
    "    print(len(usecols))\n",
    "    assert (pst.nnz_obs-len(org_nnz_obs_names))==12*len(usecols), [i for i in pst.nnz_obs_names if i not in org_nnz_obs_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nnz_obs_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nnz_obs_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell does some sneaky things in the background to populate `obsvals` for forecast observations just so that we can keep track of the truth. In real-world applications you might assign values that reflect decision-criteria (such as limits at which \"bad things\" happen, for example) simply as a convenience. For the purposes of history matching, these values have no impact because they are assigned zero weight. They can play a role in specifying constraints when undertaking optimisation problems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.forecast_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbd.prep_forecasts(pst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.forecast_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Weights\n",
    "\n",
    "We are going to start off by taking a look at our current objective function value and the relative contributions from the various observation groups. Recall that this is the objective function value with **initial parameter values** and the default observations weights.\n",
    "\n",
    "First off, we need to get PEST to run the model once so that the objective function can be calculated. Let's do that now. Start by reading the control file and checking that NOPTMAX is set to zero:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check noptmax\n",
    "pst.control_data.noptmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You got a zero? Alrighty then! Let's write the updated control file and run PEST again and see what that has done to our Phi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d,pst_file),version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"pestpp-ies.exe {0}\".format(pst_file),cwd=t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to reload the `Pst` control file so that the residuals are updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(t_d, pst_file))\n",
    "pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeepers - that's large! Before we race off and start running PEST to lower it we should compare simulated and measured values and take a look at the components of Phi. \n",
    "\n",
    "Let's start with taking a closer look. The `pst.phi_components` attribute returns a dictionary of the observation group names and their contribution to the overall value of Phi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.phi_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unfortunately, in this case we have too many observation groups to easily display. (We assigned each individual time series to its own observation group; this is a default setting in `pyemu.PstFrom`.)\n",
    "\n",
    "So let's use `Pandas` to help us summarize this information (note: `pyemu.plot_utils.res_phi_pie()` does the same thing, but it looks a bit ugly because of the large number of observation groups). To make it easier, we are going to just look at the nonzero observation groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnz_phi_components = {k:pst.phi_components[k] for k in pst.nnz_obs_groups} # that's a dictionary comprehension there y'all\n",
    "nnz_phi_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And while we are at it, plot these in a pie chart. \n",
    "\n",
    "If you wish, try displaying this with `pyemu.plot_utils.res_phi_pie()` instead. Because of the large number of columns it's not going to be pretty, but it gets the job done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phicomp = pd.Series(nnz_phi_components)\n",
    "plt.pie(phicomp, labels=phicomp.index.values);\n",
    "#pyemu.plot_utils.res_phi_pie(pst,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that is certainly not ideal - phi is dominated by the SFR observation groups. Why? Because the observation values of these are much larger than those of the other observation groups... and we assigned the same weight to all of them...\n",
    "\n",
    "Let's try error-based weighting combined with common sense (i.e. subjectivity) instead. For each observation type we will assign a weight equal to the inverse of error. Conceptually, this error reflects our estimate of measurement noise. In practice, it reflects both measurement noise and model error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_obs = pst.observation_data.loc[pst.nnz_obs_names,:]\n",
    "nz_obs.oname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.loc[nz_obs.loc[(nz_obs.oname=='hds'), 'obsnme'], 'weight'] = 1/0.1\n",
    "obs.loc[nz_obs.loc[(nz_obs.oname=='hdstd'), 'obsnme'], 'weight'] = 1/0.05\n",
    "obs.loc[nz_obs.loc[(nz_obs.oname=='hdsvd'), 'obsnme'], 'weight'] = 1/0.01\n",
    "# fanciness alert: heteroskedasticity; error will be proportional to the obsveration value for SFR obs\n",
    "obs.loc[nz_obs.loc[(nz_obs.oname=='sfr'), 'obsnme'], 'weight'] = 1/ abs(0.1 * nz_obs.loc[(nz_obs.oname=='sfr'), 'obsval'])\n",
    "obs.loc[nz_obs.loc[(nz_obs.oname=='sfrtd'), 'obsnme'], 'weight'] = 1/100 #abs(0.5* nz_obs.loc[(nz_obs.oname=='sfrtd'), 'obsval'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how that has affected the contributions to Phi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "phicomp = pd.Series({k:pst.phi_components[k] for k in pst.nnz_obs_groups})\n",
    "plt.pie(phicomp, labels=phicomp.index.values);\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better! Now weights reflect the quality of the data. Another aspect to consider is if observation group contributions to Phi are relatively balanced...do you think any of these observation groups will dominate Phi (or be lost)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell adds in a column to the `pst.observation_data` for checking purposes in subsequent tutorials. Pretend you didn't see it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.nnz_obs_names,'observed'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to rewrite the .pst file!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d,pst_file),version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Observation Weights and Measurement Noise\n",
    "\n",
    "Let's visualize what these weights actually imply. \n",
    "\n",
    "We are going to generate an ensemble of observation values. These values will be sampled from a Gaussian distribution, in which the observation value in the `Pst` control file is the mean and the observation weights reflect the inverse of standard deviation. (See the \"intro to pyemu\" tutorial for an introduction to  `pyemu.ObservationEnsemble`.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe = pyemu.ObservationEnsemble.from_gaussian_draw(pst=pst, \n",
    "                                                num_reals=50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function in the next cell does a bunch of funky things in order to plot observation time series (convenient that we stored all the necessary information in observation names, hey?). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_obs_ts(pst, oe, obsgrps=pst.nnz_obs_groups, subset=None):\n",
    "    # get times and \"measured values\"\n",
    "    nz_obs = pst.observation_data.loc[pst.nnz_obs_names,:].copy()\n",
    "    nz_obs['time'] = nz_obs['time'].astype(float)\n",
    "    nz_obs.sort_values(['obgnme','time'], inplace=True)\n",
    "    \n",
    "    # to plot current model outputs\n",
    "    res = pst.res.copy()\n",
    "    res['time'] = pst.observation_data['time'].astype(float)\n",
    "    res.sort_values(['group','time'], inplace=True)\n",
    "    \n",
    "    # optional: plot only a subset of reals to see their patterns better\n",
    "    if subset is not None:\n",
    "        plot_idx = np.random.choice(oe.index.values, subset)\n",
    "    else:\n",
    "        plot_idx = oe.index.values\n",
    "    for nz_group in obsgrps:\n",
    "        nz_obs_group = nz_obs.loc[nz_obs.obgnme==nz_group,:]\n",
    "        nz_obs_meas = res.loc[(res['group']==nz_group) & res['weight']!=0]\n",
    "\n",
    "        fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "        # plot ensemble of time series of measured + noise\n",
    "        [ax.plot(nz_obs_group.time, oe.loc[r, nz_obs_group.obsnme] ,color=\"r\",lw=0.3) for r in plot_idx]\n",
    "        # plot the envelope of noise realizations as well\n",
    "        ax.fill_between(nz_obs_group.time, oe._df[nz_obs_group.obsnme].min(), \n",
    "                        oe._df[nz_obs_group.obsnme].max(), color=\"r\", zorder = 0, alpha=.2)\n",
    "        # plot simulated time series\n",
    "        ax.plot(nz_obs_group.time,nz_obs_group.obsval,\"b\")\n",
    "        # plot measured time series\n",
    "        ax.plot(nz_obs_meas.time, nz_obs_meas.modelled, 'k', linestyle='--')\n",
    "        ax.set_title(nz_group)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the time series of observations for the SFR measurment types. Run the next cell.\n",
    "\n",
    "The blue line is the time series of \"field-measured\" values (in the control file). The red lines are a few instances of time series from the observation ensemble. The dashed black line is the model output with initial parameter values. What we are saying with these weights is that any of these red lines could also have been \"measured\". Due to the noise (e.g. uncertainty) in our measurements, the true value can fall anywhere in that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_obs = [i for i in pst.nnz_obs_groups if 'sfr' in i]\n",
    "plot_obs_ts(pst, oe, ts_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Aside: A Convenience of Error-Based Weighting\n",
    "\n",
    "A convenience of weighting with the inverse of the measurement uncertainty is that it is easy to know what the ideal Phi should be: it should be equal to the number of nonzero-weighted observations. This of course assumes that all model-to-measurement misfit is due to *measurement* uncertainty. In practice, model error usually plays a larger role, as we will see in other tutorials. \n",
    "\n",
    "Just to demonstrate what we mean, let's quickly do some math. Imagine that we have:\n",
    " - 2 observations\n",
    " - both have measured values of 1\n",
    " - with measurement standard deviation (e.g. uncertainty) of 0.25\n",
    " - therefore, they are assigned a weight of 1/0.25 = 4\n",
    "\n",
    "So, all that we know is that the true measured values are most likely to be somewhere between 0.75 and 1.25. Therefore, the best that a modelled value should be expected to achieve is 1 +/- 0.25. \n",
    "\n",
    "Let's say we obtain modelled values for each observation: \n",
    " - 1.25\n",
    " - 0.75\n",
    "\n",
    " Let's calculate Phi for such a case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 1/0.25\n",
    "# res = (weight * (meas - sim)) ^ 2\n",
    "res1 = (weight * (1 - 1.25))**2\n",
    "res2 = (weight * (1 - 0.75))**2\n",
    "# sum the squared weighted residuals\n",
    "phi = res1 + res2\n",
    "phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it, the value of Phi is equal to the number of observations. Getting a better fit than that means we are just \"fitting noise\".\n",
    "\n",
    "### Back to Freyberg.\n",
    "\n",
    "So, how many nonzero observations do we have in the dataset? Recall this number is recorded in the control file and easily accessible through the `Pst.nnz_obs` attribute. So our current Phi (see `pst.phi`) is a bit higher than that. Hopefully history matching will help us bring it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Target phi:',pst.nnz_obs)\n",
    "print('Current phi:', pst.phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so we have our observation weights sorted out. Let's just make a quick check of how model outputs and measured values compare by looking at a one-to-one plot for each observation group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = pst.plot(kind=\"1to1\");\n",
    "pst.res.loc[pst.nnz_obs_names,:]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelated Transient Noise\n",
    "\n",
    "(Sounds like a pretty cool name for a band...) \n",
    "\n",
    "Lastly, let's also generate our observation covariance matrix. Often this step is omitted if the observation weights in the `* observation data` section reflect observation uncertainty. Here we will implement it expressly to demonstrate how to add autocorrelated transient noise! When is the last time you saw that in the wild?\n",
    "\n",
    "First let us start by generating a covariance matrix, using the weights recorded in the `pst.observation_data` section. This is accomplished with `pyemu.Cov.from_observation_data()`. This will generate a covariance matrix for **all** observations in the control file, even the zero-weighted ones. \n",
    "\n",
    "However, we are only interested in keeping the nonzero-weighted observations. We can use `pyemu` matrix-manipulation convenience functions to `.get()` the covariance matrix rows/columns that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "\n",
    "# generate cov matrix for all obs using weights in the control file\n",
    "obs_cov = pyemu.Cov.from_observation_data(pst, )\n",
    "\n",
    "# reduce cov down to only include non-zero obsverations\n",
    "obs_cov = obs_cov.get(row_names=pst.nnz_obs_names, col_names=pst.nnz_obs_names, )\n",
    "\n",
    "# side note: \n",
    "# we are saving the diagonal (e.g no correlation) obsevration uncertainty cov matrix\n",
    "# to an external file for use in a later tutorial\n",
    "obs_cov.to_coo(os.path.join(t_d,\"obs_cov_diag.jcb\"))\n",
    "\n",
    "# make it a dataframe to make life easier\n",
    "df = obs_cov.to_dataframe()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this returned a diagonal covariance matrix (e.g. only values in the diagonal are nonzero). This implies that there is no covariance between observation noise. \n",
    "\n",
    "You can plot the matrix, but due to the scale it wont look particularly interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(df.values)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...you can kinda see something there (that's the surface flow observations... large variance). Let's instead just look at a small part, focusing on observations that make up a single time series. \n",
    "\n",
    "So as you can see below, the matrix diagonal reflects individual observation uncertainties, whilst all the off-diagonals are zero (e.g. no correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_select = obs.loc[(obs.obgnme=='oname:hds_otype:lst_usecol:trgw-0-26-6') & (obs.weight>0)]\n",
    "\n",
    "plt.imshow(df.loc[obs_select.obsnme,obs_select.obsnme].values)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But recall that most of the observations are time series. Here we are saying \"how wrong the measurement is today, has nothing to do with how wrong it was yesterday\". Is that reasonable? For some noise (e.g. white noise), yes, surely. But perhaps not for all noise. So how can we express that \"if my measurement was wrong yesterday, then it is *likely* to be wrong in the same way today\"? Through covariance, that's how. We can use the same priciples that we employed to specify parameter spatial/temporal covariance earlier on.\n",
    "\n",
    "We will now demonstrate this for the single time series we looked at above. \n",
    "\n",
    "First, construct a geostatistical structure and variogram. Here we are using a range of 90 days; for an exponential variogram `a` is range/3. From these, construct a generic covariance matrix for generic parameters with \"coordinates\" corresponding to the observation times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(a=730,contribution=1.0)\n",
    "x = obs_select.time.astype(float).values #np.arange(float(obs.time.values[:11].max()))\n",
    "y = np.zeros_like(x)\n",
    "names = [\"obs_{0}\".format(xx) for xx in x]\n",
    "cov = v.covariance_matrix(x,y,names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, scale the values in the generic covariance matrix to reflect the observation uncertainties and take a peek.\n",
    "\n",
    "And there you have it, off-diagonal elements are no longer zero and show greater correlation between uncertainties of observation which happen closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_group = cov.x.copy()\n",
    "w = obs_select.weight.mean()\n",
    "v = (1./w)**2\n",
    "x_group *= v\n",
    "df.loc[obs_select.obsnme,obs_select.obsnme] = x_group\n",
    "\n",
    "plt.imshow(df.loc[obs_select.obsnme,obs_select.obsnme].values);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement that to loop over all the nonzero time-series observation groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obgnme in pst.nnz_obs_groups:\n",
    "    obs_select = obs.loc[(obs.obgnme==obgnme) & (obs.weight>0)]\n",
    "\n",
    "    v = pyemu.geostats.ExpVario(a=30,contribution=1.0)\n",
    "    x = obs_select.time.astype(float).values #np.arange(float(obs.time.values[:11].max()))\n",
    "    y = np.zeros_like(x)\n",
    "    names = [\"obs_{0}\".format(xx) for xx in x]\n",
    "    cov = v.covariance_matrix(x,y,names=names)\n",
    "\n",
    "\n",
    "    x_group = cov.x.copy()\n",
    "    w = obs_select.weight.mean()\n",
    "    v = (1./w)**2\n",
    "    x_group *= v\n",
    "    df.loc[obs_select.obsnme,obs_select.obsnme] = x_group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then re-construct a `Cov` object and record it to an external file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_cov_tv = pyemu.Cov.from_dataframe(df)\n",
    "obs_cov_tv.to_coo(os.path.join(t_d,\"obs_cov.jcb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you made it this far, good on you! Let's finish with some eye candy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = obs_cov_tv.to_pearson().x\n",
    "x[np.abs(x)<0.00001]= np.nan\n",
    "plt.imshow(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beautiful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing here, let's just see what this actually implies for our observation ensembles. As before, let's generate an ensemble of observations, but this time we pass the `obs_cov_tv` covarince matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_tv = pyemu.ObservationEnsemble.from_gaussian_draw(pst=pst, \n",
    "                                                num_reals=50,\n",
    "                                                cov=obs_cov_tv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a smaller subset of the same ensemble of obs timeseries with non-correlated noise we saw earlier, just to make it easy to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Non correlated observation noise:')\n",
    "plot_obs_ts(pst, oe, ts_obs, subset=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a subset of the ensemble of observation time series with autocorrelated noise. Note how these are smoother and less of a jumble of spaghetti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Autocorrelated observation noise:')\n",
    "ts_obs = [i for i in pst.nnz_obs_groups if 'sfr' in i]\n",
    "plot_obs_ts(pst, oe_tv, ts_obs, subset=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Final Aside: Weighting for Visibility\n",
    "\n",
    "(The following is an aside and does not apply for subsequent tutorials.)\n",
    "\n",
    "As discussed above, a practical means of accommodating this situation is to weight all observation groups\n",
    "such that they contribute an equal amount to the starting measurement objective function. In this\n",
    "manner, no single group dominates the objective function, or is dominated by others; the information\n",
    "contained in each group is therefore equally “visible” to PEST.\n",
    "\n",
    "The `Pst.adjust_weights()` method provides a mechanism to fine tune observation weights according to their contribution to the objective function. (Side note: the `PWTADJ1` utility from the PEST-suite automates this same process of \"weighting for visibility\".) \n",
    "\n",
    "We start by creating a dictionary of nonzero-weighted observation group names and their respective contributions to the objective function. For example, we will specify that we want each group to contribute a value of 100 to the objective function. (Why 100? No particular reason. Could just as easily be 1000. Or 578. Doesn't really matter. 100 is a nice round number though.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of group names and weights\n",
    "balanced_groups = {grp:100 for grp in pst.nnz_obs_groups}\n",
    "balanced_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `pst.adjust_weights()` method to adjust the observation weights in the `pst` control file object. (*Remember! This all only happens in memory. It does not get written to the PEST control file yet!*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pro Tip**: Of course, we could also assign variable proportions of the _a priori_ objective function to reflect which groups of observations we interpret as more or less important in guiding the history matching. We are not constrained to making the components equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all nonzero-weighted groups have a contribution of 100.0\n",
    "pst.adjust_weights(obsgrp_dict=balanced_groups,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now plot the Phi components again, voila! We have a nice even distribution of Phi from each observation group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phicomp = pd.Series(pst.phi_components)\n",
    "plt.pie(phicomp, labels=phicomp.index.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some **caution** is required here. Observation weights and how these pertain to history-matching *versus* how they pertain to generating an observation ensemble for use with `PESTPP-IES` or FOSM is a frequent source of confusion.\n",
    "\n",
    " - When **history-matching**, observation weights listed in the control file determine their contribution to the objective function, and therefore to the parameter estimation process. Here, observation weights may be assigned to reflect observation uncertainty, the balance required for equal \"visibility\", or other modeller-defined (and perhaps subjective...) measures of observation worth.  \n",
    " - When undertaking **uncertainty analysis**, weights should ideally reflect the inverse of observation error (or the standard deviation of measurement noise). Keep this in mind when using `PESTPP-GLM` or `PESTPP-IES`. If the observations in the control file do not have error-based weighting then (1) care is required if using `PESTPP-GLM` for FOSM and (2) make sure to provide an adequately prepared observation ensemble to `PESTPP-IES`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made it this far? Congratulations! Have a cookie :)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
