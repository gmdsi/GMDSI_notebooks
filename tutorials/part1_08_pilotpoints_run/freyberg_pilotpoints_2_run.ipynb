{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilot Points Run\n",
    "\n",
    "In this notebook we are going to calibrate the Freyberg model with pilot points as the parameterisation device for hydraulic conductivity. We are using the same PEST control file constructed in the \"freyberg pilotpoints setup\" notebook. \n",
    "\n",
    " \n",
    "### Admin\n",
    "We have provided some pre-cooked PEST dataset files, wraped around the modified Freyberg model. This is the same dataset introduced in the \"freyberg_pest_setup\" and subsequent notebooks. We pick up here after the \"freyberg pilotpoints setup\" notebook.\n",
    "\n",
    "The functions in the next cell import required dependencies and prepare a folder for you. This folder contains the model files and a preliminary PEST setup. Run the cells, then inspect the new folder named \"freyberg_mf6\" which has been created in your tutorial directory. (Just press `shift+enter` to run the cells). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "import psutil\n",
    "import shutil\n",
    "\n",
    "sys.path.insert(0,os.path.join(\"..\", \"..\", \"dependencies\"))\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd\n",
    "\n",
    "plt.rcParams['font.size'] = 10\n",
    "pyemu.plot_utils.font =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder containing original model files\n",
    "org_d = os.path.join('..', '..', 'models', 'monthly_model_files_1lyr_newstress')\n",
    "# a dir to hold a copy of the org model files\n",
    "tmp_d = os.path.join('freyberg_mf6')\n",
    "if os.path.exists(tmp_d):\n",
    "    shutil.rmtree(tmp_d)\n",
    "shutil.copytree(org_d,tmp_d)\n",
    "# get executables\n",
    "hbd.prep_bins(tmp_d)\n",
    "# get dependency folders\n",
    "hbd.prep_deps(tmp_d)\n",
    "# run our convenience functions to prepare the PEST and model folder\n",
    "hbd.prep_pest(tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the non-pilot point setup\n",
    "Just so we can compare; its quite quick so no worries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_base = pyemu.Pst(os.path.join(tmp_d,'freyberg.pst'))\n",
    "pst_base.control_data.noptmax=20\n",
    "par = pst_base.parameter_data\n",
    "par.loc['rch0', 'partrans'] = 'log'\n",
    "obs = pst_base.observation_data\n",
    "obs.loc[(obs.obgnme==\"gage-1\") & (obs['gage-1'].astype(float)<=4018.5), \"weight\"] = 0.005\n",
    "pst_base.write(os.path.join(tmp_d, 'freyberg.pst'))\n",
    "pyemu.os_utils.run(\"pestpp-glm freyberg.pst\", cwd=tmp_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_base = pyemu.Pst(os.path.join(tmp_d,'freyberg.pst'))\n",
    "assert pst_base.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall what phi we achieve usgin homogneous `hk1` and `rch0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_base.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can beat that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run the pilot point setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the PEST control file set up with pilot point parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenience function that builds a new control file with pilot point parameters for hk\n",
    "hbd.add_ppoints(tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the control file with pilot point parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(tmp_d,'freyberg_pp.pst'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We (the authors) have added a few more parameters in the background. These include well pumping rates and STR inflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.adj_par_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, run once to check that it works (trust but verify!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax=0\n",
    "pst.write(os.path.join(tmp_d, 'freyberg_pp.pst'))\n",
    "\n",
    "pyemu.os_utils.run(\"pestpp-glm freyberg_pp.pst\", cwd=tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if it completed sucessfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(tmp_d, 'freyberg_pp.pst'))\n",
    "assert pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right then, let's increase NOPTMAX and start parameter estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always remember to re-write the control file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(tmp_d, 'freyberg_pp.pst'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, good to go.\n",
    "\n",
    "Now, when using derivative-based methods such as are implemented in PEST++GLM (and PEST/PEST_HP) the cost of more adjustable parameters is ...more run-time. Recall that PEST(++) needs to run the model the same number of times as there are adjustable parameters in order to fill the Jacobian matrix.\n",
    "\n",
    "We just went from having 2 adjustable paramters to having 30. So this is going to take quite a bit longer. \n",
    "\n",
    "Up until now, we have been running a single instance of `pestpp-glm`. Now, we are going to run `pestpp-glm` in parallel. \n",
    "\n",
    "To speed up the process, you will want to distribute the workload across as many parallel agents as possible. Normally, you will want to use the same number of agents (or less) as you have available CPU cores. Most personal computers (i.e. desktops or laptops) these days have between 4 and 10 cores. Servers or HPCs may have many more cores than this. Another limitation to keep in mind is the read/write speed of your machines disk (e.g. your hard drive). PEST and the model software are going to be reading and writting lots of files. This often slows things down if agents are competing for the same resources to read/write to disk.\n",
    "\n",
    "The first thing we will do is specify the number of agents we are going to use.\n",
    "\n",
    "### Attention!\n",
    "\n",
    "You must specify the number which is adequate for ***your*** machine! Make sure to assign an appropriate value for the following `num_workers` variable. (If you are unsure how many cores you have, you can use `psutil` to check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psutil.cpu_count(logical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of parallel agents\n",
    "num_workers = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shall specify the PEST run-manager/master directory folder as `m_d`. This is where outcomes of the PEST run will be recorded. It should be different from the `t_d` folder, which contains the \"template\" of the PEST dataset. This keeps everything separate and avoids silly mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d='master_pp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell deploys the PEST agents and manager and then starts the run using `pestpp-glm`. Run it by pressing `shift+enter`.\n",
    "\n",
    "If you wish to see the outputs in real-time, switch over to the terminal window (the one which you used to launch the `jupyter notebook`). There you should see `pestpp-glm`'s progress written to the terminal window in real-time. \n",
    "\n",
    "If you open the tutorial folder, you should also see a bunch of new folders there named `worker_0`, `worker_1`, etc. These are the agent folders. The `m_d` folder is where the manager is running. \n",
    "\n",
    "This run should take several minutes to complete (depending on the number of workers and the speed of your machine). If you get an error, make sure that your firewall or antivirus software is not blocking `pestpp-glm` from communicating with the agents (this is a common problem!).\n",
    "\n",
    "> **Pro Tip**: Running PEST from within a `jupyter notebook` has a tendency to slow things down and hog alot of RAM. When modelling in the \"real world\" it is more efficient to implement workflows in scripts which you can call from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(tmp_d, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-glm', #the PEST software version we want to run\n",
    "                            'freyberg_pp.pst', # the control file to use with PEST\n",
    "                            num_workers=num_workers, #how many agents to deploy\n",
    "                            worker_root='.', #where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, #the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcomes\n",
    "\n",
    "Re-load the control file and check the new Phi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d, 'freyberg_pp.pst'))\n",
    "assert pst.phi!=pst_base.phi\n",
    "pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet - we did way better! More parameters means more \"flexibility\" for PEST to obtain a better fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.phi / pst_base.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the Phi progress. Not too bad. Looks like PEST could still do better if we let it carry on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obj = pd.read_csv(os.path.join(m_d,\"freyberg_pp.iobj\"),index_col=0)\n",
    "df_obj.total_phi.plot()\n",
    "plt.ylabel('total_phi');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the fits with measured values? Doing better than before, for sure. Not perfect, but pretty good.\n",
    "\n",
    " > (side note: recall we \"conveniently\" didn't add pilot points for recharge as well...not to mention all the other poorly known or unknowable parameter values...pumping rates, storage parameters, GHB parameters, aquifer geometry...etc, etc...perhaps we should have?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = pst.plot(kind=\"1to1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the posterior parameter uncetanties for each group of pilot points (`hk1` and `rchpp`). The next cell plots the prbability distribution for each parameter in each parameter group. Recall that each pilot point is assigned a unique parmaeter, so in each plot we are displaying 30 distributions. We are also plotting the parmater upper and lower bounds as vertical dashed black lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.adj_par_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "df_paru = pd.read_csv(os.path.join(m_d,\"freyberg_pp.par.usum.csv\"),index_col=0)\n",
    "\n",
    "fig, axes=plt.subplots(1,len(pst.adj_par_groups),figsize=(12,5))\n",
    "i=0\n",
    "for pargp in pst.adj_par_groups:\n",
    "    ax = axes[i]\n",
    "    i+=1\n",
    "    pars = par.loc[par.pargp==pargp].parnme.values\n",
    "    df_par = df_paru.loc[pars,:]\n",
    "    ax = pyemu.plot_utils.plot_summary_distributions(df_par,label_post=False, ax=ax)\n",
    "    mn = np.log10(pst.parameter_data.loc[pars[0].lower(),\"parlbnd\"])\n",
    "    mx = np.log10(pst.parameter_data.loc[pars[0].lower(),\"parubnd\"])\n",
    "    ax.set_title(pargp)\n",
    "    ax.plot([mn,mn],ax.get_ylim(),\"k--\")\n",
    "    ax.plot([mx,mx],ax.get_ylim(),\"k--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes - that doesn't look good. Lots of parameters are right up againts the bounds. (The center of the distributions is centered on the black lines.) This implies that they are taking on exteme values in order to achieve that \"excelent\" fit with measured data. \n",
    "\n",
    "Is this realistic? No, not really. It often means parameters are taking on compensatory roles to make up for strucutural error or poor conceptualization.\n",
    "\n",
    "Another noteworthy aspect is that `rchpp` parameters have deviated from the prior. But most of the `hk1` parameters have not changed much. Why do you think this is? Usually we assume that we are more confident that we \"know\" recharge better than hydraulic conductivity. After all, hydraulic conductivity can vary by orders of magnitude, whislt recharge is usually only expected to vary within an order of magnitude (_big ugly rule of thumb alert!_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the calibrated parameter field. First we just need to update the model files with the calibrated parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parrep(os.path.join(m_d, \"freyberg_pp.par\" ))\n",
    "pst.write_input_files(pst_path=m_d)\n",
    "pyemu.geostats.fac2real(os.path.join(m_d,\"hkpp.dat\"),\n",
    "                        factors_file=os.path.join(m_d,\"hkpp.dat.fac\"),\n",
    "                        out_file=os.path.join(m_d,\"freyberg6.npf_k_layer1.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run('python forward_run.py', cwd=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then use `flopy` to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp = pyemu.pp_utils.pp_tpl_to_dataframe(os.path.join(m_d,\"hkpp.dat.tpl\"))\n",
    "sim = flopy.mf6.MFSimulation.load(sim_ws=m_d, verbosity_level=0) #modflow.Modflow.load(fs.MODEL_NAM,model_ws=working_dir,load_only=[])\n",
    "gwf= sim.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "mm = flopy.plot.PlotMapView(model=gwf, ax=ax, layer=0)\n",
    "\n",
    "k = gwf.npf.k.get_data()\n",
    "ca = mm.plot_array(np.log10(k), masked_values=[1e30],)\n",
    "cb = plt.colorbar(ca, shrink=0.5)\n",
    "cb.ax.set_title('$Log_{10}K$')\n",
    "\n",
    "mm.plot_grid(alpha=0.5)\n",
    "mm.plot_inactive()\n",
    "ax.set_title('$K$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that compare to the true values? The patterns are not very similar...and those \"bulls-eyes\" in the calibrated field don't look very realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwf = hbd.plot_truth_k(m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is wrong... We better checkout the forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs, axes = pyemu.plot_utils.plot_summary_distributions(os.path.join(m_d,\n",
    "                    \"freyberg_pp.pred.usum.csv\"),subplots=True, )\n",
    "for ax in axes:\n",
    "    fname = ax.get_title().lower()\n",
    "    ylim = ax.get_ylim()\n",
    "    v = pst.observation_data.loc[fname,\"obsval\"]\n",
    "    ax.plot([v,v],ylim,\"b--\")\n",
    "    ax.set_ylim(0, ylim[-1])\n",
    "figs[0].tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sheesh! What happened? We are still failing to capture many of our forecast \"truths\" in the posterior. The answer: overfitting. We specified lots of parameters, so we are able to fit the observations really well - too well.  \n",
    "\n",
    "Even though we are able to measure water levels very precisely, the model has problems (model error, also referred to as structural error), so we shouldn't expect the model to reproduce the observations so well.  So how do we control this overfitting? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last few tutorials have shown us a few things:\n",
    " - Having more parameters than observations allows us to have an overdetermined, and therefore solvable, inverse problem. But! It under-represents uncertainty. Using few parameters will by default fail to capture nuances of properties and processes of a complex real world system. \n",
    " - More parameters introduces more flexibility. This can mean better fits with calibration data. It also may mean more uncertainty. (This leads to an often touted falacy that a model should \"use fewer parameters to have less uncertainty\". Doing so doesn't reduce uncertainty. It merely ignores it.) \n",
    " - But more parameters also means more problems. \n",
    "     - Parameters may be correlated, resulting in non-unique solutions to the inverse problem. \n",
    "     - But more parameters than observations results in an ill-posed inverse problem. We might be able to achive excelent fits with obsevration data, but this comes at the costof \"unreasoanble\" parameter values (e.g. overfitting). \n",
    "     \n",
    "So how can we use many parameters and avoid overfitting? Enter \"regularization\". Stick around for the next episode."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
