{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Monte Carlo\n",
    "\n",
    "Prior-based (or \"unconstrained\") Monte Carlo is a useful but underused analysis. It is conceptually simple, does not require much in terms of algorithmic controls and forces the modeller to think about the prior parameter probability distribution - both the mean vector (i.e. the initial parameter values) and the prior parameter covariance matrix. \n",
    "\n",
    "The idea is simple: sample many sets of parameters (i.e. an ensemble) from a prior probability distribution, run the forward model for each realization in this ensemble, and then collate the results. Do not try and fit historical data (yet!). Do not throw any of the simulations out because they \"do not represent historical data well\". This allows us to explore the entire range of model outcomes across the (prior) range of parameter values. It lets us investigate model stability (e.g. can the model setup handle the parameters we are throwing at it?). It also lets us start to think critically about what observations the model will be able to match. If we can't match observations we can explore deficiencies in the parameters, the model, or even the observation values themselves.\n",
    "\n",
    "Sometimes, it shows us that history matching is not required - saving us a whole lot of time and effort!\n",
    "\n",
    "In this notebook we will demonstrate:\n",
    " - how to use `pyEMU` to run `PESTPP` in parallel locally (that is, on your machine only)\n",
    " - using `PESTPP-SWP` or `PESTPP-IES` to undertake prior Monte Carlo with an existing geostatistically correlated prior parameter ensemble\n",
    " - using `PESTPP-IES` to undertake prior Monte Carlo with an uncorrelated prior parameter ensemble \n",
    " - postprocessing stochastic model outputs\n",
    "\n",
    "### The modified Freyberg PEST dataset\n",
    "\n",
    "The modified Freyberg model is introduced in another tutorial notebook (see [\"intro to freyberg model\"](../part0_02_intro_to_freyberg_model/intro_freyberg_model.ipynb)). The current notebook picks up following the [\"freyberg psfrom pest setup\"](../part2_01_pstfrom_pest_setup/freyberg_pstfrom_pest_setup.ipynb) notebook, in which a high-dimensional PEST dataset was constructed using `pyemu.PstFrom`. You may also wish to go through the [\"intro to pyemu\"](../part0_intro_to_pyemu/intro_to_pyemu.ipynb) and [\"pstfrom sneakpeak\"](../part1_02_pest_setup/pstfrom_sneakpeak.ipynb) notebooks beforehand.\n",
    "\n",
    "The next couple of cells load necessary dependencies and call a convenience function to prepare the PEST dataset folder for you. This is the same dataset that was constructed during the [\"freyberg psfrom pest setup\"](../part2_01_pstfrom_pest_setup/freyberg_pstfrom_pest_setup.ipynb) tutorial. Simply press `shift+enter` to run the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt;\n",
    "import psutil \n",
    "\n",
    "import sys\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the temporary working folder\n",
    "t_d = os.path.join('freyberg6_template')\n",
    "if os.path.exists(t_d):\n",
    "    shutil.rmtree(t_d)\n",
    "\n",
    "org_t_d = os.path.join(\"..\",\"part2_02_obs_and_weights\",\"freyberg6_template\")\n",
    "if not os.path.exists(org_t_d):\n",
    "    raise Exception(\"you need to run the '/part2_02_obs_and_weights/freyberg_obs_and_weights.ipynb' notebook\")\n",
    "shutil.copytree(org_t_d,t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the PEST control file as a `Pst` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_path = os.path.join(t_d, 'freyberg_mf6.pst')\n",
    "pst = pyemu.Pst(os.path.join(t_d, 'freyberg_mf6.pst'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if obs&weights notebook has been run\n",
    "if not pst.observation_data.observed.sum()>0:\n",
    "    raise Exception(\"You need to run the '/part2_02_obs_and_weights/freyberg_obs_and_weights.ipynb' notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the prior parameter ensemble we generated previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(t_d) if f.endswith(\".jcb\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pyemu.ParameterEnsemble.from_binary(pst=pst,filename=os.path.join(t_d,\"prior_pe.jcb\"))\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Ensemble in Parallel\n",
    "\n",
    "Here we are going to make use of `PESTPP-SWP` to execute the prior Monte Carlo in parallel. `PESTPP-SWP` is a simple parametric sweep utility to run a collection of parameter sets in parallel and collate the results.\n",
    "\n",
    "So let's start by specifying the name of the prior parameter ensemble file that we generated previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options['sweep_parameter_csv_file'] = 'prior_pe.jcb'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, rewrite the PEST control file. If you open `freyberg_mf6.pst` in a text editor, you'll see a new PEST++ control variable has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 0 # this is ignored by pestpp-swp, but we can use it to do a test run below\n",
    "pst.write(os.path.join(t_d, 'freyberg_mf6.pst'),version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always advisable to do the good ol' `noptmax=0` test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"pestpp-ies freyberg_mf6.pst\",cwd=t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to run `PESTPP-SWP` in parallel. \n",
    "\n",
    "To speed up the process, you will want to distribute the workload across as many parallel agents as possible. Normally, you will want to use the same number of agents (or fewer) as you have available CPU cores. Most personal computers (i.e. desktops or laptops) these days have between 4 and 10 cores. Servers or HPC cluster nodes may have many more cores than this. Another limitation to keep in mind is the read/write speed of your machines disk (e.g. your hard drive). PEST and the model software are going to be reading and writing lots of files. This often slows things down if agents are competing for the same resources to read/write to disk.\n",
    "\n",
    "The first thing we will do is specify the number of agents we are going to use.\n",
    "\n",
    "# Attention!\n",
    "\n",
    "You must specify the number which is adequate for ***your*** machine! Make sure to assign an appropriate value for the following `num_workers` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = psutil.cpu_count(logical=False) # update this according to your resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shall specify the PEST run-manager/master directory folder as `m_d`. This is where outcomes of the PEST run will be recorded. It should be different from the `t_d` folder, which contains the \"template\" of the PEST dataset. This keeps everything separate and avoids silly mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = os.path.join('master_priormc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell deploys the PEST agents and manager and then starts the run using `PEST-SWP` (using `pestpp-swp freyberg_mf6.pst /h localhost:4004` on the agents, and `pestpp-swp freyberg_mf6.pst /h :4004` on the manager).\n",
    "\n",
    "Run it by pressing `shift+enter`.\n",
    "\n",
    "If you open the tutorial folder, you should also see a bunch of new folders there named `worker_0`, `worker_1`, etc. These are the agent folders. The `master_priormc` folder is where the manager is running. \n",
    "\n",
    "This run may take several minutes to complete (depending on the number of workers and the speed of your machine). If you get an error, make sure that your firewall or antivirus software is not blocking `PEST-SWP` from communicating with the agents (this is a common problem!).\n",
    "\n",
    "> **Pro Tip**: Running PEST from within a Jupyter notebook has a tendency to slow things down and hog a lot of RAM. When modelling in the \"real-world\" it is more efficient to implement workflows in scripts which you can call from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(t_d, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-swp', #the PEST software version we want to run\n",
    "                            'freyberg_mf6.pst', # the control file to use with PEST\n",
    "                            num_workers=num_workers, #how many agents to deploy\n",
    "                            worker_root='.', #where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, #the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Outcomes\n",
    "\n",
    "By default, `PESTPP-SWP` writes the results of the parametric sweep to a csv file called `sweep_out.csv`.  This file has columns for each observation listed in the control file, plus columns for total phi and phi for each observation group (calculated using the weights in the control file).  It also has columns for the `input_run_id` and `failed_flag` to help you align these outputs with the inputs and also track any failed runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's check if any runs failed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.read_csv(os.path.join(m_d,\"sweep_out.csv\"),index_col=0)\n",
    "print('number of realization in the ensemble before dropping: ' + str(obs_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = obs_df.loc[obs_df.failed_flag==0,:]\n",
    "print('number of realization in the ensemble **after** dropping: ' + str(obs_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are they the same? Good, that means none failed. If any had failed, this would be an opportunity to go and figure out why, by identifying the parameter realisations that failed and figuring out why they may have had trouble converging. This might lead to discovering inadequacies in the model configuration and/or parameterisation.  Better to catch them now, before spending a lot of effort history matching the model... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the distribution of Phi obtained for the ensemble. Some pretty high values there. But that's fine. We are not concerned with getting a \"good fit\" in prior MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.phi.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More important is to inspect whether the ***distribution*** of simulated observations encompass measured values. Our first concern is to ensure that the model is ***able*** to captured observed behaviour. If measured values do not fall within the range of simualted values, this is a sign that something ain't right and we should revisit our model or prior parameter distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check is to plot stochastic (ensemble-based) one-to-one plots. We can plot one-to-one plots for obsvervation groups using the `pyemu.plot_utils.ensemble_res_1to1()` method. However, in our case that will result in lots of plots (we have many observation groups!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyemu.plot_utils.ensemble_res_1to1(obs_df, pst);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to uncomment the previous cell and see what happens. This can be useful for a quick review, but for the purposes of this tutorial, let's just look at four observation groups (recall, each group is made up of a time series of observations from a single location).\n",
    "\n",
    "Now, this plot does not look particularily pretty... but we aren't here for pretty, we are here for results! What are we concerned with? Whether the range of ensemble simulated outcomes form the prior covers the measured values. Recall that plots on the left are one-to-one plots and on the right the residuals are displayed. In both cases, a grey line represents the range of simulated values for a given observation\n",
    "\n",
    "In plots on the left, each grey line should interesect the one-to-one line. In the plots on the right, each grey line should intersect the \"zero-residual\" line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_weighted_obs_groups = [i for i in pst.obs_groups if i not in pst.nnz_obs_groups]\n",
    "len(zero_weighted_obs_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.plot_utils.ensemble_res_1to1(obs_df, pst, skip_groups=zero_weighted_obs_groups); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the prior covers the \"measured\" values (which is good).\n",
    "\n",
    "But hold on a second! What about measurement noise? If we are saying that it is *possible* that our measurements are wrong by a certain amount, shouldn't we make sure our model can represent conditions in which they are? Yes, of course!\n",
    "\n",
    "No worries, `pyemu` has you covered. Let's quickly cook up an ensemble of observations with noise. (Recall we recorded a covariance matrix of observation noise during the \"freyberg pstfrom pest setup\" notebook; this has also been discussed in the \"observation and weights\" notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_cov = pyemu.Cov.from_binary(os.path.join(t_d, 'obs_cov.jcb'))\n",
    "obs_plus_noise = pyemu.ObservationEnsemble.from_gaussian_draw(pst=pst, cov=obs_cov);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's plot that again but with observation noise. \n",
    "\n",
    "Aha! Good, not only do our ensemble of model outcomes cover the measured values, but they also entirely cover the range of measured values with noise (red shaded area in the plot below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.plot_utils.ensemble_res_1to1(obs_df,\n",
    "                                    pst, \n",
    "                                    skip_groups=zero_weighted_obs_groups,\n",
    "                                    base_ensemble=obs_plus_noise); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another, perhaps coarser, method to quickly explore outcomes is to look at histograms of observations. \n",
    "\n",
    "The following figure groups observations according to type (just to lump them together and make a smaller plot) and then plots histograms of observation values. Grey shaded columns represent simulated values from the prior. Red shaded columns represent the ensemble of measured values + noise. The grey columns should ideally be spread wider than the red columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = pst.observation_data.loc[pst.nnz_obs_names].apply(lambda x: x.usecol + \" \"+x.oname,axis=1).to_dict()\n",
    "plot_cols = {v: [k] for k, v in plot_cols.items()}\n",
    "pyemu.plot_utils.ensemble_helper({\"r\":obs_plus_noise,\"0.5\":obs_df}, \n",
    "                                  plot_cols=plot_cols,bins=20,sync_bins=True,\n",
    "                                  )\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the observed versus simulated time series: everyone's favorite!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.try_parse_name_metadata()\n",
    "obs = pst.observation_data.copy()\n",
    "obs = obs.loc[obs.oname.apply(lambda x: x in [\"hds\",\"sfr\"])]\n",
    "obs = obs.loc[obs.obgnme.apply(lambda x: x in pst.nnz_obs_groups),:]\n",
    "obs.obgnme.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ogs = obs.obgnme.unique()\n",
    "fig,axes = plt.subplots(len(ogs),1,figsize=(10,5*len(ogs)))\n",
    "ogs.sort()\n",
    "for ax,og in zip(axes,ogs):\n",
    "    oobs = obs.loc[obs.obgnme==og,:].copy()\n",
    "    oobs.loc[:,\"time\"] = oobs.time.astype(float)\n",
    "    oobs.sort_values(by=\"time\",inplace=True)\n",
    "    tvals = oobs.time.values\n",
    "    onames = oobs.obsnme.values\n",
    "    [ax.plot(tvals,obs_df.loc[i,onames].values,\"0.5\",lw=0.5,alpha=0.5) for i in obs_df.index]\n",
    "    oobs = oobs.loc[oobs.weight>0,:]\n",
    "    ax.plot(oobs.time,oobs.obsval,\"r-\",lw=2)\n",
    "    ax.set_title(og,loc=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasts\n",
    "\n",
    "As usual, we bring this story back to the forecasts - after all they are why we are modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.forecast_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will plot the distribution of each forecast obtained by running the prior parameter ensemble. Because we are using a synthetic model, we also have the privilege of being able to plot the \"truth\" (in the real world we don't know the truth of course). \n",
    "\n",
    "Many modelling analyses could stop here. If outcomes from a prior MC analysis show that the simulated distribution of forecasts *does not* cause some \"bad thing\" to happen within an \"acceptable\" confidence, then you are done. No need to go and do expensive and time-consuming history matching! \n",
    "\n",
    "On the other hand, if the uncertainty (i.e. variance) is unacceptably wide, then it *may* be justifiable to try to reduce forecast uncertainty through history matching. But only if you have forecast-sensitive observation data, and if the model is amenable to assimilating these data! How do I know that you ask? Worry not, we will get to this in subsequent tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in pst.forecast_names:\n",
    "    plt.figure()\n",
    "    ax = obs_df.loc[:,forecast].plot(kind=\"hist\",color=\"0.5\",alpha=0.5, bins=20)\n",
    "    ax.set_title(forecast)\n",
    "    fval = pst.observation_data.loc[forecast,\"obsval\"]\n",
    "    ax.plot([fval,fval],ax.get_ylim(),\"r-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior MC without Covariance\n",
    "\n",
    "The workflow above demonstrated how to use `PESTPP-SWP` to complete a \"sweep\" of model runs. Here, we undertook prior Monte Carlo by running models with an ensemble of parameter sets sampled from the prior parameter probability distribution. \n",
    "\n",
    "The same can be accomplished with `PESTPP-IES` by assigning the `NOPTMAX` control variable to `-1` and either providing `PESTPP-IES` with a predefined parameter ensemble (the same as we did for `PESTPP-SWP`) or by providing the parameter covariance matrix and allowing `PESTPP-IES` to sample from the prior itself.\n",
    "\n",
    "The next few cells do something slightly different. Here we will use `PESTPP-IES` to undertake prior MC, but assuming no correlation between parameters. Here, prior parameter uncertainty is diagonal and determined solely by the parameter bounds in the PEST control file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set some PEST++ options and rewrite the control file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options['ies_num_reals'] = 250\n",
    "pst.control_data.noptmax = -1\n",
    "pst.write(os.path.join(t_d,\"freyberg_diagprior.pst\"),version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `PESTPP-IES`. This should take about the same amount of time as `PESTPP-SWP` did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = \"master_diagonal_prior_monte_carlo\" \n",
    "pyemu.os_utils.start_workers(t_d,\"pestpp-ies\",\"freyberg_diagprior.pst\",\n",
    "                            num_workers=num_workers,\n",
    "                            worker_root='.',\n",
    "                            master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in the results. Let's just look at the forecasts. (Feel free to repeat the plots we did above if you wish to compare them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.read_csv(os.path.join(m_d,\"freyberg_diagprior.0.obs.csv\"),index_col=0)\n",
    "\n",
    "pst.try_parse_name_metadata()\n",
    "obs = pst.observation_data.copy()\n",
    "obs = obs.loc[obs.oname.apply(lambda x: x in [\"hds\",\"sfr\"])]\n",
    "obs = obs.loc[obs.obgnme.apply(lambda x: x in pst.nnz_obs_groups),:]\n",
    "obs.obgnme.unique()\n",
    "ogs = obs.obgnme.unique()\n",
    "fig,axes = plt.subplots(len(ogs),1,figsize=(10,5*len(ogs)))\n",
    "ogs.sort()\n",
    "for ax,og in zip(axes,ogs):\n",
    "    oobs = obs.loc[obs.obgnme==og,:].copy()\n",
    "    oobs.loc[:,\"time\"] = oobs.time.astype(float)\n",
    "    oobs.sort_values(by=\"time\",inplace=True)\n",
    "    tvals = oobs.time.values\n",
    "    onames = oobs.obsnme.values\n",
    "    [ax.plot(tvals,obs_df.loc[i,onames].values,\"0.5\",lw=0.5,alpha=0.5) for i in obs_df.index]\n",
    "    oobs = oobs.loc[oobs.weight>0,:]\n",
    "    ax.plot(oobs.time,oobs.obsval,\"r-\",lw=2)\n",
    "    ax.set_title(og,loc=\"left\")\n",
    "\n",
    "    \n",
    "for forecast in pst.forecast_names:\n",
    "    plt.figure()\n",
    "    # plot histogram of forecast values recorded in the simulated prior observation csv\n",
    "    ax = obs_df.loc[:,forecast].plot(kind=\"hist\",color=\"0.5\",alpha=0.5, bins=20)\n",
    "    ax.set_title(forecast)\n",
    "    # plot the forecast value in the \"observation data\"; this value happens to be the \"truth\"\n",
    "    fval = pst.observation_data.loc[forecast,\"obsval\"]\n",
    "    ax.plot([fval,fval],ax.get_ylim(),\"r-\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
