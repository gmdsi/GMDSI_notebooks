{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Monte Carlo\n",
    "\n",
    "Prior-based (or \"unconstrained\") Monte Carlo is a usefull, but quite often underused, analysis. It is conceptually simple, does not require much in terms of algorithmic controls and forces the modeller to think about the prior parameter probability distribution - both the mean vector (i.e. the initial parameter values) and the prior parameter covariance matrix. \n",
    "\n",
    "The idea is simple: sample many sets of parameters (i.e. an ensemble) from a prior probability distribution and simulate the model with this parameter ensemble. Do not try and fit historical data (yet!). Do not throw any of the simulations out because they \"do not represent historical data well\". This allows us to explore the entire range of model outcomes across the (prior) range of parameter vaues. It let's us investigate model stability (e.g. can the model setup handle the parameters we are throwing at it?). It also let's us start to think critically about what observations the model will be able to match.\n",
    "\n",
    "Sometimes, it shows us that history matching is not required - saving us a whole lot of time and effort!\n",
    "\n",
    "In this notebook we will demonstrate:\n",
    " - how to generate a parameter ensemble, making use of a (previously prepared) parameter covariance matrix.\n",
    " - how to use `pyemu` to run `pestpp` in parallel\n",
    " - using `pestpp-swp` to batch process a series of modeller-prepared model runs\n",
    " - using `pestpp-ies` to accomplish the same thing\n",
    " - post-processing stochastic model outputs\n",
    "\n",
    "### 1. The modified Freyberg PEST dataset\n",
    "\n",
    "The modified Freyberg model is introduced in another tutorial notebook (see \"freyberg intro to model\"). The current notebook picks up following the \"freyberg psfrom pest setup\" notebook, in which a high-dimensional PEST dataset was constructed using `pyemu.PstFrom`. You may also wish to go through the \"intro to pyemu\" notebook beforehand.\n",
    "\n",
    "The next couple of cells load necessary dependencies and call a convenience function to prepare the PEST dataset folder for you. This is the same dataset that was constructed during the \"freyberg pstfrom pest setup\" tutorial. Simply press `shift+enter` to run the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pyemu\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# import pre-prepared convenience functions\n",
    "import herebedragons as hbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the temporary working folder\n",
    "t_d = os.path.join('freyberg6_template')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the convenience function to get the pre-preprepared PEST dataset;\n",
    "# this is the same dataset consutructed in the \"obs and weights\" tutorial\n",
    "hbd.dir_cleancopy(org_d=os.path.join('..','..', 'models','freyberg_obs_and_weights'), \n",
    "                new_d=t_d)\n",
    "# get and unzip the prior covariance JCB file\n",
    "hbd.unzip(os.path.join('..','..','models','prior_cov.zip'), os.path.join(t_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the PEST control file as a `Pst` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(t_d, 'freyberg_mf6.pst'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating the Prior Parameter Ensemble\n",
    "\n",
    "We can use one of the `pyemu.ParameterEnsemble` class `draw` methods to generate stochastic values from (multivariate) (log) gaussian, uniform and triangular distributions.  Much of what we do is predicated on the gaussian distribution, so let's use that here.\n",
    "\n",
    "These `draw` methods use initial parameter values in the control file (the `Pst.parameter_data.parval1` attribute) as the $\\boldsymbol{\\mu}$  (mean) prior parameter vector.  \n",
    "\n",
    "The gaussian draw accepts a `cov` arg which can be a `pyemu.Cov` instance.  If this isn't passed, then the draw method constructs a diagonal covariance matrix from the parameter bounds (assuming a certain number of standard deviations represented by the distance between the bounds - the `sigma_range` argument). \n",
    "\n",
    "Recall from the \"freyberg pstfrom pest setup\" tutorial, that we generated a parameter covariance matrix and stored this as an external file named `prior_cov.jcb`. This covariance matrix describes our prior parameter uncertainties and how these are correlated to each other. Let's start by reading this file as a `pyemu.Cov` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = pyemu.Cov.from_binary(os.path.join(t_d, 'prior_cov.jcb'))\n",
    "\n",
    "# take a peek at the matrix if you like\n",
    "#cov.to_dataframe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pyemu.ParameterEnsemble.from_gaussian_draw(pst=pst, # the pest control file from which parameter means and bounds are obtained\n",
    "                                                cov=cov, # the parameter prior covariance matrix\n",
    "                                                num_reals=200) # the number of realisations in the ensemble\n",
    "# enforce parameter bounds\n",
    "pe.enforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pyemu.ParameterEnsemble` object can be accesed as a `Pandas.DataFrame`. Makes it easier to slice and dice using `.loc` or `.iloc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of one of the parameters across the ensemble. Plot a histogram for one of the K parameters listed in the `pst` control file. \n",
    "\n",
    "As expected, it shows a log-normal distribution (recall that K parmaeters are log-transformed) and it respects the parmaeter upper/lower bounds specified in the control file (beacause we used `pe.enforce()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.loc[:,pst.par_names[0]].hist(bins=50,facecolor=\"0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated the prior ensemble, we can record it as an external file for future use. This can be a CSV or binary (JCB or JCO) file.\n",
    "\n",
    "Let's record it as a JCB (the file will be a bit smaller)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.to_binary(os.path.join(t_d, 'prior_pe.jcb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the Ensemble in Parallel\n",
    "\n",
    "Here we are going to make use of the `pestpp-swp`. Alternatively, we could use `pestpp-ies` instead.\n",
    "\n",
    "`pestpp-swp` runs a a a model using a suite of parameter fields which must be supplied in a CSV or binary file. Model outputs calculated with these parameter fields are recorded in another CSV file. By default, `pestpp-swp` assumes that the parameter fields are in the file `sweep_in.csv`. If they are not (as is our case) the user must provide the respective file name. \n",
    "\n",
    "So let's start by specifying the parameter input file fro `pestpp-swp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options['sweep_parameter_csv_file'] = 'prior_pe.jcb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, re-write the PEST control file. If you open `priormc.pst` in a text editor, you'll see a new PEST++ control variable has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d, 'priormc.pst'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to run `pestpp-swp` in parallel. \n",
    "\n",
    "To speed up the process, you will want to distribute the workload across as many parallel agents as possible. Normally, you will want to use the same number of agents (or less) as you have available CPU cores. Most personal computers (i.e. desktops or laptops) these days have between 4 and 10 cores. Servers or HPCs may have many more cores than this. Another limitation to keep in mind is the read/write speed of your machines disk (e.g. your hard drive). PEST and the model software are going to be reading and writting lots of files. This often slows things down if agents are competing for the same resources to read/write to disk.\n",
    "\n",
    "The first thing we will do is specify the number of agents we are going to use.\n",
    "\n",
    "# Attention!\n",
    "\n",
    "You must specify the number which is adequate for ***your*** machine! Make sure to assign an appropriate value for the following `num_workers` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shall specify the PEST run-manager/master directory folder as `m_d`. This is where outcomes of the PEST run will be recorded. It should be different from the `t_d` folder, which contains the \"template\" of the PEST dataset. This keeps everything separate and avoids silly mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = os.path.join('master_priormc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell deploys the PEST agents and manager and then starts the run using `pestpp-swp`. Run it by pressing `shift+enter`.\n",
    "\n",
    "If you wish to see the outputs in real-time, switch over to the terminal window (the one which you used to launch the `jupyter notebook`). There you should see `pestpp-swp`'s progress. \n",
    "\n",
    "If you open the tutorial folder, you should also see a bunch of new folders there named `worker_0`, `worker_1`, etc. These are the agent folders. The `master_priormc` folder is where the manager is running. \n",
    "\n",
    "This run should take several minutes to complete (depending on the number of workers and the speed of your machine). If you get an error, make sure that your firewall or antivirus software is not blocking `pestpp-swp` from communicating with the agents (this is a common problem!).\n",
    "\n",
    "> **Pro Tip**: Running PEST from within a `jupyter notebook` has a tendency to slow things down and hog alot of RAM (at least if you are using Visual Studio Code, as I am). When modelling in the \"real world\" it is more efficient to implement workflows in scripts which you can call from the command line. For example, for this case it took me 20min when running `pestpp-swp` from the `jupyter notebook`, but only 5min when running form the comand line. If you inspect the tutorial folder, you will find a file named `run_swp.py` that accomplishes this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(t_d, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-swp', #the PEST software version we want to run\n",
    "                            'priormc.pst', # the control file to use with PEST\n",
    "                            num_workers=num_workers, #how many agents to deploy\n",
    "                            worker_root='.', #where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, #the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore the Outcomes\n",
    "\n",
    "By default, `pestpp-swp` writes the results of the parametric sweep to a csv file called `sweep_out.csv`.  This file has columns for each observation listed in the control file, plus columns for total phi and phi for each observation group (calculated using the weights in the control file).  It also has columns for the `input_run_id` and `failed_flag` to help you align these outputs with the inputs and also track any failed runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's check if any runs failed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.read_csv(os.path.join(m_d,\"sweep_out.csv\"),index_col=0)\n",
    "print('number of realization in the ensemble before dropping: ' + str(obs_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = obs_df.loc[obs_df.failed_flag==0,:]\n",
    "print('number of realization in the ensemble **after** dropping: ' + str(obs_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are they the same? Good, that means none failed. If any had failed, this would be an opportunity to go and figure out why, by identifying the parameter realisations that failed and figuring out why they may have had trouble converging. This might lead to discovering inadequacies in the model configuration and/or parameterisation.  Better to catch them now, before spending alot of effort history matching the model... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the distribution of Phi obtained for the ensemble. Some pretty high values there. But that's fine. We are not concerned with getting a \"good fit\" in prior MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.phi.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More important is to inspect whether the ***distribution*** of simulated observations encompass measured values. Our first concern is to ensure that the model is ***able*** to captured observed behaviour. If measured values do not fall within the range of simualted values, this is a sign that something ain't right and we should revisit our model or prior parameter distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check is to plot stochastic (ensemble-based) 1-to-1 plots. We can plot 1to1 plots for obsvervation groups using the `pyemu.plot_utils.ensemble_res_1to1()` method. However, in our case that will result in lots of plots (we have many obs groups!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyemu.plot_utils.ensemble_res_1to1(obs_df, pst);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to uncomment the previous cell and see what happens. This can be usefull for a quick review, but for the purposes of this tutorial, let's just look at four observation groups (recall, each group is made up of a time series of observations from a single location).\n",
    "\n",
    "Now, this plot does not look particularily pretty...but we aren't here for pretty, we are here for results! What are we concerned with? Whether the range of ensemble simulated outcomes form the prior covers the measured values. Recall that plots on the left are 1to1 plots and on the right the residuals ar edisplayed.  In both cases, a grey line represents the range of simulated values for a given observation\n",
    "\n",
    "In plots on the right, each grey line should interesect the 1-to-1 line. In the plots on the right, each grey line should intersect the \"zero-residual\" line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.plot_utils.ensemble_res_1to1(obs_df, pst, skip_groups=pst.nnz_obs_groups[:-4]); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the prior covers the \"measured\" values (which is good).\n",
    "\n",
    "But hold on a second! What about measurement noise? If we are saying that it is *possible* that our measurements are wrong by a certain amount, shouldn't we make sure our model can represent conditions in which they are? Yes, of course!\n",
    "\n",
    "No worries, `pyemu` has you covered. Let's quickly cook up an ensemble of observations with noise. (Recall we recorded a covariance matrix of observation noise during the \"freyberg pstfrom pest setup\" notebook; this has also been discussed in the \"observation and weights\" notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_cov = pyemu.Cov.from_binary(os.path.join(t_d, 'obs_cov.jcb'))\n",
    "obs_plus_noise = pyemu.ObservationEnsemble.from_gaussian_draw(pst=pst, cov=obs_cov);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's plot that again but with observation noise. \n",
    "\n",
    "Aha! Good, not only do our ensemble of model outcomes cover the measured values, but they also entirely cover the range of measured values with noise (red shaded area in the plot below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.plot_utils.ensemble_res_1to1(obs_df, pst, skip_groups=pst.nnz_obs_groups[:-4], base_ensemble=obs_plus_noise); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another, perhaps coarser, method to quickly explore outcomes is to look at histograms of observations. \n",
    "\n",
    "The following figure groups obsevrations according to type (just to lump them together and make a smaller plot) and then plots histograms of observation values. Grey shaded columns represent simulated values from the prior. Red shaded columns represent the ensemble of measured values + noise. The grey columns should ideally be spread wider than the red columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = pst.observation_data.loc[pst.nnz_obs_names].apply(lambda x: x.usecol + \" \"+x.oname,axis=1).to_dict()\n",
    "plot_cols = {v: [k] for k, v in plot_cols.items()}\n",
    "pyemu.plot_utils.ensemble_helper({\"r\":obs_plus_noise,\"0.5\":obs_df}, \n",
    "                                  plot_cols=plot_cols,bins=20,sync_bins=True,\n",
    "                                  )\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Forecasts\n",
    "\n",
    "As usual, we bring this story back to the forecasts - after all they are why we are modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.forecast_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will plot the distribution of each forecast obtained by running the prior parameter ensemble. Because we are using a synthetic model, we also have the privilege of being able to plot the \"truth\" (in the real world we dont know the truth of course). \n",
    "\n",
    "Many modelling analyses could stop here. If outcomes from a prior MC analysis show that the simulated distribution of forecasts *does not* cause some \"bad-thing\" to happen within an \"acceptable\" confidence, then you are done. No need to go and do expensive and time-consuming history-matching! \n",
    "\n",
    "On the other hand, if the uncertainty (e.g. variance) is unacceptably wide, then it *may* be justifiable to try to reduce forecast unertainty through history matching. But only if you have forecast-sensitive observation data, and if the model is amenable to assimilating these data! How do I know that you ask? Worry not, we will get to this in subsequent tutorials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in pst.forecast_names:\n",
    "    plt.figure()\n",
    "    ax = obs_df.loc[:,forecast].plot(kind=\"hist\",color=\"0.5\",alpha=0.5)\n",
    "    ax.set_title(forecast)\n",
    "    fval = pst.observation_data.loc[forecast,\"obsval\"]\n",
    "    ax.plot([fval,fval],ax.get_ylim(),\"r-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior MC without Covariance\n",
    "\n",
    "The workflow above demonstrated how to use `pestpp-swp` to undertake a \"sweep\" of model runs. Here, we undertook prior Monte Carlo by running models with an ensemble of parameter sets sampled from the prior parameter probability distribution. \n",
    "\n",
    "The same can be accomplished with `pestpp-ies` by assigning the `NOPTMAX` control variable to -1 and either providing `pestpp-ies` with a predefined parameter ensebmle (the same as we did for `pestpp-swp`) or by providing the parameter covariance matrix and allowing `pestpp-ies` to sample from the prior itself.\n",
    "\n",
    "The next few cells do something slightly different. Here we will use `pestpp-ies` to undertake prior MC, but assuming to correlation between parameters. Here, prior parameter uncertainty is diagonal and determined solely based on the parameter bounds in the PEST control file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set some `pest++` options and re-write the control file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options['ies_num_reals'] = 200\n",
    "pst.control_data.noptmax = -1\n",
    "pst.write(os.path.join(t_d,\"freyberg_diagprior.pst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `pstpp-ies`. This should take about the same amount of time as `pestpp-swp` did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = \"master_diagonal_prior_monte_carlo\" \n",
    "pyemu.os_utils.start_workers(t_d,\"pestpp-ies\",\"freyberg_diagprior.pst\",\n",
    "                            num_workers=num_workers,\n",
    "                            worker_root='.',\n",
    "                            master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in the results. Let's just look at the forecasts. (Feel free to repeat the plots we did above if you wish to compare them.)\n",
    "\n",
    "So, what do you think? Did ignoring parameter covariance make our forecasts more or less robust? It seems like our prior is failing to capture some of the \"true\" values now. This highlights the role of connectivity and \"hydraulic structures\" in some groundwater predictions and the importance of having a well constructed prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.read_csv(os.path.join(m_d,\"freyberg_diagprior.0.obs.csv\"),index_col=0)\n",
    "\n",
    "for forecast in pst.forecast_names:\n",
    "    plt.figure()\n",
    "    # plot histogram of forecast values recorded in the simulated prior observation csv\n",
    "    ax = obs_df.loc[:,forecast].plot(kind=\"hist\",color=\"0.5\",alpha=0.5)\n",
    "    ax.set_title(forecast)\n",
    "    # plot the forecast value in the \"observation data\"; this value happens to be the \"truth\"\n",
    "    fval = pst.observation_data.loc[forecast,\"obsval\"]\n",
    "    ax.plot([fval,fval],ax.get_ylim(),\"r-\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d91d695e32284e5f4db43d0a55a7ffe722d99eb6050b5b06eff0d966e4449445"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('gmdsitut')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
