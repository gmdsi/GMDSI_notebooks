{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayes' Theorem\n",
    "> ### _\"When the facts change, I change my mind. What do you do, sir?\"_\n",
    "> --John Maynard Keynes\n",
    "\n",
    "Loosely, Bayes' Theorem can be interpeted as \n",
    "\n",
    " $$P\\left(\\boldsymbol{\\theta}|\\textbf{d}\\right)=\\frac{P\\left(\\textbf{d}|\\boldsymbol{\\theta}\\right) P\\left> (\\boldsymbol{\\theta}\\right)}{P\\left(\\textbf{d}\\right)}$$\n",
    " \n",
    " $\\boldsymbol{\\theta}$  are parameters, $\\mathbf{d}$ are the data and  $|$ means \"conditional on\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is really just rearranging the law of conditional probabilities:\n",
    "\n",
    " $$P\\left(\\boldsymbol{\\theta}|\\textbf{d}\\right)P\\left(\\textbf{d}\\right)=P\\left(\\textbf{d}|\\boldsymbol{\\theta}\\right) P\\left(\\boldsymbol{\\theta}\\right)$$\n",
    "\n",
    " _Um, what?_ Let's use pictres to make this easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Venn diagram to explore conditional probabilities\n",
    "<img src=\"intro_to_bayes_files\\conditional_probability.png\" style=\"float: left; width: 50%; margin-right: 1%; margin-bottom: 0.5em;\">\n",
    "<p style=\"clear: both;\">\n",
    "By Gnathan87 - Own work, CC0, https://commons.wikimedia.org/w/index.php?curid=15991401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is the probability of $A$ if we know we are in $B_1$? The equation for this is:\n",
    "\n",
    "$$P\\left(A|B_1\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is easy to see that it is 100% or:\n",
    "\n",
    "$$P\\left(A|B_1\\right)=1.0$$\n",
    "\n",
    "Why? Because the $B_1$ circle is entirely within the $A$ circle. Therefore, if we know we are in $B_1$, then we must also be in $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a general rule, we can state \n",
    "$$P\\left(A|B_1\\right)=\\frac{P\\left(A\\cap B_1\\right)}{P\\left(B_1\\right)}$$\n",
    "\n",
    "or, equivalently \n",
    "$$P\\left(A\\cap B_1\\right)=P\\left(A|B_1\\right)P\\left(B_1\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " So what about $P\\left(A|B_2\\right)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "P\\left(A|B_2\\right)=\\frac{P\\left(A\\cap B_2\\right)}{P\\left(B_2\\right)}=\\frac{0.12}{0.12+0.04}=0.75\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "Now we can derive Bayes' theorem because joint probabilities are symmetrical. Switching notation to \n",
    "$$\\boldsymbol{\\theta} \\text{ and }\n",
    "\\mathbf{d}$$\n",
    "\n",
    "$$P\\left(\\boldsymbol{\\theta}\\cap \\mathbf{d}\\right)=P\\left(\\mathbf{d}\\cap \\boldsymbol{\\theta}\\right)$$\n",
    "\n",
    "$$P\\left(\\boldsymbol{\\theta}|\\textbf{d}\\right) P\\left(\\textbf{d}\\right) = P\\left(\\textbf{d}|\\boldsymbol{\\theta}\\right) P\\left(\\boldsymbol{\\theta}\\right)$$\n",
    "\n",
    " With the tiniest little algebra, we get Bayes' theorem -- #boom#!\n",
    "\n",
    "$$P\\left(\\boldsymbol{\\theta}|\\textbf{d}\\right) = \\frac{P\\left(\\textbf{d}|\\boldsymbol{\\theta}\\right) P\\left(\\boldsymbol{\\theta}\\right)}{P\\left(\\textbf{d}\\right)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So, what does this really mean?  \n",
    "\n",
    "## A practical example\n",
    "\n",
    " Let's play with a concrete example, one hinging on life, death, trust, and promises kept!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    " \n",
    "<img src=\"intro_to_bayes_files\\plant.JPG\" style=\"float: left; width: 28%; margin-right: 1%; margin-bottom: 0em;\">\n",
    "<img src=\"intro_to_bayes_files\\dead_plant.JPG\" style=\"float: left; width: 30%; margin-right: 1%; margin-bottom: 0em;\">\n",
    "<p style=\"clear: both;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " You have a plant at home, and you're going to go away for a week. If it gets watered, its probability of dying is 15%. If it doesn't get watered, it is 80% likely to die. You ask your partner to water it for you and you are 90% certain they will do it.\n",
    "\n",
    " We can express this all in terms of probabilities and conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### First a couple definitions:\n",
    "\n",
    "$\\theta_w$: partner waters the plant\n",
    "\n",
    "$\\theta_{nw}$: partner forgets to water the plant\n",
    "\n",
    "$d_a$: plant is alive when we return \n",
    "\n",
    "$d_d$: plant is dead when we return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " $\\mathbf{d} = [d_a,d_d]$: a vector of all possible outcomes\n",
    " \n",
    " $\\boldsymbol{\\theta} = [\\theta_w,\\theta_{nw}]$: a vector of all possible outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cool, so let's express what we know in probability equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P\\left(d_d | \\theta_w\\right)=0.15$$\n",
    "$$P\\left(d_d | \\theta_{nw}\\right)=0.8$$\n",
    "$$P\\left(\\theta_w\\right)=0.9$$\n",
    "$$P\\left(\\theta_{nw}\\right)=0.1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And we can assign these as python variables to get our maths groove on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDd_thw = 0.15\n",
    "PDd_thnw = 0.8\n",
    "Prior_thw = 0.9\n",
    "Prior_thnw = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can ask questions like, \"what is the probability the plant is dead\"\n",
    "\n",
    "To calculate, we add up all the conditional probablities like this:\n",
    "\n",
    "$$P\\left(d_d\\right) = P\\left(d_d\\cap\\theta_w\\right) + P\\left(d_d\\cap\\theta_{nw}\\right)$$\n",
    "\n",
    "$$P\\left(d_d\\right) = P\\left(d_d|\\theta_w\\right)P\\left(\\theta_w\\right) + P\\left(d_d|\\theta_{nw}\\right)P\\left(\\theta_{nw}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDd = PDd_thw*Prior_thw + PDd_thnw*Prior_thnw\n",
    "print ('Probability Plant is dead = {0:.3f}'.format(PDd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since we only have two discrete outcomes, the probability of the plant being alive is simply \n",
    "\n",
    "$$P\\left(d_a\\right)=1-P\\left(d_d\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDa = 1-PDd\n",
    "print ('Probability Plant is alive = {0:.3f}'.format(PDa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Great! So we can incorporate all the possible arrangements of events to determine likely outcomes. But....what we are _really_ interested in is what we learn with partial information. This is where household harmony can be made or broken!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning from information\n",
    "\n",
    "We come home and see that the plant is dead (crumbs!). Who to blame? What is the probability that our partner forgot to water it? \n",
    "\n",
    "Mathematically, this is;\n",
    "$$P\\left(\\theta_{nw} | d_d\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use Bayes' theorem to evaluate this new information (e.g. we have observed that the plant is dead)\n",
    "\n",
    "$$P\\left(\\theta_{nw} | d_d\\right) = \\frac{P\\left(d_d | \\theta_{nw}\\right) P\\left(\\theta_{nw}\\right)}{P\\left(d_d\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PthnwDd = PDd_thnw * Prior_thnw/ PDd\n",
    "print (\"Probability that partner failed to water the plant\")\n",
    "print(\"having seen it's dead is {0:.3f}\".format(PthnwDd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alternatively, we can see the converse: How likely did our partner water the plant given that it's alive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$P\\left(\\theta_w | d_a\\right) = \\frac{P\\left(d_a | \\theta_w\\right) P\\left(\\theta_w\\right)}{P\\left(d_a\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PthwDa = (1-PDd_thw) * Prior_thw/ PDa\n",
    "print (\"Probability that partner did water the plant\")\n",
    "print (\"having seen it's alive is {0:.3f}\".format(PthwDa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How likely did our partner forget, given that we see it's alive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P\\left(\\theta_{nw} | d_a\\right) = \\frac{P\\left(d_a | \\theta_{nw}\\right) P\\left(\\theta_{nw}\\right)}{P\\left(d_a\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PthnwDa = (1-PDd_thnw) * Prior_thnw/ PDa\n",
    "print (\"Probability that partner forgot to water the plant\")\n",
    "print(\"having seen it's alive is {0:.3f}\".format(PthnwDa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous variables\n",
    "\n",
    "Right then, but we are in the world of continuous variables, not simple discrete probabilities\n",
    "\n",
    "This means that we end up with probability density functions rather than discrete probabilities and the denominator on the RHS gets tricky to evaluate (the total probability). Luckily, we are mostly conncerned with finding the parameters that maximize the probability and less concerned with the probability itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"intro_to_bayes_files\\Fig10.3_Bayes_figure.png\" style=\"float: left; width: 75%; margin-right: 1%; margin-bottom: 0.5em;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is a learning framework, where what we know at the end is a function of what we started with and what we _learned_ through a new experiment (model) or new information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\underbrace{P(\\boldsymbol{\\theta}|\\textbf{d})}_{\\substack{\\text{posterior} \\\\ \\text{pdf}}} \\propto \\underbrace{\\mathcal{L}( \\boldsymbol{\\theta}| \\textbf{d})}_{\\substack{\\text{likelihood} \\\\ \\text{function}}} \\underbrace{P(\\boldsymbol{\\theta})}_{\\substack{\\text{prior } \\\\ \\text{pdf}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\underbrace{P(\\boldsymbol{\\theta}|\\textbf{d})}_{\\substack{\\text{what we} \\\\ \\text{know now}}} \\propto \\underbrace{\\mathcal{L}(\\boldsymbol{\\theta} | \\textbf{d})}_{\\substack{\\text{what we} \\\\ \\text{learned}}} \\underbrace{P(\\boldsymbol{\\theta})}_{\\substack{\\text{what we} \\\\ \\text{knew}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's look at an interactive example of how distributions behave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayes_helper as bh\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh.plot_posterior(prior_mean=10, prior_std=11, likeli_mean = 25, likeli_std=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(bh.plot_posterior,\n",
    "         prior_mean=(1, 20., .5), likeli_mean=(1, 20, 1), \n",
    "         prior_std=(.1, 8, .1), likeli_std=(.1, 8, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The mandatory \"coin-flipping example\" \n",
    "> _Borrowed from **Bayesian Methods for Hackers**. The full Github repository is available [here](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)_\n",
    "\n",
    "We can start with an \"ignorance\" prior - equal probabilities of all outcomes (both, in the case---heads and tails). By flipping a coin we can observer outcomes, constantly updating and learning from each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trials = 1000\n",
    "# The code below can be passed over, as it is currently not important, plus it\n",
    "# uses advanced topics we have not covered yet. \n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "figsize(11, 9)\n",
    "\n",
    "import scipy.stats as stats\n",
    "dist = stats.beta\n",
    "n_trials = [0, 1, 2, 3, 4, 5, 8, 15, 50, max_trials]\n",
    "data = stats.bernoulli.rvs(0.5, size=n_trials[-1])\n",
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "# For the already prepared, I'm using Binomial's conj. prior.\n",
    "for k, N in enumerate(n_trials):\n",
    "    sx = plt.subplot(int(len(n_trials) / 2), 2, k + 1)\n",
    "    plt.xlabel(\"$p$, probability of heads\") \\\n",
    "        if k in [0, len(n_trials) - 1] else None\n",
    "    plt.setp(sx.get_yticklabels(), visible=False)\n",
    "    heads = data[:N].sum()\n",
    "    y = dist.pdf(x, 1 + heads, 1 + N - heads)\n",
    "    plt.plot(x, y, label=\"observe %d tosses,\\n %d heads\" % (N, heads))\n",
    "    plt.fill_between(x, 0, y, color=\"#348ABD\", alpha=0.4)\n",
    "    plt.vlines(0.5, 0, 4, color=\"k\", linestyles=\"--\", lw=1)\n",
    "\n",
    "    leg = plt.legend()\n",
    "    leg.get_frame().set_alpha(0.4)\n",
    "    plt.autoscale(tight=True)\n",
    "plt.suptitle(\"Bayesian updating of posterior probabilities\",\n",
    "             y=1.02,\n",
    "             fontsize=14)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The posterior probabilities are represented by the curves, and our uncertainty is proportional to the width of the curve. As the plot above shows, as we start to observe data our posterior probabilities start to shift and move around. Eventually, as we observe more and more data (coin-flips), our probabilities will tighten closer and closer around the true value of $p=0.5$ (marked by a dashed line). \n",
    "\n",
    "Notice that the plots are not always *peaked* at 0.5. There is no reason it should be: recall we assumed we did not have a prior opinion of what $p$ is. In fact, if we observe quite extreme data, say 8 flips and only 1 observed heads, our distribution would look very biased *away* from lumping around 0.5 (with no prior opinion, how confident would you feel betting on a fair coin after observing 8 tails and 1 head). As more data accumulates, we would see more and more probability being assigned at $p=0.5$, though never all of it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"intro_to_bayes_files\\bayes_editorial.png\" style=\"float: left; width: 75%; margin-right: 1%; margin-bottom: 0.5em;\">\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
