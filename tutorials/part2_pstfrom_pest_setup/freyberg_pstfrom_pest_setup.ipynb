{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the PEST(++) interface around the modified Freyberg model\n",
    "\n",
    "In this notebook, we will construct a complex model independent (non-intrusive) interface around an existing `MODFLOW6` model using `pyEMU`. We assume that the reader is at least partially familiar with PEST(++) file formats and working philosophy. \n",
    "\n",
    "The modified Freyberg groundwater flow model has been constructed and is described in a previous notebook from this series. We will construct the entire PEST(++) interface from scratch here. This setup will be built upon in subsequent tutorials. \n",
    "\n",
    "We will rely heavily on the `pyemu.PstFrom` class. Although here we employ it with a `MODFLOW6` model, `PstFrom` is designed to be general and software independent (mostly). Some features are only available for `MODFLOW` models (e.g. `SpatialReference`).\n",
    "\n",
    "The `PstFrom` class automates the construction of high-dimensional PEST(++) interfaces with all the bells and whistles. It provides easy-to-use functions to process model input and output files into PEST(++) datasets. It can assist with setting up spatio-temporaly varying parameters. It handles the generation of geostatisical prior covariance matrices and ensembles. It automates writting a \"model run\" script. It provides tools to add custom pre- and post-processing functions to this script. It makes adding tweaks and fixes to the PEST(++) interface a breeze. All of this from the comfort of your favourite Python IDE.\n",
    "\n",
    "During this tutorial we are going to construct a PEST dataset. Amongst other things, we will demonstrate:\n",
    " - how to add observations & parameters from model output & input files;\n",
    " - how to add pre- and post-processing functions to the \"model run\" script;\n",
    " - how to generate geostatistical structures for spatialy and temporally correlated parameters;\n",
    " - how to edit parameter/observation data sections;\n",
    " - how to generate a prior parameter covariance matrix and prior parameter ensemble;\n",
    "\n",
    "\n",
    "\n",
    "First, let's get our model files and sort out some admin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Admin & Organize Folders\n",
    "First some admin. Load the dependencies and organize model folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:33.684338Z",
     "iopub.status.busy": "2022-04-11T03:51:33.684338Z",
     "iopub.status.idle": "2022-04-11T03:51:34.772540Z",
     "shell.execute_reply": "2022-04-11T03:51:34.773536Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import platform\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pyemu\n",
    "import flopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "sys.path.append(\"..\")\n",
    "import herebedragons as hbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be calling a few external programs throughout this tutorial. Namely, MODFLOW 6 and PESTPP-GLM. For the purposes of the tutorial(s), we have included executables in the tutorial repository. They are in the `bin` folder, organized by operating system and will programmatically copied into the working dirs as needed. \n",
    "\n",
    "Some may prefer that executables be located in a folder that is cited in your computerâ€™s PATH environment variable. Doing so allows you to run them from a command prompt open to any other folder without having to include the full path to these executables in the command to run them. \n",
    "\n",
    "However, in situations where someone has several active projects and each may use difference versions of compiled binary codes, , this may not be practical. In such cases, we can simply place the executables in the folder from which they will be executed.  So, let's copy the necessary executables into our working folder using a simple helper function:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy the original model folder into a new working directory, just to ensure we don't mess up the base files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:35.090266Z",
     "iopub.status.busy": "2022-04-11T03:51:35.089268Z",
     "iopub.status.idle": "2022-04-11T03:51:35.290999Z",
     "shell.execute_reply": "2022-04-11T03:51:35.290002Z"
    }
   },
   "outputs": [],
   "source": [
    "# folder containing original model files\n",
    "org_d = os.path.join('..', '..', 'models', 'freyberg_mf6')\n",
    "\n",
    "# a dir to hold a copy of the org model files\n",
    "tmp_d = os.path.join('freyberg_mf6')\n",
    "\n",
    "if os.path.exists(tmp_d):\n",
    "    shutil.rmtree(tmp_d)\n",
    "shutil.copytree(org_d,tmp_d)\n",
    "\n",
    "hbd.prep_bins(tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you inspect the model folder, you will see that all the `MODFLOW6` model files have been written \"externally\". This is key for working with the `PstFrom` class (or with PEST(++) in general, really). Essentialy, all pertinent model inputs have been written as independent files in either array or list format. This makes it easier for us to programiatically access and re-write the values in these files.\n",
    "\n",
    "Array files contain a data type (usually floating points). List files will have a few columns that contain index information and then columns of floating point values (they have a tabular format; think `.csv` files or DataFrames). The `PstFrom` class provides methods for processing these file types into a PEST(++) dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:35.295020Z",
     "iopub.status.busy": "2022-04-11T03:51:35.294039Z",
     "iopub.status.idle": "2022-04-11T03:51:35.305958Z",
     "shell.execute_reply": "2022-04-11T03:51:35.305958Z"
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need just a tiny bit of info about the spatial discretization of the model - this is needed to work out separation distances between parameters to build a geostatistical prior covariance matrix later.\n",
    "\n",
    "Here we will load the flopy sim and model instance just to help us define some quantities later - flopy is ***not required*** to use the `PstFrom` class. ***Neither is MODFLOW***. However, at the time of writting, support for `SpatialReference` to spatially locate parameters is limited to structured grid models.\n",
    "\n",
    "Load the simulation. Run it once to make sure it works and to ***make sure that model output files are in the folder***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:35.349841Z",
     "iopub.status.busy": "2022-04-11T03:51:35.348844Z",
     "iopub.status.idle": "2022-04-11T03:51:36.419821Z",
     "shell.execute_reply": "2022-04-11T03:51:36.420860Z"
    }
   },
   "outputs": [],
   "source": [
    "# load simulation\n",
    "sim = flopy.mf6.MFSimulation.load(sim_ws=tmp_d)\n",
    "# load flow model\n",
    "gwf = sim.get_model()\n",
    "\n",
    "# run the model once to make sure it works\n",
    "pyemu.os_utils.run(\"mf6\",cwd=tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spatial Reference\n",
    "Now we can instantiate a `SpatialReference`. This will later be passed to `PstFrom` to assist with spatially locating parameters (e.g. pilot points and/or cell-by-cell parameters).  You can also use the flopy `modelgrid` class instance that is attached to the simulation, but `SpatialReference` is cleaner and faster for structured grids..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:36.424810Z",
     "iopub.status.busy": "2022-04-11T03:51:36.422815Z",
     "iopub.status.idle": "2022-04-11T03:51:36.436778Z",
     "shell.execute_reply": "2022-04-11T03:51:36.435781Z"
    }
   },
   "outputs": [],
   "source": [
    "sr = pyemu.helpers.SpatialReference.from_namfile(\n",
    "        os.path.join(tmp_d, \"freyberg6.nam\"),\n",
    "        delr=gwf.dis.delr.array, delc=gwf.dis.delc.array)\n",
    "sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiate PstFrom\n",
    "\n",
    "Now we can start to construct the PEST(++) interface by instantiating a `PstFrom` class instance. There are a few things that we need to specify up front:\n",
    "\n",
    " - the folder in which we currently have model files (e.g. `tmp_d`). PstFrom will copy all the files from this directory into a new \"template\" folder.\n",
    " - **template folder**: this is a folder in which the PEST dataset will be constructed - this folder will hold the model files plus all of the files needed to run PEST(++). This folder/dataset will form the template for subsequent deployment of PEST(++).\n",
    " - **longnames**: for backwards compatibility with PEST and PEST_HP (i.e. non-PEST++ versions), which have upper limits to parameter/obsveration names (PEST++ does not). Setting this value to False is only recommended if required. \n",
    " - Whether the model is `zero based` or not.\n",
    " - (optional) the **spatial reference**, as previously discussed. This is only requried if using `pyEMU` to define parameter spatial correlation. Alternatively, you can define these yourself or use utilities available in the PEST-suite. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:36.443759Z",
     "iopub.status.busy": "2022-04-11T03:51:36.442761Z",
     "iopub.status.idle": "2022-04-11T03:51:37.401480Z",
     "shell.execute_reply": "2022-04-11T03:51:37.402513Z"
    }
   },
   "outputs": [],
   "source": [
    "# specify a template directory (i.e. the PstFrom working folder)\n",
    "template_ws = os.path.join(\"freyberg6_template\")\n",
    "start_datetime=\"1-1-2008\"\n",
    "# instantiate PstFrom\n",
    "pf = pyemu.utils.PstFrom(original_d=tmp_d, # where the model is stored\n",
    "                            new_d=template_ws, # the PEST template folder\n",
    "                            remove_existing=True, # ensures a clean start\n",
    "                            longnames=True, # set False if using PEST/PEST_HP\n",
    "                            spatial_reference=sr, #the spatial reference we generated earlier\n",
    "                            zero_based=False, # does the MODEL use zero based indices? For example, MODFLOW does NOT\n",
    "                            start_datetime=start_datetime, # required when specifying temporal correlation between parameters\n",
    "                            echo=False) # to stop PstFrom from writting lots of infromation to the notebook; experiment by setting it as True to see the difference; usefull for troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(template_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that when `PstFrom` is instantiated, it starts by copying the `original_d` to the `new_d`.  sweet as!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Observations\n",
    "\n",
    "We now have a `PstFrom` instance assigned to the variable `pf`. For now it is only an empty container to which we can start adding \"observations\", \"parameters\" and other bits and bobs.\n",
    "\n",
    "Lets start with observations because they are easier. `MODFLOW6` makes life even easier by recording observations in nicely organized .csv files. Isn't that a peach!\n",
    "\n",
    "#### 4.1 Freyberg Recap\n",
    "As you may recall from the \"*intro to Freyberg*\" tutorial, the model is configured to record time series of head at observation wells, and flux at three locations along the river. These are recorded in external .csv files named `heads.csv` and `sfr.csv`, respectively. You should be able to see these files in the model folder.\n",
    "\n",
    "Recall that each .csv houses records of observation time-series. Outputs are recorded for each simulated stress-period. The model starts with a single steady-state stress-period, followed by 24 monthly transient stress-periods. The steady-state and first 12 transient stress-periods simulate the history-matching period. The last 12 transient stress periods simulate future conditions (i.e. the prediction period)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.405498Z",
     "iopub.status.busy": "2022-04-11T03:51:37.405498Z",
     "iopub.status.idle": "2022-04-11T03:51:37.417473Z",
     "shell.execute_reply": "2022-04-11T03:51:37.418469Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the output csv file names\n",
    "for i in gwf.obs:\n",
    "    print(i.output.obs_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the 'heads.csv' file. First load it as a DataFrame to take a look.\n",
    "\n",
    "As you can see, there are many columns, one for each observation site. Conveniently, * *cough* * they are named according to the cell layer, row and column. Note that at every site, there is an observation in both the top and bottom layer (0_ and 2_). We will make use of this later to create \"secondary observations\" of head differences between layers...but let's not get distracted.\n",
    "\n",
    "The values in the *.csv* file were generated by running the model. (***IMPORTANT!***) However, `PstFrom` assumes that values in this file are the *target* observation values, and they will be used to populate the PEST(++) dataset.  This lets the user quickly verify that the `PstFrom` process reproduces the same model output files - an important thing to test!\n",
    "\n",
    "Now, you can and should change the observation values later on for the quantities that coorespond to actual observation data.  This is the standard workflow when using `PstFrom` because it allows users to separate the PEST interface setup from the always-important process of setting observation values and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.421461Z",
     "iopub.status.busy": "2022-04-11T03:51:37.421461Z",
     "iopub.status.idle": "2022-04-11T03:51:37.449398Z",
     "shell.execute_reply": "2022-04-11T03:51:37.450384Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(template_ws,\"heads.csv\"),index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Prepare Observation Files\n",
    "\n",
    "In the next cell we read a .csv that contains measured obsevration values. As it has the exact same structure as our model output file, we can simpy replace the values in the model output file (and re-write it!). In real-world cases this step will likley be a bit more involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.453343Z",
     "iopub.status.busy": "2022-04-11T03:51:37.453343Z",
     "iopub.status.idle": "2022-04-11T03:51:37.465008Z",
     "shell.execute_reply": "2022-04-11T03:51:37.466004Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the measured head values\n",
    "hds_meas = pd.read_csv(os.path.join('..', '..', 'models', 'freyberg_mf6_truth',\"heads.csv\"),\n",
    "                    index_col=0)\n",
    "hds_meas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Adding Observations\n",
    "\n",
    "First, we will use the `PstFrom.add_observations()` method to add observations to our `pf` object. This method can use ***list-type*** files, where the data are organized in column/tabular format with one or more index columns and one or more data columns.  This method can also use ***array-type*** files, where the data are organized in a 2-D array structure (we will see this one later...)\n",
    "\n",
    "We are going to tell `pf` which columns of this file contain observations. Values in these columns will be assigned to *observation values*.\n",
    "\n",
    "We can also inform it if there is an index column (or columns). Values in this column will be included in the *observation names*. \n",
    "\n",
    "We could also specify which rows to include as observations. But observations are free...so why not keep them all! \n",
    "\n",
    "Let's add observations from `heads.csv`. The first column of this file records the time at which the value is simulated. Let's use that as the index column (this becomes useful later on to post-process results). We want all other columns as observation values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.470027Z",
     "iopub.status.busy": "2022-04-11T03:51:37.468997Z",
     "iopub.status.idle": "2022-04-11T03:51:37.514128Z",
     "shell.execute_reply": "2022-04-11T03:51:37.514128Z"
    }
   },
   "outputs": [],
   "source": [
    "hds_df = pf.add_observations(\"heads.csv\", # the model output file to read\n",
    "                            insfile=\"heads.csv.ins\", #optional, the instruction file name\n",
    "                            index_cols=\"time\", #column header to use as index; can also use column number (zero-based) instead of the header name\n",
    "                            use_cols=list(df.columns.values), #names of columns that include observation values; can also use column number (zero-based) instead of the header name\n",
    "                            prefix=\"hds\") #prefix to all observation names; choose something logical and easy o find. We use it later on to select obsevrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect what we just created. \n",
    "\n",
    "We can see that the `.add_observations()` method returned a dataframe with lots of useful info: \n",
    "\n",
    " - the observation names that were formed (see `obsnme` column); note that these inlcude lots of usefull metadata like the column name, index value and so on;\n",
    " - the values that were read from `heads.csv` (see `obsval` column); \n",
    " - some generic weights and group names; note that observations are grouped according to the column of the model output .csv. Alternatively, we could have specified a list of observation group names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.520978Z",
     "iopub.status.busy": "2022-04-11T03:51:37.520978Z",
     "iopub.status.idle": "2022-04-11T03:51:37.528997Z",
     "shell.execute_reply": "2022-04-11T03:51:37.529954Z"
    }
   },
   "outputs": [],
   "source": [
    "hds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, no PEST *control file* has been created, we have simply prepared to add these observations to the control file later. Everything is still only stored in memory. However, a PEST *instruction* file has been created in the template folder (`template_ws`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.532947Z",
     "iopub.status.busy": "2022-04-11T03:51:37.532947Z",
     "iopub.status.idle": "2022-04-11T03:51:37.544181Z",
     "shell.execute_reply": "2022-04-11T03:51:37.545213Z"
    }
   },
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".ins\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blimey, wasn't that easy? Automatically monitoring thousands of model output quantities as observations into a PEST dataset becomes a breeze!\n",
    "\n",
    "Let's quickly do the same thing for the SFR observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.549200Z",
     "iopub.status.busy": "2022-04-11T03:51:37.548170Z",
     "iopub.status.idle": "2022-04-11T03:51:37.560826Z",
     "shell.execute_reply": "2022-04-11T03:51:37.559802Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(template_ws, \"sfr.csv\"), index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.564789Z",
     "iopub.status.busy": "2022-04-11T03:51:37.564789Z",
     "iopub.status.idle": "2022-04-11T03:51:37.607876Z",
     "shell.execute_reply": "2022-04-11T03:51:37.606879Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the measured  values\n",
    "meas_sfr = pd.read_csv(os.path.join('..', '..', 'models', 'freyberg_mf6_truth',\"sfr.csv\"),\n",
    "                    index_col=0)\n",
    "# add the observations to pf\n",
    "sfr_df = pf.add_observations(\"sfr.csv\", # the model output file to read\n",
    "                            insfile=\"sfr.csv.ins\", #optional, the instruction file name\n",
    "                            index_cols=\"time\", #column header to use as index; can also use column number (zero-based) instead of the header name\n",
    "                            use_cols=list(df.columns.values), #names of columns that include observation values; can also use column number (zero-based) instead of the header name\n",
    "                            prefix=\"sfr\") #prefix to all observation names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to add observations of particle travel time and status. Unfortuantely the file written by MODPATH7 is not easily injestible by `PstFrom`. So we are going to need to \"manually\" construct an instruction file. We could add that now with the `PstFrom.add_observations_from_ins()` method, but we will wait and  add these after constructing the `Pst` object - soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Parameters\n",
    "\n",
    "The `PstFrom.add_parameters()` method reads model input files and adds parameters to the PEST(++) dataset. Parameterisation can be configured in several ways. \n",
    "\n",
    " - model input files can be in array or list format;\n",
    " - parameters can be setup as different \"types\". Each value in model input files can (1) each be a separate parameter (\"grid\" scale parameters), (2) be grouped into \"zones\" or (3) all be treated as a single parameter (\"constant\" type). Alteratvely, (4) parameters can be assigned to pilot points, from which individual parameter values are subsequently interpolated. `PstFrom` adds the relevant pre-processing steps to assign paramter values directly into the \"model run\" script.\n",
    " - parameter values can be setup as \"direct\", \"multiplier\" or \"addend\". This means the \"parameter value\" which PEST(++) sees can be (1) the same value the model sees, (2) a multiplier on the value in the existing/original model input file, or (3) a value which is added to the value in the existing/original model input file. This is very nifty and allows for some pretty advanced parameterization schemes by allowing mixtures of different types of parameters. `PstFrom` is designed to preferentially use parameters setup as multipliers (that is the default parameter type). This let us preserve the existing model inputs and treat them as the mean of the prior parameter distribution. Once again, relevant pre-processing scripts are automatically added to the \"model run\" script (discussed later) so that the multiplicative and additive parameterization process is not something the user has to worry about.\n",
    "\n",
    "\n",
    "#### 5.1. Freyberg Recap\n",
    "\n",
    "As discussed, all model inputs are stored in external files. Some are arrays. Others are lists. Recall that our model has 3 layers. It is transient. Hydraulic properties (Kh, Kv, Ss, Sy) vary in space. Recharge varies over both space and time. We have GHBs, SFR and WEL boundary conditions. GHB parameters are constant over time, but vary spatially. SFR inflow varies over time. Pumping rates of individual wells are uncertain in space and and time.\n",
    "\n",
    "All of these have some degree of spatial and/or temporal correlation.\n",
    "\n",
    "#### 5.2. Geostatistical Structures\n",
    "\n",
    "Parameter correlation plays a role in (1) regularization when giving preference to the emergence of patterns of spatial heterogeneity and (2) when specifying the prior parameter probability distribution (which is what regularization is enforcing!). Since we are all sophisticated and recognize the importance of expressing spatial and temporal uncertainty (e.g. heterogeneity) in the model inputs (and the corresponding spatial correlation in those uncertain inputs), let's use geostatistics to express uncertainty. To do that we need to define \"geostatistical structures\". \n",
    "\n",
    "For the sake of this tutorial, let's assume that heterogeneity in all spatially distributed parameters share the same statistical characteristics. Likewise for temporally varying parameters. We will therefore only  construct two geostatisitcal structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.621846Z",
     "iopub.status.busy": "2022-04-11T03:51:37.620841Z",
     "iopub.status.idle": "2022-04-11T03:51:37.889665Z",
     "shell.execute_reply": "2022-04-11T03:51:37.888668Z"
    }
   },
   "outputs": [],
   "source": [
    "# exponential variogram for spatially varying parameters\n",
    "v_space = pyemu.geostats.ExpVario(contribution=1.0, #sill\n",
    "                                    a=1000, # range of correlation; length units of the model. In our case 'meters'\n",
    "                                    anisotropy=1.0, #name says it all\n",
    "                                    bearing=0.0 #angle in degrees East of North corresponding to anisotropy ellipse\n",
    "                                    )\n",
    "\n",
    "# geostatistical structure for spatially varying parameters\n",
    "grid_gs = pyemu.geostats.GeoStruct(variograms=v_space, transform='log') \n",
    "\n",
    "# plot the gs if you like:\n",
    "grid_gs.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.892657Z",
     "iopub.status.busy": "2022-04-11T03:51:37.892657Z",
     "iopub.status.idle": "2022-04-11T03:51:37.904659Z",
     "shell.execute_reply": "2022-04-11T03:51:37.904659Z"
    }
   },
   "outputs": [],
   "source": [
    "# exponential variogram for time varying parameters\n",
    "v_time = pyemu.geostats.ExpVario(contribution=1.0, #sill\n",
    "                                    a=60, # range of correlation; length time units (days)\n",
    "                                    anisotropy=1.0, #do not change for 1-D time\n",
    "                                    bearing=0.0 #do not change for 1-D time\n",
    "                                    )\n",
    "\n",
    "# geostatistical structure for time varying parameters\n",
    "temporal_gs = pyemu.geostats.GeoStruct(variograms=v_time, transform='none') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Add Parameters\n",
    "\n",
    "Let's start by adding parameters of hydraulic properties that vary in space (but not time) and which are housed in array-type files (e.g. Kh, Kv, Ss, Sy). We will start by demonstrating step-by-step for Kh.\n",
    "\n",
    "First, find all the external array files that contain Kh values. In our case, these are the files with \"npf_k_\" in the file name. As you can see below, there is one file for each model layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.908648Z",
     "iopub.status.busy": "2022-04-11T03:51:37.907691Z",
     "iopub.status.idle": "2022-04-11T03:51:37.919670Z",
     "shell.execute_reply": "2022-04-11T03:51:37.920616Z"
    }
   },
   "outputs": [],
   "source": [
    "tag = \"npf_k_\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup multiple spatial scales of parameters for Kh. To do this we will use three of the parameter \"types\" described above. The coarse scale will be a `constant` single value for each array. The medium scale will `pilot points`. The finest scale will use parameters as the `grid` scale (a unique parameter for each model cell!)\n",
    "\n",
    "Each scale of parameters will work with the others as multipliers with the existing Kh arrays. (This all happens at runtime as part of the \"model run\" script.) Think of the scales as dials that PEST(++) can turn to improve the fit. The \"coarse\" scale is one big dial that alows PEST to move everything at once - that is, change the mean of the entire Kh array. The \"medium\" dials are few (but not too many) that allow PEST to adjust broad areas, but not making eveything move. The \"fine\" scales are lots of small dials that allow PEST(++) to have very detailed control, tweaking parameter values within very small areas. \n",
    "\n",
    "However, because we are working with parameter `multipliers`, we will need to specify two sets of parameter bounds: \n",
    " - `upper_bound` and `lower_bound` are the standard control file bounds (the bounds on the parameters that PEST sees), while\n",
    " - `ult_ubound` and `ult_lbound` are bounds that are applied at runtime to the resulting (multiplied out) model input array that MODFLOW reads. \n",
    " \n",
    "Since we are using sets of multipliers, it is important to make sure we keep the resulting model input arrays within the range of realistic values.\n",
    "\n",
    "#### 5.3.1. Array Files\n",
    "\n",
    "We will first demonstrate steb-by-step for `freyberg6.npf_k_layer1.txt`. We will start with grid scale parameters. These are multipliers assigned to each individual value in the array.\n",
    "\n",
    "We start by getting the idomain array. As our model has inactive cells, this helps us avoid adding unncessary parameters. It is also required later when generating pilot points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.923642Z",
     "iopub.status.busy": "2022-04-11T03:51:37.922646Z",
     "iopub.status.idle": "2022-04-11T03:51:37.934855Z",
     "shell.execute_reply": "2022-04-11T03:51:37.935852Z"
    }
   },
   "outputs": [],
   "source": [
    "# as IDOMIAN is the same in all layers, we can use any layer\n",
    "ib = gwf.dis.idomain.get_data(layer=0)\n",
    "plt.imshow(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.939841Z",
     "iopub.status.busy": "2022-04-11T03:51:37.939841Z",
     "iopub.status.idle": "2022-04-11T03:51:37.983702Z",
     "shell.execute_reply": "2022-04-11T03:51:37.982704Z"
    }
   },
   "outputs": [],
   "source": [
    "f = 'freyberg6.npf_k_layer1.txt'\n",
    "\n",
    "# grid (fine) scale parameters\n",
    "df_gr = pf.add_parameters(f,\n",
    "                zone_array=ib, #as we have inactie model cells, we can avoid assigning these as parameters\n",
    "                par_type=\"grid\", #specify the type, these will be unique parameters for each cell\n",
    "                geostruct=grid_gs, # the gestatisical structure for spatial correlation \n",
    "                par_name_base=f.split('.')[1].replace(\"_\",\"\")+\"gr\", #specify a parameter name base that allows us to easily identify the filename and parameter type. \"_gr\" for \"grid\", and so forth.\n",
    "                pargp=f.split('.')[1].replace(\"_\",\"\")+\"gr\", #likewise for the parameter group name\n",
    "                lower_bound=0.2, upper_bound=5.0, #parameter lower and upper bound\n",
    "                ult_ubound=100, ult_lbound=0.01 # The ultimate bounds for multiplied model input values. Here we are stating that, after accounting for all multipliers, Kh cannot exceed these values. Very important with multipliers\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As when adding observations,  `pf.add_parameters()` returns a dataframe. Take a look. You may recognize alot of the information that appears in a PEST `*parameter data` section. All of this is still only housed in memory for now. We will write the PEST control file later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:37.998663Z",
     "iopub.status.busy": "2022-04-11T03:51:37.998663Z",
     "iopub.status.idle": "2022-04-11T03:51:38.014149Z",
     "shell.execute_reply": "2022-04-11T03:51:38.012925Z"
    }
   },
   "outputs": [],
   "source": [
    "df_gr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `add_parameters()` call also wrote a template file that PEST(++) will use to populate the multiplier array at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember!  no pest control file has been made yet. `PstFrom` is simply preparing to make a control file later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add pilot point (medium scale) multiplier parameters to the same model input file. These multipliers are assigned to pilot points, which are subsequently interpolated to values in the array.\n",
    "\n",
    "You can add pilot points in two ways:\n",
    "\n",
    "1. `PstFrom` can generate them for you on a regular grid or \n",
    "2. you can supply `PstFrom` with existing pilot point location information in the form of a dataframe or a point-coverage shapefile. \n",
    "\n",
    "When you change `par_type` to \"pilotpoints\", by default, a regular grid of pilot points is setup using a default `pp_space` value of 10 (which is every 10th row and column). You can chnge this spacing by passing a integer to `pp_space` (as demonstrated below). \n",
    "\n",
    "Alternatively you can specify a filename or dataframe with pilot point locations. If you supply `pp_space` as a `str` it is assumed to be a filename. The extension is the guide: \".csv\" for dataframe, \".shp\" for shapefile (point-type). Anything else and the file is assumed to be a pilot points file type. The dataframe (or .csv file) must have \"name\", \"x\", and \"y\" as columns - it can have more, but must have those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:38.017912Z",
     "iopub.status.busy": "2022-04-11T03:51:38.017912Z",
     "iopub.status.idle": "2022-04-11T03:51:54.515499Z",
     "shell.execute_reply": "2022-04-11T03:51:54.515499Z"
    }
   },
   "outputs": [],
   "source": [
    "# pilot point (medium) scale parameters\n",
    "df_pp = pf.add_parameters(f,\n",
    "                    zone_array=ib,\n",
    "                    par_type=\"pilotpoints\",\n",
    "                    geostruct=grid_gs,\n",
    "                    par_name_base=f.split('.')[1].replace(\"_\",\"\")+\"pp\",\n",
    "                    pargp=f.split('.')[1].replace(\"_\",\"\")+\"pp\",\n",
    "                    lower_bound=0.2,upper_bound=5.0,\n",
    "                    ult_ubound=100, ult_lbound=0.01,\n",
    "                    pp_space=4) # `PstFrom` will generate a unifrom grid of pilot points in every 4th row and column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(4,6))\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.pcolormesh(sr.xcentergrid, sr.ycentergrid,ib)\n",
    "ax.scatter(df_pp.x,df_pp.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, add the constant (coarse) parameter multiplier. This is a single multiplier value applied to all values in the array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:54.520486Z",
     "iopub.status.busy": "2022-04-11T03:51:54.520486Z",
     "iopub.status.idle": "2022-04-11T03:51:54.549053Z",
     "shell.execute_reply": "2022-04-11T03:51:54.548419Z"
    }
   },
   "outputs": [],
   "source": [
    "# constant (coarse) scale parameters\n",
    "df_cst = pf.add_parameters(f,\n",
    "                    zone_array=ib,\n",
    "                    par_type=\"constant\",\n",
    "                    geostruct=grid_gs,\n",
    "                    par_name_base=f.split('.')[1].replace(\"_\",\"\")+\"cn\",\n",
    "                    pargp=f.split('.')[1].replace(\"_\",\"\")+\"cn\",\n",
    "                    lower_bound=0.2,upper_bound=5.0,\n",
    "                    ult_ubound=100, ult_lbound=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see three template files have been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to navigate to the `template_ws` and inspect these files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do that for Kh in the other layers. We are going to be doing this a few times, so lets write a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:54.559058Z",
     "iopub.status.busy": "2022-04-11T03:51:54.558032Z",
     "iopub.status.idle": "2022-04-11T03:51:54.563047Z",
     "shell.execute_reply": "2022-04-11T03:51:54.562053Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_mult_pars(f, lb=0.2, ub=5.0, ulb=0.01, uub=100, add_coarse=True):\n",
    "    if isinstance(f,str):\n",
    "        base = f.split(\".\")[1].replace(\"_\",\"\")\n",
    "    else:\n",
    "        base = f[0].split(\".\")[1]\n",
    "    # grid (fine) scale parameters\n",
    "    pf.add_parameters(f,\n",
    "                    zone_array=ib,\n",
    "                    par_type=\"grid\", #specify the type, these will be unique parameters for each cell\n",
    "                    geostruct=grid_gs, # the gestatisical structure for spatial correlation \n",
    "                    par_name_base=base+\"gr\", #specify a parameter name base that allows us to easily identify the filename and parameter type. \"_gr\" for \"grid\", and so forth.\n",
    "                    pargp=base+\"gr\", #likewise for the parameter group name\n",
    "                    lower_bound=lb, upper_bound=ub, #parameter lower and upper bound\n",
    "                    ult_ubound=uub, ult_lbound=ulb # The ultimate bounds for multiplied model input values. Here we are stating that, after accounting for all multipliers, Kh cannot exceed these values. Very important with multipliers\n",
    "                    )\n",
    "                    \n",
    "    # pilot point (medium) scale parameters\n",
    "    pf.add_parameters(f,\n",
    "                        zone_array=ib,\n",
    "                        par_type=\"pilotpoints\",\n",
    "                        geostruct=grid_gs,\n",
    "                        par_name_base=base+\"pp\",\n",
    "                        pargp=base+\"pp\",\n",
    "                        lower_bound=lb, upper_bound=ub,\n",
    "                        ult_ubound=uub, ult_lbound=ulb,\n",
    "                        pp_space=4) # `PstFrom` will generate a unifrom grid of pilot points in every 4th row and column\n",
    "    if add_coarse==True:\n",
    "        # constant (coarse) scale parameters\n",
    "        pf.add_parameters(f,\n",
    "                            zone_array=ib,\n",
    "                            par_type=\"constant\",\n",
    "                            geostruct=grid_gs,\n",
    "                            par_name_base=base+\"cn\",\n",
    "                            pargp=base+\"cn\",\n",
    "                            lower_bound=lb, upper_bound=ub,\n",
    "                            ult_ubound=uub, ult_lbound=ulb)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:54.567003Z",
     "iopub.status.busy": "2022-04-11T03:51:54.566040Z",
     "iopub.status.idle": "2022-04-11T03:51:54.862241Z",
     "shell.execute_reply": "2022-04-11T03:51:54.861243Z"
    }
   },
   "outputs": [],
   "source": [
    "for f in files[1:]:\n",
    "    add_mult_pars(f, lb=0.2, ub=5.0, ulb=0.01, uub=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well...hot damn, wasn't that easy? Let's speed through the other array parameter files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:54.872213Z",
     "iopub.status.busy": "2022-04-11T03:51:54.866229Z",
     "iopub.status.idle": "2022-04-11T03:51:56.011487Z",
     "shell.execute_reply": "2022-04-11T03:51:56.010465Z"
    }
   },
   "outputs": [],
   "source": [
    "# for Kv\n",
    "tag = \"npf_k33\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "for f in files:\n",
    "    add_mult_pars(f, lb=0.2, ub=5.0, ulb=0.01, uub=100)\n",
    "\n",
    "# for Ss\n",
    "tag = \"sto_ss\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "# only for layer 2 and 3; we aren't monsters\n",
    "for f in files[1:]: \n",
    "    add_mult_pars(f, lb=0.2, ub=5.0, ulb=1e-6, uub=1e-3)\n",
    "\n",
    "# For Sy\n",
    "tag = \"sto_sy\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "# only for layer 1\n",
    "f = files[0]\n",
    "add_mult_pars(f, lb=0.2, ub=5.0, ulb=0.01, uub=0.4)\n",
    "\n",
    "\n",
    "# For porosity\n",
    "tag = \"ne_\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "# only for layer 1\n",
    "for f in files: \n",
    "    add_mult_pars(f, lb=0.2, ub=5.0, ulb=0.01, uub=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom!  We just conquered property parameterization in a big way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2. Spatial and Temporal Correlation\n",
    "\n",
    "Now, you may be thinking \"shouldn't recharge have temporal correlation as well?\". \n",
    "\n",
    "Damn straight it should. Now, this requires a little trickery because native handling in spatiotemporal correlation is hard to do.  So what we are going to do is split this correlation into two setup of multiplier parameters.  One set of parameters will be constant in space but vary (and be correlated) in time.  The other set of multiplier parameters will be constant in time but vary (and be correlated) in space.  Since both of these sets of parameters are multipliers, we implicitly represent the concept that recharge is uncertain and correlated in both space and time.  Easy as!\n",
    "\n",
    "First we need to construct a container of stress period datetimes. (This relies on specifying the start_datetime argument when instantiating `PstFrom`.) These datetime values will specify the postion of parameters on the time-axis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:56.017472Z",
     "iopub.status.busy": "2022-04-11T03:51:56.017472Z",
     "iopub.status.idle": "2022-04-11T03:51:56.028729Z",
     "shell.execute_reply": "2022-04-11T03:51:56.027688Z"
    }
   },
   "outputs": [],
   "source": [
    "# build up a container of stress period start datetimes - this will\n",
    "# be used to specify the datetime of each multipler parameter\n",
    "\n",
    "dts = pd.to_datetime(start_datetime) + pd.to_timedelta(np.cumsum(sim.tdis.perioddata.array[\"perlen\"]),unit='d')\n",
    "\n",
    "dts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use the same parameter group name (`pargp`) and same geostruct, `PstFrom` will treat parameters setup across different calls to `add_parameters()` as correlated - ***WARNING*** do not try to express spatial and temporal correlation together - as discussed above, #badtimes.  In this case, we want to express temporal correlation in the recharge multiplier parameters that are \"constant\" type in space so that there is one recharge multiplier parameter for each stress period that shares a parameter group name across different calls to `add_parameters`. So, we use the same parameter group names for each stress period data file, and specify the `datetime` and `geostruct` arguments.\n",
    "\n",
    "Including temporal correlation introduces an additional challenge. Interpolation between points that share a common coordinate creates all types of trouble. We are going to have many parameters during each stress period (a single point on the time-axis). To get around this challenge we need to be a bit sneaky.\n",
    "\n",
    "\n",
    "First, we will apply the multiple *spatial* scales of parameter multiplers (`constant`, `pilot point` and `grid`) as we did for hyraulic properties.  We do this for all recharge files at once, which tells `PstFrom` to broadcast (e.g. share) the same parameters for all of those files: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Recharge; \n",
    "tag = \"rch_recharge\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "# the spatial multiplier parameters; just use the same function\n",
    "add_mult_pars(files, lb=0.2, ub=5.0, ulb=2e-5, uub=2e-4, add_coarse=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will asign an additional `constant` multiplier parameter for each recharge stress-period file (so, a single multiplier for all recharge paramaters for each stress period). We will specify temporal correlation for these `constant` multipliers. These will all have the same parameter group name, as discussed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:56.038687Z",
     "iopub.status.busy": "2022-04-11T03:51:56.037658Z",
     "iopub.status.idle": "2022-04-11T03:51:59.208457Z",
     "shell.execute_reply": "2022-04-11T03:51:59.209454Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for f in files:   \n",
    "    # multiplier that includes temporal correlation\n",
    "    # get the stress period number from the file name\n",
    "    kper = int(f.split('.')[1].split('_')[-1]) - 1  \n",
    "    # add the constant parameters (with temporal correlation)\n",
    "    pf.add_parameters(filenames=f,\n",
    "                    zone_array=ib,\n",
    "                    par_type=\"constant\",\n",
    "                    par_name_base=f.split('.')[1]+\"tcn\",\n",
    "                    pargp=f.split('.')[1]+\"tcn\",\n",
    "                    lower_bound=0.5, upper_bound=1.5,\n",
    "                    ult_ubound=2e-4, ult_lbound=2e-5,\n",
    "                    datetime=dts[kper], # this places the parameter value on the \"time axis\"\n",
    "                    geostruct=temporal_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. List Files\n",
    "\n",
    "Adding parameters from list-type files follows similar principles. As with observation files, they must be tabular. Certain columns are specified as index columns and are used to populate parameter names, as well as provide the parameters' spatial location. Other columns are specified as containing parameter values. \n",
    "\n",
    "Parameters can be `grid` or `constant`. As before, values can be assigned `directly`, as `multipliers` or as `additives`.\n",
    "\n",
    "We will demonstrate for the boundary-condition input files. \n",
    "\n",
    "Starting off with GHBs. Let's inspect the folder. As you can see, there is a single input file (GHB parameters are assumed to not vary over time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:59.213481Z",
     "iopub.status.busy": "2022-04-11T03:51:59.212448Z",
     "iopub.status.idle": "2022-04-11T03:51:59.224532Z",
     "shell.execute_reply": "2022-04-11T03:51:59.224532Z"
    }
   },
   "outputs": [],
   "source": [
    "tag = \"ghb_stress_period_data\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these boundaries are likely to be very influential, we want to include a robust representation of their uncertainty - both head and conductance and at multiple scales.  \n",
    "\n",
    "Let's parameterize both GHB conductance and head:\n",
    "\n",
    " - For conductance, we shall use two scales of `multiplier` parameters (`constant` and `grid`).\n",
    "\n",
    " - For heads, multipliers are not ideal. Insead we will use `additive` parameters. Again, with a coarse and fine scale.\n",
    "\n",
    " **ATTENTION!** \n",
    " \n",
    " Additive parameters by default get assigned an initial parameter value of zero. This can be problematic later on when computing the derivatives. Be sure to either apply a parameter offset, or use \"absolute\" increment types in the parameter group section (we will implement the latter option further on in the current tutorial.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:59.232511Z",
     "iopub.status.busy": "2022-04-11T03:51:59.227524Z",
     "iopub.status.idle": "2022-04-11T03:51:59.334189Z",
     "shell.execute_reply": "2022-04-11T03:51:59.333192Z"
    }
   },
   "outputs": [],
   "source": [
    "tag = \"ghb_stress_period_data\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "\n",
    "for f in files:\n",
    "    # constant and grid scale multiplier conductance parameters\n",
    "    name = 'ghbcond'\n",
    "    pf.add_parameters(f,\n",
    "                        par_type=\"grid\",\n",
    "                        geostruct=grid_gs,\n",
    "                        par_name_base=name+\"gr\",\n",
    "                        pargp=name+\"gr\",\n",
    "                        index_cols=[0,1,2], #column containing lay,row,col\n",
    "                        use_cols=[4], #column containing conductance values\n",
    "                        lower_bound=0.1,upper_bound=10.0,\n",
    "                        ult_lbound=0.1, ult_ubound=100) #absolute limits\n",
    "    pf.add_parameters(f,\n",
    "                        par_type=\"constant\",\n",
    "                        geostruct=grid_gs,\n",
    "                        par_name_base=name+\"cn\",\n",
    "                        pargp=name+\"cn\",\n",
    "                        index_cols=[0,1,2],\n",
    "                        use_cols=[4],  \n",
    "                        lower_bound=0.1,upper_bound=10.0,\n",
    "                        ult_lbound=0.1, ult_ubound=100) #absolute limits\n",
    "\n",
    "    # constant and grid scale additive head parameters\n",
    "    name = 'ghbhead'\n",
    "    pf.add_parameters(f,\n",
    "                        par_type=\"grid\",\n",
    "                        geostruct=grid_gs,\n",
    "                        par_name_base=name+\"gr\",\n",
    "                        pargp=name+\"gr\",\n",
    "                        index_cols=[0,1,2],\n",
    "                        use_cols=[3],   # column containing head values\n",
    "                        lower_bound=-2.0,upper_bound=2.0,\n",
    "                        par_style=\"a\", # specify additive parameter\n",
    "                        transform=\"none\", # specify not log-transform\n",
    "                        ult_lbound=32.5, ult_ubound=42) #absolute limits; make sure head is never lower than the bottom of layer1\n",
    "    pf.add_parameters(f,\n",
    "                        par_type=\"constant\",\n",
    "                        geostruct=grid_gs,\n",
    "                        par_name_base=name+\"cn\",\n",
    "                        pargp=name+\"cn\",\n",
    "                        index_cols=[0,1,2],\n",
    "                        use_cols=[3],\n",
    "                        lower_bound=-2.0,upper_bound=2.0, \n",
    "                        par_style=\"a\", \n",
    "                        transform=\"none\",\n",
    "                        ult_lbound=32.5, ult_ubound=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy peasy.\n",
    "\n",
    "Now, this will make some people uncomfortable, but how well do we really ever know historic water use flux rates in space and in time? hmmm, not really! And just a little uncertainty in historic water use can result in large changes in simulated water levels...So lets add parameters to represent that uncertainty in the model inputs.\n",
    "\n",
    "For wells it may not (or it may...) make sense to include spatial correlation. Here we will assume temporal correlation - its reasonable that pumping rates today will be similar to pumping rates yesterday. \n",
    "\n",
    "Pumping rates for different stress periods are in separate files. We will call `.add_parameters()` for each file. But we want to specify correlation between parameters in different files. As explained above for recharge, we do this with the parameter group name.\n",
    "\n",
    "OK, let's get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, including temporal correlation introduces an additional challenge. We use the same approach described for recharge parmaeters:\n",
    "\n",
    " - First, we will asign a `constant` multiplier parameter for each WEL stress-period file (so, a single multiplier for all well pumping rates for each stress period). We will specify temporal correlation for these `constant` multipliers.\n",
    "\n",
    " - Then, we will also have `grid` type multiplier parameters for each WEL stress period file (so, multipliers for individual well pumping rate during each stress period). These will not include (temporal) correlation. (We could in principle include spatial correlation here if we wanted to; but let's not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:51:59.340175Z",
     "iopub.status.busy": "2022-04-11T03:51:59.339187Z",
     "iopub.status.idle": "2022-04-11T03:52:00.538272Z",
     "shell.execute_reply": "2022-04-11T03:52:00.537278Z"
    }
   },
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(template_ws) if \"wel_stress_period_data\" in f and f.endswith(\".txt\")]\n",
    "\n",
    "for f in files:\n",
    "    # get the stress period number from the file name\n",
    "    kper = int(f.split('.')[1].split('_')[-1]) - 1  \n",
    "    \n",
    "    # add the constant parameters (with temporal correlation)\n",
    "    pf.add_parameters(filenames=f,\n",
    "                        index_cols=[0,1,2], #columns that specify cell location\n",
    "                        use_cols=[3],       #columns with parameter values\n",
    "                        par_type=\"constant\",    #each well will be adjustable\n",
    "                        par_name_base=\"welcst\",\n",
    "                        pargp=\"welcst\", \n",
    "                        upper_bound = 1.5, lower_bound=0.5,\n",
    "                        datetime=dts[kper], # this places the parameter value on the \"time axis\"\n",
    "                        geostruct=temporal_gs)\n",
    "    \n",
    "    # add the grid parameters; each individual well\n",
    "    pf.add_parameters(filenames=f,\n",
    "                        index_cols=[0,1,2], #columns that specify cell location \n",
    "                        use_cols=[3],       #columns with parameter values\n",
    "                        par_type=\"grid\",    #each well will be adjustable\n",
    "                        par_name_base=\"welgrd\",\n",
    "                        pargp=\"welgrd\", \n",
    "                        upper_bound = 1.5, lower_bound=0.5,\n",
    "                        datetime=dts[kper]) # this places the parameter value on the \"time axis\"\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, our favourite (not!) boundary-condition: SFR.\n",
    "\n",
    "Let's parameterize conductance (time-invariant) and inflow (time-variant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:00.547241Z",
     "iopub.status.busy": "2022-04-11T03:52:00.546247Z",
     "iopub.status.idle": "2022-04-11T03:52:00.615481Z",
     "shell.execute_reply": "2022-04-11T03:52:00.614347Z"
    }
   },
   "outputs": [],
   "source": [
    "# SFR conductance\n",
    "tag = \"sfr_packagedata\"\n",
    "files = [f for f in os.listdir(template_ws) if tag in f.lower() and f.endswith(\".txt\")]\n",
    "assert len(files) == 1 # There can be only one! It is tradition. Jokes.\n",
    "print(files)\n",
    "f = files[0]\n",
    "# constant and grid scale multiplier conductance parameters\n",
    "name = \"sfrcond\"\n",
    "pf.add_parameters(f,\n",
    "                par_type=\"grid\",\n",
    "                geostruct=grid_gs,\n",
    "                par_name_base=name+\"gr\",\n",
    "                pargp=name+\"gr\",\n",
    "                index_cols=[0,2,3],\n",
    "                use_cols=[9],\n",
    "                lower_bound=0.1,upper_bound=10.0,\n",
    "                ult_lbound=0.01, ult_ubound=10) #absolute limits\n",
    "pf.add_parameters(f,\n",
    "                par_type=\"constant\",\n",
    "                geostruct=grid_gs,\n",
    "                par_name_base=name+\"cn\",\n",
    "                pargp=name+\"cn\",\n",
    "                index_cols=[0,2,3],\n",
    "                use_cols=[9],\n",
    "                lower_bound=0.1,upper_bound=10.0,\n",
    "                ult_lbound=0.01, ult_ubound=10) #absolute limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:00.624312Z",
     "iopub.status.busy": "2022-04-11T03:52:00.623277Z",
     "iopub.status.idle": "2022-04-11T03:52:01.337431Z",
     "shell.execute_reply": "2022-04-11T03:52:01.336434Z"
    }
   },
   "outputs": [],
   "source": [
    "# SFR inflow\n",
    "files = [f for f in os.listdir(template_ws) if \"sfr_perioddata\" in f and f.endswith(\".txt\")]\n",
    "for f in files:\n",
    "    # get the stress period number from the file name\n",
    "    kper = int(f.split('.')[1].split('_')[-1]) - 1  \n",
    "    # add the parameters\n",
    "    pf.add_parameters(filenames=f,\n",
    "                        index_cols=[0], #reach number\n",
    "                        use_cols=[2],   #columns with parameter values\n",
    "                        par_type=\"grid\",    \n",
    "                        par_name_base=\"sfrgr\",\n",
    "                        pargp=\"sfrgr\", \n",
    "                        upper_bound = 1.5, lower_bound=0.5, #don't need ult_bounds because it is a single multiplier\n",
    "                        datetime=dts[kper], # this places the parameter value on the \"time axis\"\n",
    "                        geostruct=temporal_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damn!  we just parameterized many recognized sources of model input uncertainty at several spatial and temporal scales.  And we expressed spatial and temporal correlation in those parameters.  One last set of parameters that we will need later for sequential data assimilation - initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(template_ws) if \"ic_strt\" in f and f.endswith(\".txt\")]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    base = f.split(\".\")[1].replace(\"_\",\"\")\n",
    "    df = pf.add_parameters(f,par_type=\"grid\",par_style=\"d\",\n",
    "                      pargp=base,par_name_base=base,upper_bound=50,\n",
    "                     lower_bound=15,zone_array=ib,transform=\"none\")\n",
    "    print(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. The Forward Run Script\n",
    "\n",
    "OK! So, we almost have all the base building blocks for a PEST(++) dataset. We have some (1) observations and some (2) parameters. We are still missing (3) the \"forward run\" script. Recall that in the PEST world, the \"model\" is not just the numerical model (e.g. MODFLOW). Instead it is a composite of the numerical model (or models) and pre- and post-processing steps, encapsulated in a \"forward run\" script which can be called from the command line. This command line instruction is what PEST(++) sees as \"the model\". During execution, PEST(++) writes values to parameter files, runs \"the model\", and then reads values from the observation files.\n",
    "\n",
    "`PstFrom` automates the generation of such a script when constructing the PEST control file. The script is written to file named `forward_run.py`. It is written in Python (this is not a PEST(++) requirement, merely a convenience...we are working in Python after all...). \n",
    "\n",
    "How about we see that in action? Magic time! Let's create the PEST control file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:01.347334Z",
     "iopub.status.busy": "2022-04-11T03:52:01.346334Z",
     "iopub.status.idle": "2022-04-11T03:52:03.279964Z",
     "shell.execute_reply": "2022-04-11T03:52:03.280957Z"
    }
   },
   "outputs": [],
   "source": [
    "pst = pf.build_pst()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! Done. (Well almost.) Check the folder. You should see a new .pst file and the `forward_run.py` file. By default, the .pst file is named after the original model folder name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:03.287903Z",
     "iopub.status.busy": "2022-04-11T03:52:03.286906Z",
     "iopub.status.idle": "2022-04-11T03:52:03.293887Z",
     "shell.execute_reply": "2022-04-11T03:52:03.292938Z"
    }
   },
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".py\") or f.endswith(\".pst\") ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get to the `pst` object later on (see also the \"intro to pyemu\" tutorial notebook). For now, let's focus on the `forward_run.py` script. It is printed out below.\n",
    "\n",
    "This script does a bunch of things:\n",
    " - it loads necessary dependecies\n",
    " - it removes model output files to avoid the possibility of files from a previous model run being read by mistake;\n",
    " - it runs pre-processing steps (see `pyemu.helpers.apply_list_and_array_pars()`;\n",
    " - it executes system commands (usually running the simulator, i.e. MODFLOW). (*This is still missing. We will demonstrate next.*)\n",
    " - it executes post-processing steps; (*for now there aren't any*)\n",
    " - ...it washes the dishes (sorry, no it doesn't...this feature is still in development)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:03.297879Z",
     "iopub.status.busy": "2022-04-11T03:52:03.296880Z",
     "iopub.status.idle": "2022-04-11T03:52:03.311091Z",
     "shell.execute_reply": "2022-04-11T03:52:03.310097Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = [print(line.rstrip()) for line in open(os.path.join(template_ws,\"forward_run.py\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty amazing. But as we just saw, we still need to add commands to actualy run the model.\n",
    "\n",
    "`PstFrom` allows you to pass a list of system commands which will be executed in sequence. It also has methods for including Python functions that run before or after the system commands. These make pre-/post-processing a piece of cake. In fact, we have already started to add to it. Remember all of the multiplier and additive parameters we setup? These all require pre-processing steps to convert the PEST-generated multipliers into model input values. `PstFrom` will automatically add these functions to the `forward_run.py` script. Nifty, hey?\n",
    "\n",
    "Next we will demonstrate how to specify the system commands and add Python functions as processing steps.\n",
    "\n",
    "#### 6.2. Sys Commands\n",
    "\n",
    "Let's start by adding a command line instruction. These are stored as a list in `PstFrom.mod_sys_cmds`, which is currently empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:03.318073Z",
     "iopub.status.busy": "2022-04-11T03:52:03.317112Z",
     "iopub.status.idle": "2022-04-11T03:52:03.324159Z",
     "shell.execute_reply": "2022-04-11T03:52:03.324159Z"
    }
   },
   "outputs": [],
   "source": [
    "pf.mod_sys_cmds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a MODFLOW6 model from the command line, you can simply execute `mf6` in the model folder. So, we can add this command by appending it to the list. (Do this only once! Every time you append 'mf6' results in an additional call to MODFLOW6, meaning the model would be run multiple times.)\n",
    "\n",
    "`PstFrom` will add a line to `forward_run.py` w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:03.328148Z",
     "iopub.status.busy": "2022-04-11T03:52:03.328148Z",
     "iopub.status.idle": "2022-04-11T03:52:03.339662Z",
     "shell.execute_reply": "2022-04-11T03:52:03.338666Z"
    }
   },
   "outputs": [],
   "source": [
    "pf.mod_sys_cmds.append(\"mf6\") #do this only once\n",
    "pf.mod_sys_cmds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to run MODPATH7, so we need to add that to the list of system commands. In this case we also need to specify the modpath sim file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:03.343652Z",
     "iopub.status.busy": "2022-04-11T03:52:03.342691Z",
     "iopub.status.idle": "2022-04-11T03:52:03.355362Z",
     "shell.execute_reply": "2022-04-11T03:52:03.354366Z"
    }
   },
   "outputs": [],
   "source": [
    "pf.mod_sys_cmds.append(\"mp7 freyberg_mp.mpsim\") #do this only once\n",
    "pf.mod_sys_cmds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's re-build the Pst control file and check out the changes ot the `forward_run.py` script.\n",
    "\n",
    "You should see that `pyemu.os_utils.run(r'mf6')` has been added after the pre-processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:03.370356Z",
     "iopub.status.busy": "2022-04-11T03:52:03.370356Z",
     "iopub.status.idle": "2022-04-11T03:52:05.279219Z",
     "shell.execute_reply": "2022-04-11T03:52:05.278255Z"
    }
   },
   "outputs": [],
   "source": [
    "pst = pf.build_pst()\n",
    "\n",
    "_ = [print(line.rstrip()) for line in open(os.path.join(template_ws,\"forward_run.py\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. Extra pre- and post-processing functions\n",
    "\n",
    "You will also certainly need to include some additional processing steps.  These are supported thru the `PstFrom.pre_py_cmds` and `PstFrom.post_py_cmds`, which are lists for pre and post model run python commands and `PstFrom.pre_sys_cmds` and `PstFrom.post_sys_cmds`, which are lists for pre and post model run system commands (these are wrapped in `pyemu.os_utils.run()`.  \n",
    "\n",
    "But what if your additional steps are actually an entire python function? Well, we got that too! `PstFrom.add_py_function()`. This method allows you to get functions from another (pre-prepared) python source file and add them to the `forward_run.py` script. We will deonstrate this to post-process secondary model observations after each run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see this py-sauce in action: we are going to add a little post-processing function to extract the final simulated water level for all model cells for the last stress period from the MF6 binary headsave file and save them to ASCII format so that PEST(++) can read them with instruction files.  And, while we are at it, lets also extract the global water budget info from the MF6 listing file and store it in dataframes - these are ususally good numbers to watch!  We will need the simulated water level arrays later for sequential data assimilation (wouldnt it be nice if MF6 supported the writing of ASCII format head arrays?).  Anyway, this function is stored in the \"helpers.py\" script.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.add_py_function(\"helpers.py\",\"extract_hds_arrays_and_list_dfs()\",is_pre_cmd=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That last argument - `is_pre_cmd` tells `PstFrom` if the python function should be treated as a pre-processor or a post-processor. So we have added that post-processor, but we still need to setup pest observations for those ASCII head arrays.  Let's do that by first calling that function to operate once within the `template_ws` to generate the arrays and then we can add them with `add_observations()`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "helpers.test_extract_hds_arrays(template_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(template_ws) if f.startswith(\"hdslay\")]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    pf.add_observations(f,prefix=f.split(\".\")[0],obsgp=f.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [\"inc.csv\",\"cum.csv\"]:\n",
    "    df = pd.read_csv(os.path.join(template_ws,f),index_col=0)\n",
    "    pf.add_observations(f,index_cols=[\"totim\"],use_cols=list(df.columns.values),\n",
    "                        prefix=f.split('.')[0],obsgp=f.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crushed it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 6.3.1. Secondary Observations\n",
    "\n",
    "Often it is usefull to include \"secondary model outcomes\" as observations. These can be important components in a history-matching dataset to tease out specific aspects of system behaviour (e.g. head differences between aquifer layers to inform vertical permeabilities). Or they may be simple summaries of modelled outputs which are of interest for a prediction (e.g. minimum simulated head over a given period).\n",
    "\n",
    "If you inspect the tutorial folder you will find a file named `helpers.py`. This is a python source file which we have prepared for you. (Open it to see how it is organized.) It contains a function named `process_secondary_obs()`. This function reads the model output .csv files, processes them and writes a series of new observation .csv files. These new files contain (1) the temporal-differences between head and SFR observations, and (2) the difference in heads between the top and bottom layers at each observation point. The new .csv files are named `heads.tdiff.csv`,`sfr.tdiff.csv` and `heads.vdiff.csv` respectively.\n",
    "\n",
    "First, lets load the function here and run it so you can see what happens. (And to make sure that the observation files are in the template folder!) \n",
    "\n",
    "Run the next cell, then inspect the template folder. You should see three new csv files. These are the new secondary observations calculated by the post-processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:05.282303Z",
     "iopub.status.busy": "2022-04-11T03:52:05.281249Z",
     "iopub.status.idle": "2022-04-11T03:52:05.310643Z",
     "shell.execute_reply": "2022-04-11T03:52:05.309644Z"
    }
   },
   "outputs": [],
   "source": [
    "# load and save the truth hds observation values so that the secondary obs\n",
    "# process yields the \"measured\" values.\n",
    "hds_meas = pd.read_csv(os.path.join('..', '..', 'models', 'freyberg_mf6_truth',\"heads.csv\"),\n",
    "                    index_col=0)\n",
    "hds_meas.to_csv(os.path.join(template_ws,\"head.csv\"))\n",
    "from helpers import  process_secondary_obs\n",
    "\n",
    "process_secondary_obs(ws=template_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:05.313639Z",
     "iopub.status.busy": "2022-04-11T03:52:05.312642Z",
     "iopub.status.idle": "2022-04-11T03:52:05.325029Z",
     "shell.execute_reply": "2022-04-11T03:52:05.326025Z"
    }
   },
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now let's add this function to the `forward_run.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:05.328875Z",
     "iopub.status.busy": "2022-04-11T03:52:05.328875Z",
     "iopub.status.idle": "2022-04-11T03:52:05.341597Z",
     "shell.execute_reply": "2022-04-11T03:52:05.340564Z"
    }
   },
   "outputs": [],
   "source": [
    "pf.add_py_function(\"helpers.py\", # the file which contains the function\n",
    "                    \"process_secondary_obs(ws='.')\", #the function, making sure to specify any arguments it may requrie\n",
    "                    is_pre_cmd=False) # whether it runs before the model system command, or after. In this case, after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, boom! Bob's your uncle. As easy as that.\n",
    "\n",
    "Now, of course we want to add these observations to `PstFrom` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:05.347581Z",
     "iopub.status.busy": "2022-04-11T03:52:05.347581Z",
     "iopub.status.idle": "2022-04-11T03:52:05.450947Z",
     "shell.execute_reply": "2022-04-11T03:52:05.450947Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(os.path.join(template_ws, \"sfr.tdiff.csv\"), index_col=0)\n",
    "_ = pf.add_observations(\"sfr.tdiff.csv\", # the model output file to read\n",
    "                            insfile=\"sfr.tdiff.csv.ins\", #optional, the instruction file name\n",
    "                            index_cols=\"time\", #column header to use as index; can also use column number (zero-based) instead of the header name\n",
    "                            use_cols=list(df.columns.values), #names of columns that include observation values; can also use column number (zero-based) instead of the header name\n",
    "                            prefix=\"sfrtd\") #prefix to all observation \n",
    "                            \n",
    "df = pd.read_csv(os.path.join(template_ws, \"heads.tdiff.csv\"), index_col=0)\n",
    "_ = pf.add_observations(\"heads.tdiff.csv\", # the model output file to read\n",
    "                            insfile=\"heads.tdiff.csv.ins\", #optional, the instruction file name\n",
    "                            index_cols=\"time\", #column header to use as index; can also use column number (zero-based) instead of the header name\n",
    "                            use_cols=list(df.columns.values), #names of columns that include observation values; can also use column number (zero-based) instead of the header name\n",
    "                            prefix=\"hdstd\") #prefix to all observation names\n",
    "\n",
    "df = pd.read_csv(os.path.join(template_ws, \"heads.vdiff.csv\"), index_col=0)\n",
    "_ = pf.add_observations(\"heads.vdiff.csv\", # the model output file to read\n",
    "                            insfile=\"heads.vdiff.csv.ins\", #optional, the instruction file name\n",
    "                            index_cols=\"time\", #column header to use as index; can also use column number (zero-based) instead of the header name\n",
    "                            use_cols=list(df.columns.values), #names of columns that include observation values; can also use column number (zero-based) instead of the header name\n",
    "                            prefix=\"hdsvd\") #prefix to all observation names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to re-build the Pst control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:05.479869Z",
     "iopub.status.busy": "2022-04-11T03:52:05.459962Z",
     "iopub.status.idle": "2022-04-11T03:52:07.388051Z",
     "shell.execute_reply": "2022-04-11T03:52:07.387054Z"
    }
   },
   "outputs": [],
   "source": [
    "pst = pf.build_pst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(line.rstrip()) for line in open(os.path.join(template_ws,\"forward_run.py\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that `extract_hds_array_and_list_dfs()` has been added to the forward run script and it is being called after MF6 runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.406002Z",
     "iopub.status.busy": "2022-04-11T03:52:07.406002Z",
     "iopub.status.idle": "2022-04-11T03:52:07.416087Z",
     "shell.execute_reply": "2022-04-11T03:52:07.415109Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. After Building the Control File\n",
    "\n",
    "At this point, we can do some additional modifications that would typically be done that are problem specific.  Here we can tweak the setup, specifying things such as observation weights, parameter bounds, transforms, control data, etc.\n",
    "\n",
    "Note that any modifications made after calling `PstFrom.build_pst()` will only exist in memory - you need to call `pf.pst.write()` to record these changes to the control file on disk.  Also note that if you call `PstFrom.build_pst()` after making some changes, these changes will be lost.  \n",
    "\n",
    "For the current case, the main thing we haven't addressed are the observations from custom *.ins files,  observation weights, parameter group INCTYP's and forecasts.\n",
    "\n",
    "We will do so now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. Add Observations from INS files\n",
    "\n",
    "Recall that we wish to include observations of particle end time and status. As mentioned earlier, MP7 output files are not ina nicely organized tabular format - so we need to construct a custom instruction file. We will do this now:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get a file with the \"measured\" values - again, so that the `obsval` quantity in the control file is the \"truth\" value so that we can check our work later (since these \"observations\" will actually be treated as unknown forecasts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.419079Z",
     "iopub.status.busy": "2022-04-11T03:52:07.419079Z",
     "iopub.status.idle": "2022-04-11T03:52:07.435302Z",
     "shell.execute_reply": "2022-04-11T03:52:07.434341Z"
    }
   },
   "outputs": [],
   "source": [
    "shutil.copy(src=os.path.join('..', '..', 'models', 'freyberg_mf6_truth', 'freyberg_mp.mpend'),\n",
    "            dst=os.path.join(template_ws, 'freyberg_mp.mpend')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the instruction file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.443314Z",
     "iopub.status.busy": "2022-04-11T03:52:07.442279Z",
     "iopub.status.idle": "2022-04-11T03:52:07.449260Z",
     "shell.execute_reply": "2022-04-11T03:52:07.448294Z"
    }
   },
   "outputs": [],
   "source": [
    "# write a really simple instruction file to read the MODPATH end point file\n",
    "out_file = \"freyberg_mp.mpend\"\n",
    "ins_file = out_file + \".ins\"\n",
    "with open(os.path.join(template_ws, ins_file),'w') as f:\n",
    "    f.write(\"pif ~\\n\")\n",
    "    f.write(\"l7 w w w w !part_status! w w !part_time!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add these observations to the `Pst`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.454281Z",
     "iopub.status.busy": "2022-04-11T03:52:07.453251Z",
     "iopub.status.idle": "2022-04-11T03:52:07.482651Z",
     "shell.execute_reply": "2022-04-11T03:52:07.481653Z"
    }
   },
   "outputs": [],
   "source": [
    "pst.add_observations(ins_file=os.path.join(template_ws, ins_file),\n",
    "                    out_file=os.path.join(template_ws, out_file),\n",
    "                            pst_path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.495642Z",
     "iopub.status.busy": "2022-04-11T03:52:07.494618Z",
     "iopub.status.idle": "2022-04-11T03:52:07.512861Z",
     "shell.execute_reply": "2022-04-11T03:52:07.510867Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.loc[obs.obsnme=='part_status', 'weight'] = 0\n",
    "obs.loc[obs.obsnme=='part_status', 'obgnme'] = 'part'\n",
    "\n",
    "obs.loc[obs.obsnme=='part_time', 'weight'] = 0\n",
    "obs.loc[obs.obsnme=='part_time', 'obgnme'] = 'part'\n",
    "\n",
    "obs.iloc[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. Setting observation weights\n",
    "\n",
    "So far, we have automated the setup for PEST(++).  But one critical task remains and there is not an easy way to automate it:  setting the actual observed values and weights in the `* observation data` information. In practice, this will require some serious data munging skills and custom code - #lyf.\n",
    "\n",
    "`PstFrom` and `Pst` will both try to read existing model output files that correspond to instruction files and put those simulated values into the `* observation data` section for the observed values (the `obsval` quantity).  So now we need to edit the `* observation data` of the control file at this stage, assigning the respective measured values into the `obsval` column. \n",
    "\n",
    "What we are unable to deal with beforehand are the observation weights. Let's take a look at the observation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pf.pst.observation_data\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that's alot of observations. And remember, alot of these pertain to the \"prediction\" period or are quantities we just want to watch as part of the PEST(++) analysis process. We definitly do not want those to have an influence on history-matching.\n",
    "\n",
    "To be on the safe side, let's assign a weight of zero to all observations. Then, we will assign weights to specific observations that we wish to include as history-matching targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.530811Z",
     "iopub.status.busy": "2022-04-11T03:52:07.530811Z",
     "iopub.status.idle": "2022-04-11T03:52:07.542947Z",
     "shell.execute_reply": "2022-04-11T03:52:07.541950Z"
    }
   },
   "outputs": [],
   "source": [
    "# preemptive avoidance of sillyness\n",
    "obs['weight'] = 0\n",
    "obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the measured head values\n",
    "hds_meas = pd.read_csv(os.path.join('..', '..', 'models', 'freyberg_mf6_truth',\"heads.csv\"),\n",
    "                    index_col=0)\n",
    "hds_meas.columns = hds_meas.columns.map(str.lower)\n",
    "hds_meas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the index and column values are baked into the `obsnme` quantity as \"time\" and \"usecol\" respectively. This is because PstFrom is our friend, and it kept track of lots of information, call it \"metadata\" (unless that term triggers you because of horrible past experiences at gov organizations!). We can now use this to help us slice and select specific observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.oname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_obs = obs.loc[obs.oname==\"hds\"]\n",
    "hds_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a subset of the full observation dataframe with just the \"trgw\" observations.  So now we just need to assign the actual observation values to these obs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets cast the \"time\" column to a float\n",
    "hds_obs.loc[:,\"time\"] = hds_obs.time.astype(float)\n",
    "# now process each unique \"usecol\" which is just the column\n",
    "# in the MF6 head obs csv file\n",
    "for usecol in hds_obs.usecol.unique():\n",
    "    # get a sub-dataframe pointer for just this usecol\n",
    "    uhds_obs = hds_obs.loc[hds_obs.usecol==usecol,:]\n",
    "    # get the corresponding measured values\n",
    "    meas = hds_meas.loc[uhds_obs.time.values,usecol]\n",
    "    # safety checks - explicit is better than implicit!\n",
    "    assert meas.shape[0] == uhds_obs.shape[0]\n",
    "    assert meas.loc[pd.isna(meas)].shape[0] == 0,pd.isna(meas)\n",
    "    # set the measured values in the global obs dataframe\n",
    "    obs.loc[uhds_obs.obsnme,\"obsval\"] = meas.values\n",
    "assert obs.loc[pd.isna(obs.obsval),:].shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's just check...\"trust but verify\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.loc[obs.usecol==usecol,\"obsval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_meas.loc[:,usecol]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same for sfr obs - we will go ahead and set the \"truth\" values for \"headwater\" and \"tailwater\" even though these are unobservable (and serve as forecasts) - this will help us later when we check the answers at the back of the book..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the measured head values\n",
    "sfr_meas = pd.read_csv(os.path.join('..', '..', 'models', 'freyberg_mf6_truth',\"sfr.csv\"),\n",
    "                    index_col=0)\n",
    "sfr_meas.columns = sfr_meas.columns.map(str.lower)\n",
    "sfr_meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for oname in sfr_meas.columns:\n",
    "    meas = sfr_meas.loc[:,oname]\n",
    "    sfr_obs = obs.loc[obs.apply(lambda x: oname in x.obsnme and x.oname==\"sfr\",axis=1),:].copy()\n",
    "    sfr_obs.loc[:,\"time\"] = sfr_obs.time.astype(float)\n",
    "    assert sfr_obs.shape[0] > 0,oname\n",
    "    assert sfr_obs.shape[0] == meas.shape[0],sfr_obs.shape\n",
    "    obs.loc[sfr_obs.obsnme,\"obsval\"] = meas.loc[sfr_obs.time.values].values\n",
    "assert obs.loc[pd.isna(obs.obsval),:].shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, just check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.loc[obs.apply(lambda x: oname in x.obsnme and x.oname==\"sfr\",axis=1),\"obsval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_meas.loc[:,oname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now let's use this same pandas magic to assign weights to each observation that has a measured counterpart. We will set weights to the inverse of the standard deviation of our measurement uncertainty (e.g. \"error-based weighting\").\n",
    "\n",
    "Let's say that stdevs for:\n",
    " - head measurements are 0.1m,\n",
    " - head time an vertical differences 0.01m\n",
    " - SFR direct and time-differnces will be 10% of the measured value (fancy-word alert: \"heteroscedasticity\")\n",
    "\n",
    "\n",
    "Recall from our model setup that we have \"measured\" field data of (1) groundwater levels in layer 1 and 3 from two locations for stress periods 2-13; and (2) surface-water flow at the \"tailwater\" gage for the same period. We need to assign weights explicitly to observations pertaining to these sites. Not to others.\n",
    "\n",
    "Head measurements sites are named:\n",
    " - trgw-0-26-6\n",
    " - trgw-2-26-6\n",
    " - trgw-0-3-8\n",
    " - trgw-2-3-8\n",
    "\n",
    "The streamflow site is named:\n",
    " - gage-1\n",
    "\n",
    "To make life easier, we will start by getting the history matching target observation names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.600893Z",
     "iopub.status.busy": "2022-04-11T03:52:07.596943Z",
     "iopub.status.idle": "2022-04-11T03:52:07.622212Z",
     "shell.execute_reply": "2022-04-11T03:52:07.620205Z"
    }
   },
   "outputs": [],
   "source": [
    "# to make life easier, slect only history matching time period\n",
    "hist_obs = obs.loc[(obs['time'].astype(float) > 3652.5) & (obs['time'].astype(float) <= 4018.5)]\n",
    "\n",
    "# get head observation names\n",
    "searchfor = ['trgw-0-26-6', 'trgw-2-26-6', 'trgw-0-3-8', 'trgw-2-3-8']\n",
    "hist_obs_hds = hist_obs.loc[hist_obs.obsnme.str.contains('|'.join(searchfor)),'obsnme'].values\n",
    "\n",
    "# SFR observation nnames\n",
    "hist_obs_sfr = hist_obs.loc[hist_obs.obsnme.str.contains('gage-1'),'obsnme'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hist_obs_hds.shape[0] > 0\n",
    "assert hist_obs_sfr.shape[0] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the names to slice and dice the observation data and update the wieghts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.641112Z",
     "iopub.status.busy": "2022-04-11T03:52:07.641112Z",
     "iopub.status.idle": "2022-04-11T03:52:07.652258Z",
     "shell.execute_reply": "2022-04-11T03:52:07.653255Z"
    }
   },
   "outputs": [],
   "source": [
    "# direct head measurments\n",
    "obs.loc[obs['obsnme'].isin(hist_obs_hds) & (obs['oname']=='hds'), 'weight'] = 1/0.1\n",
    "obs.loc[obs['obsnme'].isin(hist_obs_hds)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.667215Z",
     "iopub.status.busy": "2022-04-11T03:52:07.666217Z",
     "iopub.status.idle": "2022-04-11T03:52:07.679591Z",
     "shell.execute_reply": "2022-04-11T03:52:07.678594Z"
    }
   },
   "outputs": [],
   "source": [
    "# head time differences\n",
    "obs.loc[obs['obsnme'].isin(hist_obs_hds) & (obs['oname']=='hdstd'), 'weight'] = 1/0.01\n",
    "# head vertical differences\n",
    "obs.loc[obs['obsnme'].isin(hist_obs_hds) & (obs['oname']=='hdsvd'), 'weight'] = 1/0.005 # This value should refelect the accuracy of field measurments; in our case, they are very precise. Using higher uncetainry results in too much oise; this is a specific to this synthetic case and should not be taken as guidance for real-world problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.686572Z",
     "iopub.status.busy": "2022-04-11T03:52:07.685575Z",
     "iopub.status.idle": "2022-04-11T03:52:07.693869Z",
     "shell.execute_reply": "2022-04-11T03:52:07.693869Z"
    }
   },
   "outputs": [],
   "source": [
    "# SFR direct measurments\n",
    "weights = 1 / abs(0.1 * obs.loc[obs['obsnme'].isin(hist_obs_sfr), 'obsval'].values)\n",
    "obs.loc[obs['obsnme'].isin(hist_obs_sfr), 'weight'] = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, observation data weights have been updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.707865Z",
     "iopub.status.busy": "2022-04-11T03:52:07.697858Z",
     "iopub.status.idle": "2022-04-11T03:52:07.725566Z",
     "shell.execute_reply": "2022-04-11T03:52:07.724570Z"
    }
   },
   "outputs": [],
   "source": [
    "obs.loc[pst.nnz_obs_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3. Parameters with Zero as Intial Value\n",
    "\n",
    "Recall that we assigned additive parameters to the GHB heads. Our initial parameter values for these parameter types were set as 0 (zero). This creates a wee bit of trouble when calculating derivatives. There are a couple of ways we could get around it. One way is to add an \"offset\" to the parameter intial values and to the parmaeter bounds. Another is to use \"absolute\" increment types (INCTYP). See the PEST manual or PEST++ user guides for descriptions of increment types. \n",
    "\n",
    "We will apply both here. \n",
    "\n",
    "We will assign INCTYP as 'absolute'. We will leave DERINC as 0.01 (the default). It is a reasonable value in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.728608Z",
     "iopub.status.busy": "2022-04-11T03:52:07.728608Z",
     "iopub.status.idle": "2022-04-11T03:52:07.740599Z",
     "shell.execute_reply": "2022-04-11T03:52:07.740599Z"
    }
   },
   "outputs": [],
   "source": [
    "head_pargps = [i for i in pst.adj_par_groups if 'head' in i]\n",
    "head_pargps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.744554Z",
     "iopub.status.busy": "2022-04-11T03:52:07.744554Z",
     "iopub.status.idle": "2022-04-11T03:52:07.757045Z",
     "shell.execute_reply": "2022-04-11T03:52:07.757045Z"
    }
   },
   "outputs": [],
   "source": [
    "pst.parameter_groups.loc[head_pargps, 'inctyp'] = 'absolute'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the \"offset\" to parameter data entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.786932Z",
     "iopub.status.busy": "2022-04-11T03:52:07.785965Z",
     "iopub.status.idle": "2022-04-11T03:52:07.804994Z",
     "shell.execute_reply": "2022-04-11T03:52:07.803957Z"
    }
   },
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "par_names = par.loc[par.parval1==0].parnme\n",
    "\n",
    "par.loc[par_names].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.808975Z",
     "iopub.status.busy": "2022-04-11T03:52:07.807981Z",
     "iopub.status.idle": "2022-04-11T03:52:07.836274Z",
     "shell.execute_reply": "2022-04-11T03:52:07.837236Z"
    }
   },
   "outputs": [],
   "source": [
    "offset = -10\n",
    "par.loc[par_names, 'offset'] = offset\n",
    "par.loc[par_names, ['parval1', 'parlbnd', 'parubnd']] -= offset\n",
    "\n",
    "par.loc[par_names].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4. Forecasts\n",
    "\n",
    "**For most models there is a forecast/prediction that someone needs. Rather than waiting until the end of the project, the forecast should be entered into your thinking and workflow __right at the beginning__.  Here we do this explicitly by monitoring the forecasts as \"observations\" in the control file.  This way, for every PEST(++) analysis we do, we can watch what is happening to the forecasts - #winning **\n",
    "\n",
    "The optional PEST++ `++forecasts` control variable allows us to provide the names of one or more observations featured in the â€œobservation dataâ€ section of the PEST control file; these are treated as predictions in FOSM predictive uncertainty analysis by PESTPP-GLM. It is also a convenient way to keep track of \"forecast\" observations (makes post-processing a wee bit easier later on).\n",
    "\n",
    "Recall that, for our synthetic case we are interested in forecasting:\n",
    "\n",
    " - groundwater level in the upper layer at row 9 and column 1 (site named \"trgw-0-9-1\") in stress period 22 (time=640);\n",
    " - the \"tailwater\" surface-water/groundwater exchange during stress period 13 (time=367); and\n",
    " - the \"headwater\" surface-water/groundwater exchange at stress period 22 (time=640).\n",
    " - the particle travel time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.841226Z",
     "iopub.status.busy": "2022-04-11T03:52:07.840229Z",
     "iopub.status.idle": "2022-04-11T03:52:07.852515Z",
     "shell.execute_reply": "2022-04-11T03:52:07.852515Z"
    }
   },
   "outputs": [],
   "source": [
    "forecasts =[\n",
    "            'oname:sfr_otype:lst_usecol:tailwater_time:4383.5',\n",
    "            'oname:sfr_otype:lst_usecol:headwater_time:4383.5',\n",
    "            'oname:hds_otype:lst_usecol:trgw-0-9-1_time:4383.5',\n",
    "            'part_time'\n",
    "            ]\n",
    "\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fobs = obs.loc[forecasts,:]\n",
    "fobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just set this optional pest++ argument because it will trigger certain automatic behavior later in PESTPP-GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.855472Z",
     "iopub.status.busy": "2022-04-11T03:52:07.855472Z",
     "iopub.status.idle": "2022-04-11T03:52:07.868111Z",
     "shell.execute_reply": "2022-04-11T03:52:07.868111Z"
    }
   },
   "outputs": [],
   "source": [
    "pst.pestpp_options['forecasts'] = forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5. Re-write the Control File!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to re-**write** the PEST control file. But beware, if you re-**build** the `Pst`, all these changes will be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:07.878083Z",
     "iopub.status.busy": "2022-04-11T03:52:07.878083Z",
     "iopub.status.idle": "2022-04-11T03:52:08.148377Z",
     "shell.execute_reply": "2022-04-11T03:52:08.149338Z"
    }
   },
   "outputs": [],
   "source": [
    "pst.write(os.path.join(template_ws, 'freyberg_mf6.pst'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that was pretty epic. We now have a (very) high-dimensional PEST interface that includes secondary observations, as well as forecasts, ready to roll. \n",
    "\n",
    "If you inspect the folder, you will see PEST control file and all the necessary instruction and template files. Because we have >10k parameters, version 2 of the PEST control file was written by default. \n",
    "\n",
    "Shall we check that it works? Let's run PEST once (i.e. with NOPTMAX=0). Now, by default, noptmax is set to zero. But just to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:08.153327Z",
     "iopub.status.busy": "2022-04-11T03:52:08.152330Z",
     "iopub.status.idle": "2022-04-11T03:52:08.168890Z",
     "shell.execute_reply": "2022-04-11T03:52:08.166895Z"
    }
   },
   "outputs": [],
   "source": [
    "pst.control_data.noptmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so when we run PEST it will call the model once and then stop. If the next cell is sucessfull, then eveything is working. Check the folder, you should see PEST output files. (We will go into these and how to process PEST outcomes in subsequent tutorials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:08.171882Z",
     "iopub.status.busy": "2022-04-11T03:52:08.171882Z",
     "iopub.status.idle": "2022-04-11T03:52:19.868569Z",
     "shell.execute_reply": "2022-04-11T03:52:19.868569Z"
    }
   },
   "outputs": [],
   "source": [
    "pyemu.os_utils.run('pestpp-glm freyberg_mf6.pst', cwd=template_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Prior Parameter Covariance Matrix\n",
    "\n",
    "One the major reasons `PstFrom` was built is to help with building the Prior - both covariance matrix and ensemble - with geostatical correlation.  Remember all that business above related to geostatical structures and correlations?  This is where is pays off.\n",
    "\n",
    "Let's see how this works.  For cases with less than about 30,000 parameters, we can actually generate and visualize the prior parameter covariance matrix.  If you have more parameters, this matrix may not fit in memory.  But, not to worry, `PstFrom` has some trickery to help generate the geostatistical prior ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:19.880500Z",
     "iopub.status.busy": "2022-04-11T03:52:19.880500Z",
     "iopub.status.idle": "2022-04-11T03:52:56.202006Z",
     "shell.execute_reply": "2022-04-11T03:52:56.203003Z"
    }
   },
   "outputs": [],
   "source": [
    "# build the prior covariance matrix and store it as a compresed bianry file (otherwise it can get huge!)\n",
    "# depending on your machine, this may take a while...\n",
    "if pf.pst.npar < 35000:  #if you have more than about 35K pars, the cov matrix becomes hard to handle\n",
    "    cov = pf.build_prior(fmt='coo', filename=os.path.join(template_ws,\"prior_cov.jcb\"))\n",
    "    # and take a peek at a slice of the matrix\n",
    "    try: \n",
    "        x = cov.x.copy()\n",
    "        x[x==0] = np.NaN\n",
    "        plt.imshow(x[:1000,:1000])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "snap!  That big block must be a grid-scale parameter group..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.row_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now generate a prior parameter ensemble. This step is relevant for using pestpp-ies in subsequent tutorials. Note: you do not have to call `build_prior()` before calling `draw()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:52:56.243893Z",
     "iopub.status.busy": "2022-04-11T03:52:56.241899Z",
     "iopub.status.idle": "2022-04-11T03:53:17.140940Z",
     "shell.execute_reply": "2022-04-11T03:53:17.141937Z"
    }
   },
   "outputs": [],
   "source": [
    "pe = pf.draw(num_reals=200, use_specsim=True) # draw parameters from the prior distribution\n",
    "pe.enforce() # enforces parameter bounds\n",
    "pe.to_binary(os.path.join(template_ws,\"prior_pe.jcb\")) #writes the paramter ensemble to binary file\n",
    "assert pe.shape[1] == pst.npar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test-run one of these geostatistical realizations (always a good idea!).  We do this by replacing the `parval1` values in the control with a row from `pe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data.loc[:,\"parval1\"] = pe.loc[pe.index[0],pst.par_names].values\n",
    "pst.parameter_data.parval1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 0\n",
    "pst.write(os.path.join(template_ws,\"test.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-glm test.pst\",cwd=template_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the multiplier files in the \"`template_ws`/mult\" folder and the MF6 input files in the `template_ws` folder contain the values cooresponding to this realization, so we can visualize the multipler parameter process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(template_ws,\"mult2model_info.csv\"))\n",
    "kh1_df = df.loc[df.model_file.str.contains(\"npf_k_layer1\"),:]\n",
    "kh1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_arr = np.loadtxt(os.path.join(template_ws,kh1_df.org_file.iloc[0]))\n",
    "inp_arr = np.loadtxt(os.path.join(template_ws,kh1_df.model_file.iloc[0]))\n",
    "mlt_arrs = [np.loadtxt(os.path.join(template_ws,afile)) for afile in kh1_df.mlt_file]\n",
    "arrs = [org_arr]\n",
    "arrs.extend(mlt_arrs)\n",
    "arrs.append(inp_arr)\n",
    "names = [\"org\"]\n",
    "names.extend([mf.split('.')[0].split('_')[-1] for mf in kh1_df.mlt_file])\n",
    "names.append(\"MF6 input\")\n",
    "fig,axes = plt.subplots(1,kh1_df.shape[0]+2,figsize=(5*kh1_df.shape[0]+2,5))\n",
    "for i,ax in enumerate(axes.flatten()):\n",
    "    arr = np.log10(arrs[i])\n",
    "    arr[ib==0] = np.NaN\n",
    "    cb = ax.imshow(arr)\n",
    "    plt.colorbar(cb,ax=ax)\n",
    "    ax.set_title(names[i],loc=\"left\")\n",
    "plt.tight_layout()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Observation Uncertainty Covariance Matrix\n",
    "\n",
    "Lastly, not so common, let's also generate our observation covariance matrix. Often this step is omitted, in particular if the observation weights in the `* observation data` section reflect observation uncertainty. Here we will implement it expressly so as to:\n",
    "\n",
    " - make life easier for the authors to organize the tutorial notebooks;\n",
    " - because we are going to be fiddlying around with the observation weights later on, and this  way we keep a \"backup\" of our observation uncertainty;\n",
    " - and to demonstrate how to add autocorrelated transient noise! When is the last time you saw that in the wild?\n",
    "\n",
    "First let us start by generating a covariance matrix, using the weights recorded in the `pst.observation_data` section. This is accomplished with `pyemu.Cov.from_observation_data()`. This will generate a covariance matrix for **all** observations in the control file, even the zero-weighted ones. \n",
    "\n",
    "However, we are only interested in keep the non-zero weighted observations. We can use `pyemu` convenient Matrix manipulation functions to `.get()` the covariance matrix rows/columns that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:53:17.158893Z",
     "iopub.status.busy": "2022-04-11T03:53:17.147921Z",
     "iopub.status.idle": "2022-04-11T03:53:17.202775Z",
     "shell.execute_reply": "2022-04-11T03:53:17.202775Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "\n",
    "# generate cov matrix for all obs using weights in the control file\n",
    "obs_cov = pyemu.Cov.from_observation_data(pst, )\n",
    "\n",
    "# reduce cov down to only include non-zero obsverations\n",
    "obs_cov = obs_cov.get(row_names=pst.nnz_obs_names, col_names=pst.nnz_obs_names, )\n",
    "\n",
    "# side note: \n",
    "# going to save the diagonal (e.g no correlation) obsevration uncertainty cov matrix\n",
    "# to an external file for use in a later tutorial\n",
    "obs_cov.to_coo(os.path.join(template_ws,\"obs_cov_diag.jcb\"))\n",
    "\n",
    "# make it a dataframe to make life easier\n",
    "df = obs_cov.to_dataframe()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this returned a diagonal covariance matrix (e.g. only values in the diagonal are non-zero). This implies that there is no covariance between observation noise. \n",
    "\n",
    "You can plot the matrix, but due to the scale it wont look particularily interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:53:17.221727Z",
     "iopub.status.busy": "2022-04-11T03:53:17.215740Z",
     "iopub.status.idle": "2022-04-11T03:53:17.380611Z",
     "shell.execute_reply": "2022-04-11T03:53:17.381297Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(df.values)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...you can kinda see something there (thats the sfr flow obs...). Let's instead just look at a small part, focusing on observations that make up a single time series. So as you can see below, the matrix diagonal reflects individual obsevation uncertianties, whilst all the off-diagonals are zero (e.g. no correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:53:17.424183Z",
     "iopub.status.busy": "2022-04-11T03:53:17.413212Z",
     "iopub.status.idle": "2022-04-11T03:53:17.540871Z",
     "shell.execute_reply": "2022-04-11T03:53:17.539873Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_select = obs.loc[(obs.obgnme=='oname:hds_otype:lst_usecol:trgw-0-26-6') & (obs.weight>0)]\n",
    "\n",
    "plt.imshow(df.loc[obs_select.obsnme,obs_select.obsnme].values)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But recall that most of the observations are time-series. Here we are saying \"how wrong the measurement is today, has nothing to do with how wrong it was yesterday\". Is that reasonable? For some noise (e.g. white noise), yes, surely. But perhaps not for all noise. So how can we express that \"if my measurement was wrong yesterday, then it is *likely* to be wrong in the same way today\"? Through covariance, that's how. We can use the same priciples that we employed to specfy parameter spatial/temporal covariance earlier on.\n",
    "\n",
    "We will now demonstrate for the single time series we looked at above. \n",
    "\n",
    "First, construct a geostatistical structure and variogram. Here we are using a range of 90 days; for an exponential variogram `a` is range/3. From these, construct a generic covariance matrix for generic parameters with \"coordinates\" corresponding to the observation times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:53:18.026586Z",
     "iopub.status.busy": "2022-04-11T03:53:18.026586Z",
     "iopub.status.idle": "2022-04-11T03:53:18.396050Z",
     "shell.execute_reply": "2022-04-11T03:53:18.395051Z"
    }
   },
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(a=30,contribution=1.0)\n",
    "x = obs_select.time.astype(float).values #np.arange(float(obs.time.values[:11].max()))\n",
    "y = np.zeros_like(x)\n",
    "names = [\"obs_{0}\".format(xx) for xx in x]\n",
    "cov = v.covariance_matrix(x,y,names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, scale the values in the generic covariance matrix to reflect the observation uncertainties and take a peek.\n",
    "\n",
    "And there you have it, off-diagonal elements are no longer zero and show greater correlation between uncertainties of observation which happen closer together (it looks a bit wonky because parameter names are not ordered acording to time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:53:18.416052Z",
     "iopub.status.busy": "2022-04-11T03:53:18.415055Z",
     "iopub.status.idle": "2022-04-11T03:53:18.548083Z",
     "shell.execute_reply": "2022-04-11T03:53:18.548083Z"
    }
   },
   "outputs": [],
   "source": [
    "x_group = cov.x.copy()\n",
    "w = obs_select.weight.mean()\n",
    "v = (1./w)**2\n",
    "x_group *= v\n",
    "df.loc[obs_select.obsnme,obs_select.obsnme] = x_group\n",
    "\n",
    "plt.imshow(df.loc[obs_select.obsnme,obs_select.obsnme].values);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement to that to loop over all the non-zero time series observation groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:53:18.558056Z",
     "iopub.status.busy": "2022-04-11T03:53:18.557059Z",
     "iopub.status.idle": "2022-04-11T03:53:18.588577Z",
     "shell.execute_reply": "2022-04-11T03:53:18.587580Z"
    }
   },
   "outputs": [],
   "source": [
    "for obgnme in pst.nnz_obs_groups:\n",
    "    obs_select = obs.loc[(obs.obgnme==obgnme) & (obs.weight>0)]\n",
    "\n",
    "    v = pyemu.geostats.ExpVario(a=30,contribution=1.0)\n",
    "    x = obs_select.time.astype(float).values #np.arange(float(obs.time.values[:11].max()))\n",
    "    y = np.zeros_like(x)\n",
    "    names = [\"obs_{0}\".format(xx) for xx in x]\n",
    "    cov = v.covariance_matrix(x,y,names=names)\n",
    "\n",
    "\n",
    "    x_group = cov.x.copy()\n",
    "    w = obs_select.weight.mean()\n",
    "    v = (1./w)**2\n",
    "    x_group *= v\n",
    "    df.loc[obs_select.obsnme,obs_select.obsnme] = x_group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then re-construct a `Cov` object and record it to an external file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T03:53:18.592594Z",
     "iopub.status.busy": "2022-04-11T03:53:18.592594Z",
     "iopub.status.idle": "2022-04-11T03:53:18.600601Z",
     "shell.execute_reply": "2022-04-11T03:53:18.599576Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_cov_tv = pyemu.Cov.from_dataframe(df)\n",
    "obs_cov_tv.to_coo(os.path.join(template_ws,\"obs_cov.jcb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you made it this far, good on you! Let's finish with some eye candy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = obs_cov_tv.to_pearson().x\n",
    "x[np.abs(x)<0.00001]= np.nan\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beautiful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
