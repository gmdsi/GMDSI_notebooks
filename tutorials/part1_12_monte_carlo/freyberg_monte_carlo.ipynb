{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Uncertainty Analysis - Monte Carlo\n",
    "\n",
    "As we've seen, First-Order-Second-Moment (FOSM) is quick and insightful.  But FOSM depends on an assumption that the relation between the model and the forecast uncertainty is linear.  But many times the world is nonlinear. Short cuts like FOSM need assumptions, but we can free ourselves by taking the brute-force approach. That is, define the parameters that are important, provide the prior uncertainty, sample those parameters many times, run the model many times, and then summarize the results.  \n",
    "\n",
    "On a more practical standpoint, the underlying theory for FOSM and why it results in shortcomings can be hard to explain to stakeholders.  Monte Carlo, however, is VERY straightforward, its large computational cost notwithstanding. \n",
    "\n",
    "Here's a flowchart from [Anderson et al. (2015)](https://www.sciencedirect.com/book/9780120581030/applied-groundwater-modeling):\n",
    "\n",
    "<img src=\"freyberg_monte_carlo_files/Fig10.14_MC_workflow.png\" style=\"float: center; width:700px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Current Tutorial\n",
    "\n",
    "In this notebook we will:\n",
    "1. Run Monte Carlo on the Freyberg model\n",
    "2. Look at parameter and forecast uncertainty \n",
    "3. Look at the effect of prior parameter uncertainty covariance\n",
    "4. Start thinking of the advantages and disadvantages of linear and nonlinear uncertainty methods\n",
    "\n",
    "This notebook is going to be computationally demanding and may tax your computer. So buckle up. \n",
    "\n",
    "\n",
    "### Admin\n",
    "\n",
    "First the usual admin of preparing folders and constructing the model and PEST datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "import shutil\n",
    "\n",
    "# sys.path.insert(0,os.path.join(\"..\", \"..\", \"dependencies\"))\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd\n",
    "\n",
    "plt.rcParams['font.size'] = 10\n",
    "pyemu.plot_utils.font =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder containing original model files\n",
    "org_d = os.path.join('..', '..', 'models', 'monthly_model_files_1lyr_newstress')\n",
    "# a dir to hold a copy of the org model files\n",
    "working_dir = os.path.join('freyberg_mf6')\n",
    "if os.path.exists(working_dir):\n",
    "    shutil.rmtree(working_dir)\n",
    "shutil.copytree(org_d,working_dir)\n",
    "# get executables\n",
    "hbd.prep_bins(working_dir)\n",
    "# get dependency folders\n",
    "hbd.prep_deps(working_dir)\n",
    "# run our convenience functions to prepare the PEST and model folder\n",
    "hbd.prep_pest(working_dir)\n",
    "# convenience function that builds a new control file with pilot point parameters for hk\n",
    "hbd.add_ppoints(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Monte Carlo uses lots and lots of forward runs so we don't want to make the mistake of burning the silicon for a PEST control file that is not right.  Here we make doubly sure that the control file has the recharge freed (not \"fixed' in the PEST control file).  \n",
    "\n",
    "### Load the `pst` control file\n",
    "\n",
    "Let's double-check what parameters we have in this version of the model using `pyemu` (you can just look in the PEST control file too).\n",
    "\n",
    "We have adjustable parameters that control SFR inflow rates, well pumping rates, hydraulic conductivity and recharge rates. Recall that by setting a parameter as \"fixed\" we are stating that we know it perfectly (should we though...?). Currently fixed parameters include porosity and future recharge.\n",
    "\n",
    "For the sake of this tutorial, and as we did in the \"sensitivity\" tutorials, let's set all the parameters free:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_name = \"freyberg_pp.pst\"\n",
    "# load the pst\n",
    "pst = pyemu.Pst(os.path.join(working_dir,pst_name))\n",
    "#update parameter data\n",
    "par = pst.parameter_data\n",
    "#update paramter transform\n",
    "par.loc[:, 'partrans'] = 'log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the model runs\n",
    "pst.control_data.noptmax=0\n",
    "# rewrite the control file\n",
    "pst.write(os.path.join(working_dir,pst_name))\n",
    "# run the model once\n",
    "pyemu.os_utils.run('pestpp-glm freyberg_pp.pst', cwd=working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(working_dir,'freyberg_pp.pst'))\n",
    "pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo\n",
    "\n",
    "In its simplest form, Monte Carlo boils down to: 1) \"draw\" many random samples of parameters from the prior probability distribution, 2) run the model, 3) look at the results.\n",
    "\n",
    "So how do we \"draw\", or sample, parameters? (Think \"draw\" as in \"drawing a card from a deck\"). We need to randomly sample parameter values from a range. This range is defined by the _prior parameter probability distribution_. As we did for FOSM, let's assume that the bounds in the parameter data section define the range of a Gaussian (or normal) distribution, and that the intial values define the mean. \n",
    "\n",
    "### The Prior\n",
    "\n",
    "We can use `pyemu` to sample parameter values from such a distribution. First, construct a covariance matrix from the parameter data in the `pst` control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_cov = pyemu.Cov.from_parameter_data(pst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we do there? We generated a covariance matrix from the parameter bound values. The next cell displays an image of the matrix. As you can see, all off-diagonal values are zero. Therefore no parameter correlation is accounted for. \n",
    "\n",
    "This covariance matrix is the _prior_ (before history matching) parameter covariance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = prior_cov.as_2d.copy()\n",
    "x[x==0] = np.nan\n",
    "plt.imshow(x)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's sample 250 sets of parameter values from the probability distribution described by this covariance matrix and the mean values (e.g. the initial parameter values in the `pst` control file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parensemble = pyemu.ParameterEnsemble.from_gaussian_draw(pst=pst,cov=prior_cov,\n",
    "                                                         num_reals=250)\n",
    "# ensure that the samples respect parameter bounds in the pst control file\n",
    "parensemble.enforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of the first 5 parameter sets of our 250 created by our draw (\"draw\" here is like \"drawing\" a card)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parensemble.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this look like in terms of spatially varying parameters? Let's just plot the hydraulic conductivity from one of these samples. (Note the log-transformed values of K):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "parnmes = par.loc[par.pargp=='hk1'].parnme.values\n",
    "pe_k = parensemble.loc[:,parnmes].copy()\n",
    "\n",
    "# use the hbd convenienc function to plot several realisations\n",
    "hbd.plot_ensemble_arr(pe_k, working_dir, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That does not look very realistic. Do these look \"right\" (from a geologic stand point)? Lots of \"random\" variation (pilot points spatially near each other can have very different values)...not much structure...why? Because we have not specified any parameter correlation. Each pilot point is statistically independent. \n",
    "\n",
    "How do we express that in the prior? We need to express parameter spatial covariance with a geostatistical structure. Much the same as we did for regularization. Let's build a covariance matrix for pilot point parameters.\n",
    "\n",
    "You should be familiar with these by now:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(contribution=1.0,a=2500,anisotropy=1.0,bearing=0.0)\n",
    "gs = pyemu.utils.geostats.GeoStruct(variograms=[v])\n",
    "pp_tpl = os.path.join(working_dir,\"hkpp.dat.tpl\")\n",
    "cov = pyemu.helpers.geostatistical_prior_builder(pst=pst, struct_dict={gs:pp_tpl})\n",
    "# display\n",
    "plt.imshow(cov.to_pearson().x,interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "cov.to_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now resample using the geostatistically informed prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parensemble = pyemu.ParameterEnsemble.from_gaussian_draw(pst=pst, cov=cov, num_reals=250,)\n",
    "# ensure that the samples respect parameter bounds in the pst control file\n",
    "parensemble.enforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of the first 5 parameter sets of our 250 created by our draw (\"draw\" here is like \"drawing\" a card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parensemble.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we plot the spatially distributed parameters (`hk1`) we can see some structure and points which are near to each other are more likely to be similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_k = parensemble.loc[:,parnmes].copy()\n",
    "# use the hbd convenience function to plot several realisations\n",
    "hbd.plot_ensemble_arr(pe_k, working_dir, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the distributions. Note that distributions are lognormal, because parameters in the `pst` are log-transformed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pname in pst.par_names[:5]:\n",
    "    ax = parensemble.loc[:,pname].hist(bins=20)\n",
    "    print(parensemble.loc[:,pname].min(),parensemble.loc[:,pname].max(),parensemble.loc[:,pname].mean())\n",
    "    ax.set_title(pname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything funny? Compare these distributions to the uper/lower bounds in the `pst.parameter_data`. There seem to be many parameters \"bunched up\" at the bounds. This is due to the gaussian distribution being truncated at the parameter bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Monte Carlo\n",
    "\n",
    "So we now have 250 different random samples of parameters. Let's run them! This is going to take some time, so let's do it in parallel using [PEST++SWP](https://github.com/usgs/pestpp/blob/master/documentation/pestpp_users_manual.md#10-pestpp-swp).\n",
    "\n",
    "First write the ensemble to an external CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parensemble.to_csv(os.path.join(working_dir,\"sweep_in.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, make sure to specify the number of agents to use. This value must be assigned according to the capacity of your machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the master directory\n",
    "m_d='master_mc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to call PEST++SWP. It'll take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(working_dir, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-swp', #the PEST software version we want to run\n",
    "                            pst_name, # the control file to use with PEST\n",
    "                            num_workers=num_workers, #how many agents to deploy\n",
    "                            worker_root='.', #where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, #the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright - let's see some MC results.  For these runs, what was the Phi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.read_csv(os.path.join(m_d,\"sweep_out.csv\"),index_col=0)\n",
    "df_out = df_out.loc[df_out.failed_flag==0,:] #drop any failed runs\n",
    "df_out = df_out.loc[~df_out.le(-2.e27).any(axis=1)] #drop extreme values\n",
    "df_out.columns = [c.lower() for c in df_out.columns]\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, some are pretty large. What was Phi with the initial parameter values??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot Phi for all 250 runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.phi.hist(bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, some of those models are really REALLY bad fits to the observations.  So, when we only use our prior knowledge to sample the parameters we get a bunch of models with unacceptable Phi, and we can consider them not reasonable.  Therefore, we should NOT include them in our uncertainty analysis.\n",
    "\n",
    "\n",
    "## Conditioning (a.k.a GLUE)\n",
    "\n",
    "__IMPORTANT:__  this is super important - in this next block we are \"conditioning\" our Monte Carlo run by removing the bad runs based on a Phi cutoff. So here is where we choose which realizations we consider __good enough__ with respect to fitting the observation data. \n",
    "\n",
    "Those realizations with a phi bigger than our cutoff are out, no matter how close to the cutoff; those that are within the cutoff are all considered equally good regardless of how close to the cutoff they reside.\n",
    "\n",
    "We started out with a Phi of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say any Phi below 2 $\\times$ `pst.phi` is acceptable (which is pretty poor by the way...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_phi = pst.phi * 2\n",
    "good_enough = df_out.loc[df_out.phi<acceptable_phi].index.values\n",
    "print(\"number of good enough realisations:\", good_enough.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the run number of the 250 that met this cutoff.  Sheesh - that's very few!\n",
    "\n",
    "Here is a __major problem with \"rejection sampling\" in high dimensions__: you have to run the model many many many many many times to find even a few realizations that fit the data acceptably well.  \n",
    "\n",
    "With all these parameters, there are so many possible combinations, that very few realizations fit the data very well...we will address this problem later, so for now, let's bump our \"good enough\" threshold to some realizations to plot.\n",
    "\n",
    "Let's plot just these \"good\" ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_out.phi.hist(alpha=0.5)\n",
    "df_out.loc[good_enough,\"phi\"].hist(color=\"g\",alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the payoff - using our good runs, let's make some probabilistic plots!  Here's our parameters\n",
    "\n",
    "Gray blocks show the full the range of the realizations. These are within the bounds of what seemed reasonable given what we knew when we started, so those grey boxes represent our prior.\n",
    "\n",
    "The blue boxes show the runs that met our criteria, so that distribution represents our posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parnme in pst.par_names[:5]:\n",
    "    ax = plt.subplot(111)\n",
    "    parensemble.loc[:,parnme].hist(bins=10,alpha=0.5,color=\"0.5\",ax=ax,)\n",
    "    parensemble.loc[good_enough,parnme].hist(bins=10,alpha=0.5,color=\"b\",ax=ax,)   \n",
    "    ax.set_title(parnme)\n",
    "    plt.ylabel('count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the FOSM results, the future recharge (`rch1`) and porosity (`ne1`) are not influenced by calibration. The conditioned parameter values should cover the same range as unconditioned values. \n",
    "\n",
    "## Let's look at the forecasts\n",
    "\n",
    "In the plots below, prior forecast distributions are shaded grey, posteriors are graded blue and the \"true\" value is shown as dashed black line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in pst.forecast_names:\n",
    "    ax = plt.subplot(111)\n",
    "    df_out.loc[:,forecast].hist(bins=10,alpha=0.5,color=\"0.5\",ax=ax)\n",
    "    df_out.loc[good_enough,forecast].hist(bins=10,alpha=0.5,color=\"b\",ax=ax)\n",
    "    v = pst.observation_data.loc[forecast,\"obsval\"]\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.plot([v,v],ylim,\"k--\",lw=2.0)\n",
    "    ax.set_ylabel('count')\n",
    "    ax.set_title(forecast)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see for some of the forecasts that the prior and posterior are similar - indicating that \"calibration\" hasn't helped reduce forecast uncertainty. That being said, given the issues noted above for high-dimensional conditioned-MC, the very very few realisations we are using here to assess the posterior make it a bit dubious.\n",
    "\n",
    "And, as you can see, some of the true forecast values are not being covered by the posterior. So - failing.\n",
    "\n",
    "It's hard to say how the posterior compares to the prior with so few \"good enough\" realizations.  To fix this problem, we have two choices:\n",
    " - run the model more times for Monte Carlo (!)\n",
    " - generate realizations that fit the data better beforehand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Monte Carlo - sampling from the linearized posterior\n",
    "\n",
    "In the previous section, we saw that none of the realizations fit the observations anywhere close to ``phimlim`` because of the dimensionality of the pilot point problem.  \n",
    "\n",
    "Here, we will use some linear algebra trickeration to \"precondition\" the realizations so that they have a better chance of fitting the observations. As we all know now, \"linear algebra\" = Jacobian!\n",
    "\n",
    "First we need to run the calibration process to get the calibrated parameters and last Jacobian. Let's do that quick-sticks now. The next cell repeats what we did during the \"freyberg regularization\" tutorial. It can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbd.prep_mc(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_name = \"freyberg_reg.pst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = 'master_amc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(working_dir, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-glm', # the PEST software version we want to run\n",
    "                            pst_name, # the control file to use with PEST\n",
    "                            num_workers=num_workers, #how many agents to deploy\n",
    "                            worker_root='.', # where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, # the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d, pst_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ```schur``` bayesian monte carlo\n",
    "\n",
    "Here, we will swap out the prior parameter covariance matrix ($\\boldsymbol{\\Sigma}_{\\theta}$) for the FOSM-based posterior parameter covariance matrix ($\\overline{\\boldsymbol{\\Sigma}}_{\\theta}$).  Everything else is exactly the same (sounds like a NIN song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyemu.Schur(jco=os.path.join(m_d,pst_name.replace(\".pst\",\".jcb\")),\n",
    "                pst=pst,\n",
    "                parcov=cov) # geostatistically informed covariance matrix we built earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the control file with the \"best-fit\" parameters (e.g. the calibrated parameters). These values are the mean of the posterior parameter probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pst.parrep(os.path.join(m_d,pst_name.replace(\".pst\",\".parb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parensemble = pyemu.ParameterEnsemble.from_gaussian_draw(pst=sc.pst,\n",
    "                                                        cov=sc.posterior_parameter, # posterior parameter covariance\n",
    "                                                        num_reals=250,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parensemble.to_csv(os.path.join(working_dir,\"sweep_in.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(working_dir, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-swp', # the PEST software version we want to run\n",
    "                            pst_name, # the control file to use with PEST\n",
    "                            num_workers=num_workers, # how many agents to deploy\n",
    "                            worker_root='.', # where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, # the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.read_csv(os.path.join(m_d,\"sweep_out.csv\"),index_col=0)\n",
    "df_out = df_out.loc[df_out.failed_flag==0,:] #drop any failed runs\n",
    "#df_out = df_out.loc[~df_out.le(-2.e27).any(axis=1)] #drop extreme values\n",
    "df_out.columns = [c.lower() for c in df_out.columns]\n",
    "df_out.phi = df_out.meas_phi\n",
    "df_out.regul_phi = 0\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.phi.hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see, using the same Phi threshold we obtain a larger number of \"good enough\" parameter sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_enough = df_out.loc[df_out.meas_phi<acceptable_phi].index.values\n",
    "print(\"number of good enough realisations:\", good_enough.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_out.phi.hist(alpha=0.5)\n",
    "df_out.loc[good_enough,\"phi\"].hist(color=\"g\",alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, even so we are failing to entirely capture the truth values of all forecasts (see \"part_time\" forecast). So our forecast posterior is overly narrow (nonconservative). \n",
    "\n",
    "Why? Because our uncertainty analysis is __not robust__ for all our forecasts, even with nonlinear Monte Carlo. The parameterization scheme is still too coarse. ...segue to high-dimension PEST setup in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in pst.forecast_names:\n",
    "    ax = plt.subplot(111)\n",
    "    df_out.loc[:,forecast].hist(bins=10,alpha=0.5,color=\"0.5\",ax=ax)\n",
    "    df_out.loc[good_enough,forecast].hist(bins=10,alpha=0.5,color=\"b\",ax=ax)\n",
    "    v = pst.observation_data.loc[forecast,\"obsval\"]\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.plot([v,v],ylim,\"k--\",lw=2.0)\n",
    "    ax.set_ylabel('count')\n",
    "    ax.set_title(forecast)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
