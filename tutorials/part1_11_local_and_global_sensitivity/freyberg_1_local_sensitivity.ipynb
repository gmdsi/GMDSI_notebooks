{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Parameter Sensitivity and Identifiability\n",
    "\n",
    "Sensitivity analysis makes use of a Jacobian matrix to determine statistical insights into a model. We have already discussed the Jacobian matrix in a few places. It is calculated by perturbing the parameter (usually by 1%) and tracking what happens to the simulated values associated with each observation.\n",
    "\n",
    "$$\n",
    "\\textrm{sensitivity coefficient}_{ij} = \\frac{\\Delta \\textrm{ simulation}_i}{\\Delta \\textrm{ parameter}_j}\n",
    "$$\n",
    "\n",
    "This is key for derivative-based parameter estimation because, as we've seen, it allows us to efficiently compute upgraded parameters to try during the lambda search.  But the Jacobian matrix can give us insight about the model in and of itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Sensitivty\n",
    "\n",
    "Recall that a Jacobian matrix stores simulation-to-parameter sensitivities. For each parameter-observation combination, we can see how much the associated simulated value changes due to a small change in the parameter. If $y$ are the simulated values and $p$ are the parameters, the equation for the $i^{th}$ observation with respect to the $j^{th}$ parameter is:  \n",
    "$$\\frac{\\partial y_i}{\\partial p_j}$$\n",
    "This can be approximated by finite differences as :  \n",
    "$$\\frac{\\partial y_i}{\\partial p_j}\\approx\\frac{y\\left(p_j+\\Delta p_j \\right)-y\\left(p_j\\right)}{\\Delta p_j}$$\n",
    "\n",
    "___Insensitive parameters___ (i.e. parameters which are not informed by calibration) are defined as those which have sensitivity coefficients larger than a modeller specified value. (Note: this is subjective! In practice, insensitive parameters are usually defined has having a sensitivity coefficient two orders of magnitude lower than the most sensitive parameter.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Identifiability\n",
    "Sensitivity analyses can unmask other artifacts that affect calibration and uncertainty. A primary issue is correlation between parameters.  For example, we saw that in a heads-only calibration we can't estimate both recharge and hydraulic conductivity independently - the parameters are correlated so that an increase in one can be offset with an increase in the other.  To address this shortcoming, Doherty and Hunt (2009) show that singular value decomposition can extend the sensitivity insight into __*parameter identifiability*__.  Parameter identifiability combines parameter insensitivity and correlation information, and reflects the robustness with which particular parameter values in a model might be calibrated. That is, an identifiable parameter is both sensitive and relatively uncorrelated and thus is more likely to be estimated (identified) than an insensitive and/or correlated parameter. \n",
    "\n",
    "Parameter identifiability is considered a \"linear method\" in that it assumes the Jacobian matrix sensitivities hold over a range of reasonable parameter values.  It is able to address parameter correlation through singular value decomposition (SVD), exactly as we've seen earlier in this course. Parameter identifiability ranges from 0 (perfectly unidentifiable with the observations available) to 1.0 (fully identifiable). So, we typically plot identifiability using a stacked bar chart which is comprised of the included singular value contributions. Another way to think of it: if a parameter is strongly in the SVD solution space (low singular value above the cutoff) it will have a higher identifiability. However, as Doherty and Hunt (2009) point out, identifiability is qualitative in nature because the singular value cutoff is user-specified.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "As has been mentioned a couple of times now, sensitivity and identifiability assumes that the relation between parameter and observation changes are linear. These sensitivities are tested for a single parameter set (i.e. the parameter values listed in the PEST control file). This parameter set can be before or after calibration (or anywhere in between). The underlying assumption is that the linear relation holds. \n",
    "\n",
    "In practice, this is rarely the case. Thus, sensitivities obtained during derivative-based parameter estimation are __local__. It only holds up in the vicinity of the current parameters. That being said, it is a quick and computationally efficient method to gain insight into the model and links between parameters and simulated outputs. \n",
    "\n",
    "An alternative is to employ _global_ sensitivity analysis methods, which we introduce in a subsequent notebook.\n",
    "\n",
    "> __References:__\n",
    ">\n",
    "> - Doherty, John, and Randall J. Hunt. 2009. “Two Statistics for Evaluating Parameter Identifiability and Error Reduction.” Journal of Hydrology 366 (1–4): 119–27. doi:10.1016/j.jhydrol.2008.12.018.\n",
    "> - Anderson, Mary P., William W. Woessner, and Randall J. Hunt. 2015. Applied Groundwater Modeling: Simulation of Flow and Advective Transport. Applied Groundwater Modeling. 2nd ed. Elsevier. https://linkinghub.elsevier.com/retrieve/pii/B9780080916385000018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "We are going to pick up where the \"Freyberg pilot points\" tutorials left off. The cells bellow prepare the model and PEST files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "import shutil\n",
    "\n",
    "# sys.path.insert(0,os.path.join(\"..\", \"..\", \"dependencies\"))\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd\n",
    "\n",
    "plt.rcParams['font.size'] = 10\n",
    "pyemu.plot_utils.font =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder containing original model files\n",
    "org_d = os.path.join('..', '..', 'models', 'monthly_model_files_1lyr_newstress')\n",
    "# a dir to hold a copy of the org model files\n",
    "working_dir = os.path.join('freyberg_mf6')\n",
    "if os.path.exists(working_dir):\n",
    "    shutil.rmtree(working_dir)\n",
    "shutil.copytree(org_d,working_dir)\n",
    "# get executables\n",
    "hbd.prep_bins(working_dir)\n",
    "# get dependency folders\n",
    "hbd.prep_deps(working_dir)\n",
    "# run our convenience functions to prepare the PEST and model folder\n",
    "hbd.prep_pest(working_dir)\n",
    "# convenience function that builds a new control file with pilot point parameters for hk\n",
    "hbd.add_ppoints(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the `pst` control file\n",
    "\n",
    "Let's double check what parameters we have in this version of the model using `pyemu` (you can just look in the PEST control file too.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_name = \"freyberg_pp.pst\"\n",
    "# load the pst\n",
    "pst = pyemu.Pst(os.path.join(working_dir,pst_name))\n",
    "# what parameter groups?\n",
    "pst.par_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are adjustable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what adjustable parameter groups?\n",
    "pst.adj_par_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have adjustable parameters that control SFR inflow rates, well pumping rates, hydraulic conductivity and recharge rates. Recall that by setting a parameter as \"fixed\" we are stating that we know it perfectly (should we though...?). Currently fixed parameters include porosity and future recharge.\n",
    "\n",
    "For the sake of this tutorial, let's set all the parameters free:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "#update paramter transform\n",
    "par.loc[:, 'partrans'] = 'log'\n",
    "#check adjustable parameter groups again\n",
    "pst.adj_par_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Jacobian\n",
    "\n",
    "First Let's calculate a single Jacobian by changing the NOPTMAX = -2.  This will need npar+1 runs. The Jacobian matrix we get is the local-scale sensitivity information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change noptmax\n",
    "pst.control_data.noptmax = -2\n",
    "# rewrite the control file!\n",
    "pst.write(os.path.join(working_dir,pst_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 10\n",
    "m_d = 'master_local'\n",
    "pyemu.os_utils.start_workers(working_dir, # the folder which contains the \"template\" PEST dataset\n",
    "                            'pestpp-glm', # the PEST software version we want to run\n",
    "                            pst_name, # the control file to use with PEST\n",
    "                            num_workers=num_workers, # how many agents to deploy\n",
    "                            worker_root='.', # where to deploy the agent directories; relative to where python is running\n",
    "                            master_dir=m_d, # the manager directory\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity\n",
    "\n",
    "Okay, let's examine the *local sensitivities* by looking at the local gradients of parameters with respect to observations (the Jacobian matrix from the PEST++ NOPTMAX = -2 run)\n",
    "\n",
    "We'll use `pyemu` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the Jacobian matrix\n",
    "jco = pyemu.Jco.from_binary(os.path.join(m_d,pst_name.replace(\".pst\",\".jcb\")))\n",
    "jco_df = jco.to_dataframe()\n",
    "# inspect the matrix as a dataframe\n",
    "jco_df = jco_df.loc[pst.nnz_obs_names,:]\n",
    "jco_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some parameters (e.g. `rch0`) have a large effect on the observations used for calibration.  The future recharge (`rch1`) has no effect on the calibration observations, but that makes sense as none of the calibration observations are in that future stress period!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about Composite Scaled Sensitivities\n",
    "As can be seen above, parameter sensitivity for any given parameter is split among all the observations in the Jacobian matrix, but the parameter sensitivity that is most important for parameter estimation is the *total* parameter sensitivity, which reflects contributions from all the observations.  \n",
    "\n",
    "How to sum the individual sensitivities in the Jacobian matrix in the most meaningful way?  In the traditional, overdetermined regression world, CSS was a popular metric. CSS is Composite Scaled Sensitivitity. It sums the observation *weighted* sensitivity to report a single number for each parameter.\n",
    "\n",
    "In the Hill and Tiedeman (2007) textbook _Effective Groundwater Model Calibration: With Analysis of Data, Sensitivities, Predictions, and Uncertainty_ this is calculated as: \n",
    "$${css_{j}=\\sqrt{\\left(\\sum_{i=1}^{ND}\\left(\\frac{\\partial y'_{i}}{\\partial b_{j}}\\right)\\left|b_{j}\\right|\\sqrt{w_{ii}}\\right)/ND}}$$\n",
    "\n",
    "where $y$ are observations, $b$ are parameters, $w$ are weights, and $ND$ is the number of observations \n",
    "\n",
    "In PEST and PEST++, it is calculated slightly differently in that scaling by the parameter values happens automatically when the parameter is subjected to a log-transform (and we can see above that all our parameters are logged). This is due to a correction that must be made in calculating the Jacobian matrix and follows from the chain rule of derivatives.  Seems somewhat academic, but let's compare the two.\n",
    "\n",
    "Let's instantiate a `Schur` object (see the \"intro to fosm\" tutorials) to calculate the sensitivities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate schur\n",
    "sc = pyemu.Schur(jco=os.path.join(m_d,pst_name.replace(\".pst\",\".jcb\")))\n",
    "\n",
    "# calcualte the parameter CSS\n",
    "css_df = sc.get_par_css_dataframe()\n",
    "css_df.sort_values(by='pest_css', ascending=False).plot(kind='bar', figsize=(13,3))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the relative ranks of the `hk` parameters agree between the two...but not so for the `rchpp` pilot point parameters, nor the `rch0` parameter. According to `pest_css` the `rch0` is the most sensitive. Not so for the `hill_css`.  Why might this be?\n",
    "\n",
    "> hint: what is the initial value of `rch0`?  What is the log of that initial value?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Okay, let's look at just the PEST CSS and rank/plot it:\n",
    "\n",
    "What does this show us? It shows which parameters have the greatest effect on the _objective function_. In other words,  sensitive parameters are those which are informed by observation data. They will be affected by calibration.\n",
    "\n",
    "Parameters which are _insensitive_ (e.g. `rch1` and `ne1`), are not affected by calibration. Why? Because we have no observation data that affects them. Makes sense: `rch1` is recharge in the future, `ne1` is not informed by head or flow data - the only observations we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "ax = css_df['pest_css'].sort_values(ascending=False).plot(kind='bar')\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So how do these parameter sensitivities affect the forecasts?  \n",
    "\n",
    "Recall that the sensitivity is calculated by differencing the two model outputs, so any model output can have a sensitivity calculated even if we don't have a measured value.  So, because we included the forecasts as observations we have sensitivities for them in our Jacobian matrix.  Let's use `pyemu` to pull just these forecasts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jco_fore_df = sc.forecasts.to_dataframe()\n",
    "jco_fore_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that porosity is 0.0, except for the travel time forecast (`part_time`), which makes sense.  \n",
    "\n",
    "Perhaps less obvious is `rch0` - why does it have sensitivity when all the forecasts are in the period that has `rch1` recharge? Well what happened in the past will affect the future..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider posterior covariance and parameter correlation\n",
    "\n",
    "Again, use `pyemu` to construct a posterior parameter covariance matrix (note that this is not quite the same thing as \"posterior parameter uncertainty\"!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = pyemu.Cov(sc.xtqx.x, names=sc.xtqx.row_names)\n",
    "covar.df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For covariance, very small numbers reflect that the parameter doesn't covary with another.  (Does it make sense that `rch1` does not covary with other parameters?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### We can visualize the correlation between the two parameters using a correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = covar.to_pearson()\n",
    "plt.imshow(R.df(), interpolation='nearest', cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the parameters are correlated perfectly to themselves (1.0 along the yellow diagonal) but they also can have appreciable correlation to each other, both positively and negatively.\n",
    "\n",
    "#### Inspect correlation for a single parameter\n",
    "\n",
    "Using pilot point `hk_i:12_j:12_zone:1`, let's look only at the parameters that have correlation > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpar = 'hk_i:12_j:12_zone:1'\n",
    "R.df().loc[cpar][np.abs(R.df().loc[cpar])>.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saying parameters are correlated is really saying that when a parameter changes it has a similar effect on the observations as the other parameter(s). So in this case that means that when `hk_i:12_j:12_zone:1.0` increases it has a similar effect on observations as increasing `hk_i:2_j:12_zone:1.0`.  If we add a new observation type (or less powerfully, an observation at a new location) we can break the correlation.  And we've seen this:  adding a flux observation broke the correlation between R and K!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this `pyemu` picture to interrogate the correlation - here we say plot this but cut out all the correlations under 0.5.  Play with this by putting other numbers between 0.3 and 1.0 and re-run the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_plot = R.as_2d.copy()\n",
    "R_plot[np.abs(R_plot)<0.5] = np.nan\n",
    "plt.imshow(R_plot, interpolation='nearest', cmap='viridis')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, correlation >0.95 or so becomes a problem for obtaining a unique solution to the parameter estimation problem. (A problem which can be avoided with regularization.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifiability\n",
    "\n",
    "Parameter identifiability combines parameter insensitivity and correlation information, and reflects the robustness with which particular parameter values in a model might be calibrated. That is, an identifiable parameter is both sensitive and relatively uncorrelated and thus is more likely to be estimated (identified) than an insensitive and/or correlated parameter. \n",
    "\n",
    "One last cool concept about identifiability that Doherty and Hunt (2009) point out: Because parameter identifiability uses the Jacobian matrix it is the *sensitivity* that matters, not the actual value specified. This means you can enter *hypothetical observations* to the existing observations, rerun the Jacobian matrix, and then re-plot identifiability. In this way identifiability becomes a quick but qualitative way to look at the worth of future data collection - an underused aspect of our modelling!   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at identifiability we will need to create an `ErrVar` object in `pyemu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = pyemu.ErrVar(jco=os.path.join(m_d,pst_name.replace(\".pst\",\".jcb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a dataframe of identifiability for any singular value cutoff (`singular_value` in the cell below). (Recall that the minimum number of singular values will be the number of nonzero observations; see the SVD regularization tutorial.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nnz_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try playing around with `singular_value` in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_value= 37\n",
    "\n",
    "id_df = ev.get_identifiability_dataframe(singular_value=singular_value).sort_values(by='ident', ascending=False)\n",
    "id_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to  visualize parameter _identifiability_  as stacked bar charts. Here we are looking at the identifiability with `singular_value` number of singular vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pyemu.plot_utils.plot_id_bar(id_df, figsize=(14,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it can be more meaningful to look at a singular value cutoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pyemu.plot_utils.plot_id_bar(id_df, nsv=10, figsize=(14,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Spatially\n",
    "\n",
    "It can be useful to display sensitivities or identifiabilities spatially. Pragmatically this can be acomplished by assigning sensitivity/identifiability values to pilot points (as if they were parameter values) and then interpolating to the model grid.\n",
    "\n",
    "The next cells do this in the background for the `hk1` pilot parameter identifiability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the identifiability values of the hk1 pilot points\n",
    "sorted_pnames = hbd.get_sorted_ppoint_names(par, working_dir)\n",
    "ident_vals = id_df.loc[sorted_pnames, 'ident'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the convenience function to interpolate to and then plot on the model grid\n",
    "hbd.plot_arr2grid(ident_vals, working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for sensitivity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the same for sensitivity CSS\n",
    "css_hk = css_df.loc[ sorted_pnames, 'pest_css'].values\n",
    "# use the conveninec function to interpolate to and then plot on the model grid\n",
    "hbd.plot_arr2grid(css_hk, working_dir, title='Sensitivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what?\n",
    "\n",
    "So what is this useful for? Well if we have a very long-running model and many adjustable parameters - _and if we really need to use derivative-based parameter estimation methods (i.e. PEST++GLM)_ - we could now identify parameters that can be fixed and/or omitted during parameter estimation (but not uncertainty analysis!). \n",
    "\n",
    "Looking at forecast-to-parameter sensitivities can also inform us of which parameters matter for our forecast. If a parameter doesn't affect a forecast, then we are less concerned with it. If it does...then we may wish to give it more attention. These methods provide useful tools for assessing details of model construction and their impact on decision-support forecasts of interest (e.g. assessing whether a boundary condition affects a forecast).\n",
    "\n",
    "\n",
    "But! As has been mentioned - these methods are _local_ and assume a linear relation between parameter changes and model output. As we have seen, this is usually not the case. A more robust measure of parameter sensitivity requires the use of global sensitivity analysis methods, discussed in the next tutorial."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
