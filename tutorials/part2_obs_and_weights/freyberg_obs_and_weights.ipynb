{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulating the Objective Function and The Dark Art of Weighting\n",
    "\n",
    "The objective function expresses model-to-measurement misfit for use in the solution of an inverse problem through the *weighted squared difference between measured and simulated observations*. There is no formula for a universally \"best approach\" to formulate an objective function. However, the universal underlying principle is to ensure that as much information as possible is transferred to the parameters of a model, in as \"safe\" a way as possible (Doherty, 2015). \n",
    "\n",
    "What do we mean by \"safe\"? We mean that, in attempting to fit model outputs to measured observations, we should minimize the potential for model structural error to induce model parameters to take on values that compensate for model imperfections. When estimated parameters take on compensating roles, the potential for error in predictions may actually increase, rather than decrease, during the process of calibration (White et al. 2014). And we don't want that!\n",
    "\n",
    "**Observation Types**\n",
    "\n",
    "In most history matching contexts a “multicomponent” objective function is recommended. Each component of this objective function is calculated on the basis of different groups of observations or of the same group of observations processed in different ways. In a nut-shell, this means as many (useful!) types of observation as are available should be included in the parameter-estimation process. This does not **just** mean different \"measurement types\"! It also means teasing out components *within* a given measurement type. These \"secondary\" components often contain information that is otherwise lost or overshadowed. \n",
    "\n",
    "**Grouping** \n",
    "\n",
    "Using a multicomponent approach can extract as much information from an observation dataset as possible and transfer this information to estimated parameters. When constructing a PEST dataset, it is often usefull (and convenient) to group observations by type. This makes it easier to customize objective function design and track the flow of infromation from data to parameters (and subsequently to predictions). Ideally, each observation grouping should illuminate and constrain the parameter estimation related to a separate aspect of the system beiong modelled (Doherty and Hunt, 2010). For example, absolute values of heads may inform parameters that control horizontal flow patterns, whilst vertical diferences between heads in different aquifers may inform parameters that control vertical flow patterns. \n",
    "\n",
    "**Weighting**\n",
    "\n",
    "A user must decide how to weight observations before estimating parameters with PEST. In some cases, it is prudent to strictly weight observations based on the inverse of the standard deviation of measurement noise. Observations with higher credibility should, without a doubt, be given more weight than those with lower credibility. However, in many history-matching contexts, model defects are just as important as noise in inducing model-to-measurement misfit as field measurements. Some types of model outputs are more affected by model imperfections than others. Notably, the effects of imperfections on model output differences (whether spatial or temporal) are frequently less than their effects on raw model outputs.\n",
    "\n",
    "As a result, including measurement differences and corresponding model output differences in a calibration dataset can often improve a history-matching process. It can also be improved by dividing components of a measurement dataset into groups based on type and location, assigning these measurements to different observation groups, and ensuring that the contributions made by each of these groups to the original objective function are roughly the same. In the parameter estimation process, the information content of each of these groups is thus given equal weight. In other cases, accommodating the \"structural\" nature of model-to-measurement misfit is preferable (i.e. the model is inherently better at fitting some measurements than others). \n",
    "\n",
    "The PEST Book (Doherty, 2015) and the USGS published report \"A Guide to Using PEST for Groundwater-Model Calibration\" (Doherty et al 2010) go into detail on formulating an objective function and discuss common issues with certain data-types. \n",
    "\n",
    "**References and Recommended Reading:**\n",
    ">  - Doherty, J., (2015). Calibration and Uncertainty Analysis for Complex Environmental Models. Watermark Numerical Computing, Brisbane, Australia. ISBN: 978-0-9943786-0-6.\n",
    ">  - <div class=\"csl-entry\">White, J. T., Doherty, J. E., &#38; Hughes, J. D. (2014). Quantifying the predictive consequences of model error with linear subspace analysis. <i>Water Resources Research</i>, <i>50</i>(2), 1152–1173. https://doi.org/10.1002/2013WR014767</div>\n",
    ">  - <div class=\"csl-entry\">Doherty, J., &#38; Hunt, R. (2010). <i>Approaches to Highly Parameterized Inversion: A Guide to Using PEST for Groundwater-Model Calibration: U.S. Geological Survey Scientific Investigations Report 2010–5169</i>. https://doi.org/https://doi.org/10.3133/sir20105169</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Recap: the modified-Freyberg PEST dataset\n",
    "\n",
    "The modified Freyberg model is introduced in another tutorial notebook (see \"freyberg intro to model\"). The current notebook picks up following the \"freyberg psfrom pest setup\" notebook, in which a high-dimensional PEST dataset was constructed using `pyemu.PstFrom`. You may also wish to go through the \"intro to pyemu\" notebook beforehand.\n",
    "\n",
    "We will now address assigning observation target values from \"measured\" data and observation weighting prior to history matching. \n",
    "\n",
    "Recall from the \"freyberg pstfrom pest setup\" notebook that we included several observation types in the history matching dataset:\n",
    " - head time-series\n",
    " - river flux time-series\n",
    " - temporal differences between both heads and flux time-series\n",
    " - vertical head differences\n",
    "\n",
    "We also included many observations of model outputs for which we do not have measured data. We kept these to make it easier to keep track of model outputs (this becomes a necessity when working with ensembles). We also included observations of \"forecasts\", i.e. model outputs of managment interest.\n",
    "\n",
    "The next couple of cells load necessary dependencies and call a convenience function to prepare the PEST dataset folder for you. This is the same dataset that was constructed during the \"pstfrom pest setup\" tutorial. \n",
    "\n",
    "Simply press `shift+enter` to run the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:15.512355Z",
     "iopub.status.busy": "2022-04-21T02:20:15.512074Z",
     "iopub.status.idle": "2022-04-21T02:20:16.361195Z",
     "shell.execute_reply": "2022-04-21T02:20:16.360743Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pyemu\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt;\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# import pre-prepared convenience functions\n",
    "import herebedragons as hbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:16.364591Z",
     "iopub.status.busy": "2022-04-21T02:20:16.364417Z",
     "iopub.status.idle": "2022-04-21T02:20:16.366731Z",
     "shell.execute_reply": "2022-04-21T02:20:16.366288Z"
    }
   },
   "outputs": [],
   "source": [
    "# specify the temporary working folder\n",
    "t_d = os.path.join('freyberg6_template')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:16.369015Z",
     "iopub.status.busy": "2022-04-21T02:20:16.368879Z",
     "iopub.status.idle": "2022-04-21T02:20:16.770806Z",
     "shell.execute_reply": "2022-04-21T02:20:16.770000Z"
    }
   },
   "outputs": [],
   "source": [
    "# use the conveninece function to get the pre-preprepared PEST dataset;\n",
    "# this is the same dataset consutrcted in the \"pstfrom\" tutorial\n",
    "if os.path.exists(t_d):\n",
    "        shutil.rmtree(t_d)\n",
    "org_t_d = os.path.join(\"..\",\"part2_pstfrom_pest_setup\",t_d)\n",
    "if not os.path.exists(org_t_d):\n",
    "    hbd.dir_cleancopy(org_d=os.path.join('..','..', 'models','freyberg_pstfrom_pest_setup'), \n",
    "                    new_d=t_d)\n",
    "else:\n",
    "    print(\"using files at \",org_t_d)\n",
    "    shutil.copytree(org_t_d,t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the `Pst` control file we constructed during the \"pstfrom\" tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:16.774558Z",
     "iopub.status.busy": "2022-04-21T02:20:16.774390Z",
     "iopub.status.idle": "2022-04-21T02:20:17.024994Z",
     "shell.execute_reply": "2022-04-21T02:20:17.024515Z"
    }
   },
   "outputs": [],
   "source": [
    "pst_file = \"freyberg_mf6.pst\"\n",
    "\n",
    "pst = pyemu.Pst(os.path.join(t_d, pst_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we constructed the PEST dataset (in the \"pstfrom\" tutorial) we simply identified what odel outputs we wanted PEST to \"observe\". In doing so, `pyemu.PstFrom` assigned observation target values that it found in the existing model output files. (Which conveniently allowed us to test whether out PEST setup was working correctly). All observation weights were assigned a default value of 1.0. \n",
    "\n",
    "As a reminder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:17.027663Z",
     "iopub.status.busy": "2022-04-21T02:20:17.027495Z",
     "iopub.status.idle": "2022-04-21T02:20:17.039965Z",
     "shell.execute_reply": "2022-04-21T02:20:17.039533Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we need to do several things:\n",
    " - replace observation target values (the `obsval` column) with corresponding vallues from \"measured data\";\n",
    " - assign meaningfull weights to history matching target observations (the `weight` column);\n",
    " - assign zero weight to observations that should not affect history matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off with the basics. First set all weights to zero. We will then go through and assign meaningfull weights only to relevant target observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:17.042555Z",
     "iopub.status.busy": "2022-04-21T02:20:17.042389Z",
     "iopub.status.idle": "2022-04-21T02:20:17.046505Z",
     "shell.execute_reply": "2022-04-21T02:20:17.046116Z"
    }
   },
   "outputs": [],
   "source": [
    "#check for nonzero weights\n",
    "obs.weight.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:17.049470Z",
     "iopub.status.busy": "2022-04-21T02:20:17.049213Z",
     "iopub.status.idle": "2022-04-21T02:20:17.055966Z",
     "shell.execute_reply": "2022-04-21T02:20:17.055185Z"
    }
   },
   "outputs": [],
   "source": [
    "# assign all weight zero\n",
    "obs.loc[:, 'weight'] = 0\n",
    "\n",
    "# check for non zero weights\n",
    "obs.weight.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measured Data\n",
    "\n",
    "In most data assimilation contexts you will have some relevant measured data (e.g. water levels, river flow rates, etc.) which correspond to simulated model outputs. These will probably not coincide exactly with your model outputs. Are the wells at the same coordinate as the center of the model cell? Do measurement times line up nicely with model output times? Doubt it. And if they do, are single measurements that match model output times biased? And so on... \n",
    "\n",
    "A modeller needs to ensure that the observation values assigned in the PEST control file aligned with simulated model outputs. This will usually require some case-specific pre-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:17.060754Z",
     "iopub.status.busy": "2022-04-21T02:20:17.060548Z",
     "iopub.status.idle": "2022-04-21T02:20:17.071387Z",
     "shell.execute_reply": "2022-04-21T02:20:17.070965Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_data = pd.read_csv(os.path.join('..', '..', 'models', 'freyberg_mf6_truth',\"obs_data.csv\"))\n",
    "obs_data.set_index('site', inplace=True)\n",
    "obs_data.fillna(0, inplace=True)\n",
    "obs_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:17.073647Z",
     "iopub.status.busy": "2022-04-21T02:20:17.073490Z",
     "iopub.status.idle": "2022-04-21T02:20:17.082272Z",
     "shell.execute_reply": "2022-04-21T02:20:17.081823Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the measured  values\n",
    "meas_sfr = pd.read_csv(os.path.join('..', '..', 'models', 'freyberg_mf6_truth',\"sfr.csv\"),\n",
    "                    index_col=0)\n",
    "\n",
    "\n",
    "meas_sfr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:17.084901Z",
     "iopub.status.busy": "2022-04-21T02:20:17.084737Z",
     "iopub.status.idle": "2022-04-21T02:20:17.097355Z",
     "shell.execute_reply": "2022-04-21T02:20:17.096992Z"
    }
   },
   "outputs": [],
   "source": [
    "#just pick the nearest to the sp end\n",
    "\n",
    "\n",
    "model_times = meas_sfr.index\n",
    "obs_sites = ['GAGE-1','TRGW-0-26-6','TRGW-2-26-6','TRGW-0-3-8','TRGW-2-3-8']#list(set(obs_data.index.values)) #\n",
    "\n",
    "\n",
    "es_obs_data = []\n",
    "for site in obs_sites:\n",
    "    site_obs_data = obs_data.loc[site,:].copy()\n",
    "    #site_obs_data.fillna(0, inplace=True)\n",
    "    site_obs_data.loc[:,\"site\"] = site_obs_data.index.values\n",
    "    site_obs_data.index = site_obs_data.time\n",
    "\n",
    "    site_obs_data = site_obs_data.reindex(model_times,method=\"nearest\")\n",
    "\n",
    "    if site_obs_data.shape != site_obs_data.dropna().shape:\n",
    "        print(\"broke\",site)\n",
    "    es_obs_data.append(site_obs_data)\n",
    "es_obs_data = pd.concat(es_obs_data,axis=0,ignore_index=True)\n",
    "es_obs_data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:17.101384Z",
     "iopub.status.busy": "2022-04-21T02:20:17.100919Z",
     "iopub.status.idle": "2022-04-21T02:20:18.024050Z",
     "shell.execute_reply": "2022-04-21T02:20:18.023635Z"
    }
   },
   "outputs": [],
   "source": [
    "for site in obs_sites:\n",
    "    #print(site)\n",
    "    site_obs_data = obs_data.loc[site,:]\n",
    "    es_site_obs_data = es_obs_data.loc[es_obs_data.site==site,:].copy()\n",
    "    es_site_obs_data.sort_values(by=\"time\",inplace=True)\n",
    "    #print(site,site_obs_data.shape)\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    ax.plot(site_obs_data.time,site_obs_data.value,\"b-\",lw=0.5)\n",
    "    #ax.plot(es_site_obs_data.datetime,es_site_obs_data.value,'r-',lw=2)\n",
    "    ax.plot(es_site_obs_data.time,es_site_obs_data.value,'r-',lw=1,marker='.',ms=10)\n",
    "    ax.set_title(site)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:18.026398Z",
     "iopub.status.busy": "2022-04-21T02:20:18.026252Z",
     "iopub.status.idle": "2022-04-21T02:20:18.836743Z",
     "shell.execute_reply": "2022-04-21T02:20:18.836348Z"
    }
   },
   "outputs": [],
   "source": [
    "ess_obs_data = {}\n",
    "for site in obs_sites:\n",
    "    #print(site)\n",
    "    site_obs_data = obs_data.loc[site,:].copy()\n",
    "    site_obs_data.loc[:,\"site\"] = site_obs_data.index.values\n",
    "    site_obs_data.index = site_obs_data.time\n",
    "    sm = site_obs_data.value.rolling(window=20,center=True,min_periods=1).mean()\n",
    "    sm_site_obs_data = sm.reindex(model_times,method=\"nearest\")\n",
    "    #ess_obs_data.append(pd.DataFrame9sm_site_obs_data)\n",
    "    ess_obs_data[site] = sm_site_obs_data\n",
    "    \n",
    "    es_site_obs_data = es_obs_data.loc[es_obs_data.site==site,:].copy()\n",
    "    es_site_obs_data.sort_values(by=\"time\",inplace=True)\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    ax.plot(site_obs_data.time,site_obs_data.value,\"b-\",lw=0.25)\n",
    "    ax.plot(es_site_obs_data.time,es_site_obs_data.value,'r-',lw=1,marker='.',ms=10)\n",
    "    ax.plot(sm_site_obs_data.index,sm_site_obs_data.values,'g-',lw=0.5,marker='.',ms=10)\n",
    "    ax.set_title(site)\n",
    "plt.show()\n",
    "ess_obs_data = pd.DataFrame(ess_obs_data)\n",
    "ess_obs_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:18.839005Z",
     "iopub.status.busy": "2022-04-21T02:20:18.838862Z",
     "iopub.status.idle": "2022-04-21T02:20:18.845861Z",
     "shell.execute_reply": "2022-04-21T02:20:18.845492Z"
    }
   },
   "outputs": [],
   "source": [
    "ess_obs_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:18.848451Z",
     "iopub.status.busy": "2022-04-21T02:20:18.848031Z",
     "iopub.status.idle": "2022-04-21T02:20:19.237304Z",
     "shell.execute_reply": "2022-04-21T02:20:19.236955Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_names = obs.loc[obs.oname.isin(['hds', 'sfr']), 'obsnme'].values\n",
    "time_str = ess_obs_data.index.map(lambda x: f\"time:{x}\").values\n",
    "missing=[]\n",
    "for col in ess_obs_data.columns:\n",
    "    obs_sufix = col.lower()+\"_\"+time_str\n",
    "    for string,oval,time in zip(obs_sufix,ess_obs_data.loc[:,col].values,ess_obs_data.index.values):\n",
    "        \n",
    "        if not any(string in obsnme for obsnme in obs_names):\n",
    "            missing.append(string)\n",
    "        # if not, then update the pst.observation_data\n",
    "        else:\n",
    "            # get a list of obsnames\n",
    "            obsnme = [ks for ks in obs_names if string in ks] \n",
    "            assert len(obsnme) == 1,string\n",
    "            obsnme = obsnme[0]\n",
    "            #ovals = ess_obs_data.loc[:,col].values\n",
    "            #print(string,obsnme,oval,time)\n",
    "            # assign the obsvals\n",
    "            obs.loc[obsnme,\"obsval\"] = oval\n",
    "            # assign a generic weight\n",
    "            if time > 3652.5 and time <=4018.5:\n",
    "                obs.loc[obsnme,\"weight\"] = 1.0\n",
    "\n",
    "if len(missing)==0:\n",
    "    print('All good.')\n",
    "    print('Number of nonzero obs:' ,pst.nnz_obs)\n",
    "    \n",
    "else:\n",
    "    print('The following obs are missing:\\n',missing)\n",
    "\n",
    "print(pst.nnz_obs_groups)\n",
    "pst.observation_data.loc[pst.nnz_obs_names,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:19.239955Z",
     "iopub.status.busy": "2022-04-21T02:20:19.239586Z",
     "iopub.status.idle": "2022-04-21T02:20:19.393489Z",
     "shell.execute_reply": "2022-04-21T02:20:19.392876Z"
    }
   },
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d,pst_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:19.396411Z",
     "iopub.status.busy": "2022-04-21T02:20:19.396223Z",
     "iopub.status.idle": "2022-04-21T02:20:26.466218Z",
     "shell.execute_reply": "2022-04-21T02:20:26.465826Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pyemu.os_utils.run(\"pestpp-ies.exe {0}\".format(pst_file),cwd=t_d)\n",
    "pst = pst = pyemu.Pst(os.path.join(t_d, pst_file))\n",
    "pst.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.phi_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:26.468964Z",
     "iopub.status.busy": "2022-04-21T02:20:26.468819Z",
     "iopub.status.idle": "2022-04-21T02:20:27.590565Z",
     "shell.execute_reply": "2022-04-21T02:20:27.590098Z"
    }
   },
   "outputs": [],
   "source": [
    "phicomp = pd.Series(pst.phi_components)\n",
    "\n",
    "plt.pie(phicomp, labels=phicomp.index.values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:27.592804Z",
     "iopub.status.busy": "2022-04-21T02:20:27.592660Z",
     "iopub.status.idle": "2022-04-21T02:20:28.645109Z",
     "shell.execute_reply": "2022-04-21T02:20:28.644719Z"
    }
   },
   "outputs": [],
   "source": [
    "pst.phi_components\n",
    "\n",
    "{k:pst.phi_components[k] for k in pst.nnz_obs_groups }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:28.647687Z",
     "iopub.status.busy": "2022-04-21T02:20:28.647427Z",
     "iopub.status.idle": "2022-04-21T02:20:29.793766Z",
     "shell.execute_reply": "2022-04-21T02:20:29.792946Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#pst.plot(kind='phi_pie');\n",
    "print('Here are the non-zero weighted observation contributions to phi')\n",
    "\n",
    "figs = pst.plot(kind=\"1to1\");\n",
    "pst.res.loc[pst.nnz_obs_names,:]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:29.796177Z",
     "iopub.status.busy": "2022-04-21T02:20:29.796014Z",
     "iopub.status.idle": "2022-04-21T02:20:30.795419Z",
     "shell.execute_reply": "2022-04-21T02:20:30.794458Z"
    }
   },
   "outputs": [],
   "source": [
    "nz_obs = pst.observation_data.loc[pst.nnz_obs_names,:].copy()\n",
    "#nz_obs.loc[:,\"datetime\"] = pd.to_datetime(nz_obs.obsnme.apply(lambda x: x.split(\"_\")[-1]))    \n",
    "for nz_group in pst.nnz_obs_groups:\n",
    "    nz_obs_group = nz_obs.loc[nz_obs.obgnme==nz_group,:]\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    ax.plot(nz_obs_group.time,nz_obs_group.obsval,\"r-\",label=\"observed\")\n",
    "    ax.plot(nz_obs_group.time,pst.res.loc[nz_obs_group.obsnme,\"modelled\"],\"b-\",label=\"simulated\")\n",
    "    ax.set_title(nz_group)\n",
    "    ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:30.798282Z",
     "iopub.status.busy": "2022-04-21T02:20:30.798139Z",
     "iopub.status.idle": "2022-04-21T02:20:30.803937Z",
     "shell.execute_reply": "2022-04-21T02:20:30.803080Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the measured  values\n",
    "#meas_hds = pd.read_csv(os.path.join('..', '..', 'models', 'freyberg_mf6_truth',\"heads.csv\"),\n",
    "#                    index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:30.806755Z",
     "iopub.status.busy": "2022-04-21T02:20:30.806586Z",
     "iopub.status.idle": "2022-04-21T02:20:30.815874Z",
     "shell.execute_reply": "2022-04-21T02:20:30.815490Z"
    }
   },
   "outputs": [],
   "source": [
    "# load and save the truth hds observation values so that the secondary obs\n",
    "# process yields the \"measured\" values.\n",
    "#hds_meas = pd.read_csv(os.path.join('..', '..', 'models', 'freyberg_mf6_truth',\"heads.csv\"),\n",
    "#                    index_col=0)\n",
    "#hds_meas.to_csv(os.path.join(t_d,\"head.csv\"))\n",
    "\n",
    "#helpers.process_secondary_obs(ws=t_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:30.818186Z",
     "iopub.status.busy": "2022-04-21T02:20:30.818048Z",
     "iopub.status.idle": "2022-04-21T02:20:30.824240Z",
     "shell.execute_reply": "2022-04-21T02:20:30.823883Z"
    }
   },
   "outputs": [],
   "source": [
    "#shutil.copy(src=os.path.join('..', '..', 'models', 'freyberg_mf6_truth', 'freyberg_mp.mpend'),\n",
    "#            dst=os.path.join(template_ws, 'freyberg_mp.mpend')\n",
    "#            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Observation Uncertainty Covariance Matrix\n",
    "\n",
    "Lastly, not so common, let's also generate our observation covariance matrix. Often this step is omitted, in particular if the observation weights in the `* observation data` section reflect observation uncertainty. Here we will implement it expressly so as to:\n",
    "\n",
    " - make life easier for the authors to organize the tutorial notebooks;\n",
    " - because we are going to be fiddlying around with the observation weights later on, and this  way we keep a \"backup\" of our observation uncertainty;\n",
    " - and to demonstrate how to add autocorrelated transient noise! When is the last time you saw that in the wild?\n",
    "\n",
    "First let us start by generating a covariance matrix, using the weights recorded in the `pst.observation_data` section. This is accomplished with `pyemu.Cov.from_observation_data()`. This will generate a covariance matrix for **all** observations in the control file, even the zero-weighted ones. \n",
    "\n",
    "However, we are only interested in keep the non-zero weighted observations. We can use `pyemu` convenient Matrix manipulation functions to `.get()` the covariance matrix rows/columns that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:30.826402Z",
     "iopub.status.busy": "2022-04-21T02:20:30.826268Z",
     "iopub.status.idle": "2022-04-21T02:20:30.857062Z",
     "shell.execute_reply": "2022-04-21T02:20:30.856449Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "\n",
    "# generate cov matrix for all obs using weights in the control file\n",
    "obs_cov = pyemu.Cov.from_observation_data(pst, )\n",
    "\n",
    "# reduce cov down to only include non-zero obsverations\n",
    "obs_cov = obs_cov.get(row_names=pst.nnz_obs_names, col_names=pst.nnz_obs_names, )\n",
    "\n",
    "# side note: \n",
    "# going to save the diagonal (e.g no correlation) obsevration uncertainty cov matrix\n",
    "# to an external file for use in a later tutorial\n",
    "obs_cov.to_coo(os.path.join(t_d,\"obs_cov_diag.jcb\"))\n",
    "\n",
    "# make it a dataframe to make life easier\n",
    "df = obs_cov.to_dataframe()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this returned a diagonal covariance matrix (e.g. only values in the diagonal are non-zero). This implies that there is no covariance between observation noise. \n",
    "\n",
    "You can plot the matrix, but due to the scale it wont look particularily interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:30.859867Z",
     "iopub.status.busy": "2022-04-21T02:20:30.859677Z",
     "iopub.status.idle": "2022-04-21T02:20:30.865574Z",
     "shell.execute_reply": "2022-04-21T02:20:30.865105Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(df.values)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...you can kinda see something there (thats the sfr flow obs...). Let's instead just look at a small part, focusing on observations that make up a single time series. So as you can see below, the matrix diagonal reflects individual obsevation uncertianties, whilst all the off-diagonals are zero (e.g. no correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:30.868229Z",
     "iopub.status.busy": "2022-04-21T02:20:30.868053Z",
     "iopub.status.idle": "2022-04-21T02:20:30.875459Z",
     "shell.execute_reply": "2022-04-21T02:20:30.875046Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_select = obs.loc[(obs.obgnme=='oname:hds_otype:lst_usecol:trgw-0-26-6') & (obs.weight>0)]\n",
    "\n",
    "plt.imshow(df.loc[obs_select.obsnme,obs_select.obsnme].values)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But recall that most of the observations are time-series. Here we are saying \"how wrong the measurement is today, has nothing to do with how wrong it was yesterday\". Is that reasonable? For some noise (e.g. white noise), yes, surely. But perhaps not for all noise. So how can we express that \"if my measurement was wrong yesterday, then it is *likely* to be wrong in the same way today\"? Through covariance, that's how. We can use the same priciples that we employed to specfy parameter spatial/temporal covariance earlier on.\n",
    "\n",
    "We will now demonstrate for the single time series we looked at above. \n",
    "\n",
    "First, construct a geostatistical structure and variogram. Here we are using a range of 90 days; for an exponential variogram `a` is range/3. From these, construct a generic covariance matrix for generic parameters with \"coordinates\" corresponding to the observation times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:30.878028Z",
     "iopub.status.busy": "2022-04-21T02:20:30.877860Z",
     "iopub.status.idle": "2022-04-21T02:20:30.884950Z",
     "shell.execute_reply": "2022-04-21T02:20:30.884521Z"
    }
   },
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(a=30,contribution=1.0)\n",
    "x = obs_select.time.astype(float).values #np.arange(float(obs.time.values[:11].max()))\n",
    "y = np.zeros_like(x)\n",
    "names = [\"obs_{0}\".format(xx) for xx in x]\n",
    "cov = v.covariance_matrix(x,y,names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, scale the values in the generic covariance matrix to reflect the observation uncertainties and take a peek.\n",
    "\n",
    "And there you have it, off-diagonal elements are no longer zero and show greater correlation between uncertainties of observation which happen closer together (it looks a bit wonky because parameter names are not ordered acording to time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:30.887363Z",
     "iopub.status.busy": "2022-04-21T02:20:30.887215Z",
     "iopub.status.idle": "2022-04-21T02:20:30.893648Z",
     "shell.execute_reply": "2022-04-21T02:20:30.893227Z"
    }
   },
   "outputs": [],
   "source": [
    "x_group = cov.x.copy()\n",
    "w = obs_select.weight.mean()\n",
    "v = (1./w)**2\n",
    "x_group *= v\n",
    "df.loc[obs_select.obsnme,obs_select.obsnme] = x_group\n",
    "\n",
    "plt.imshow(df.loc[obs_select.obsnme,obs_select.obsnme].values);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement to that to loop over all the non-zero time series observation groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:30.895953Z",
     "iopub.status.busy": "2022-04-21T02:20:30.895804Z",
     "iopub.status.idle": "2022-04-21T02:20:30.904813Z",
     "shell.execute_reply": "2022-04-21T02:20:30.904436Z"
    }
   },
   "outputs": [],
   "source": [
    "for obgnme in pst.nnz_obs_groups:\n",
    "    obs_select = obs.loc[(obs.obgnme==obgnme) & (obs.weight>0)]\n",
    "\n",
    "    v = pyemu.geostats.ExpVario(a=30,contribution=1.0)\n",
    "    x = obs_select.time.astype(float).values #np.arange(float(obs.time.values[:11].max()))\n",
    "    y = np.zeros_like(x)\n",
    "    names = [\"obs_{0}\".format(xx) for xx in x]\n",
    "    cov = v.covariance_matrix(x,y,names=names)\n",
    "\n",
    "\n",
    "    x_group = cov.x.copy()\n",
    "    w = obs_select.weight.mean()\n",
    "    v = (1./w)**2\n",
    "    x_group *= v\n",
    "    df.loc[obs_select.obsnme,obs_select.obsnme] = x_group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then re-construct a `Cov` object and record it to an external file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:30.907194Z",
     "iopub.status.busy": "2022-04-21T02:20:30.907067Z",
     "iopub.status.idle": "2022-04-21T02:20:30.912701Z",
     "shell.execute_reply": "2022-04-21T02:20:30.912295Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_cov_tv = pyemu.Cov.from_dataframe(df)\n",
    "obs_cov_tv.to_coo(os.path.join(t_d,\"obs_cov.jcb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you made it this far, good on you! Let's finish with some eye candy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:30.914920Z",
     "iopub.status.busy": "2022-04-21T02:20:30.914786Z",
     "iopub.status.idle": "2022-04-21T02:20:30.920416Z",
     "shell.execute_reply": "2022-04-21T02:20:30.920048Z"
    }
   },
   "outputs": [],
   "source": [
    "x = obs_cov_tv.to_pearson().x\n",
    "x[np.abs(x)<0.00001]= np.nan\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beautiful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run PEST once\n",
    "\n",
    "We are going to start off by taking a look at our current objective function value and the relative contributions from the various observation groups. Recall that this is the objective function value with **initial parameter values** and observations weighted according to the **inverse of the measurement standard deviation**.\n",
    "\n",
    "First off, we need to get PEST to run the model once so that the objective function can be calculated. Let's do that now. Start by reading the control file and checking that NOPTMAX is set to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:30.922734Z",
     "iopub.status.busy": "2022-04-21T02:20:30.922599Z",
     "iopub.status.idle": "2022-04-21T02:20:31.125203Z",
     "shell.execute_reply": "2022-04-21T02:20:31.124768Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will reference the PEST control file basename quite a lot, so let's make a variable\n",
    "basename = 'freyberg_mf6'\n",
    "\n",
    "# load the contorl file\n",
    "pst = pyemu.Pst(os.path.join(t_d, f'{basename}.pst'))\n",
    "\n",
    "# check noptmax\n",
    "pst.control_data.noptmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You got a zero? Good, let's run PEST and then update the `Pst` residuals attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:31.127993Z",
     "iopub.status.busy": "2022-04-21T02:20:31.127801Z",
     "iopub.status.idle": "2022-04-21T02:20:37.723700Z",
     "shell.execute_reply": "2022-04-21T02:20:37.723230Z"
    }
   },
   "outputs": [],
   "source": [
    "# run pest++glm; may take a few seconds\n",
    "pyemu.os_utils.run(f\"pestpp-glm {basename}.pst\", cwd=t_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:37.726462Z",
     "iopub.status.busy": "2022-04-21T02:20:37.726213Z",
     "iopub.status.idle": "2022-04-21T02:20:37.737916Z",
     "shell.execute_reply": "2022-04-21T02:20:37.737331Z"
    }
   },
   "outputs": [],
   "source": [
    "# update the Pst residuals from the newly generated *.rei file\n",
    "pst.set_res(os.path.join(t_d, f'{basename}.rei'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. Lets take a look at the value of Phi (e.g. the objective function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:37.740677Z",
     "iopub.status.busy": "2022-04-21T02:20:37.740494Z",
     "iopub.status.idle": "2022-04-21T02:20:37.924351Z",
     "shell.execute_reply": "2022-04-21T02:20:37.923929Z"
    }
   },
   "outputs": [],
   "source": [
    "pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty high. A convenience of weighting with the inverse of the measurement uncertainty is that it is easy to know what the ideal Phi should be: it should be equal to the number of non-zero weighted observations. This of course assumes that all model-to-measurment misfit is due to *measurement* uncertainty. In practice, model error usualy plays a larger role, as we will see in other tutorials. \n",
    "\n",
    "Just to demonstrate what we mean, let's quickly do some math. Imagine that we have:\n",
    " - 2 observations\n",
    " - both have measured values of 1\n",
    " - with measurement standard deviation (e.g. uncertainty) of 0.25\n",
    " - therefore, their are assigned a weight of 1/0.25 = 4\n",
    "\n",
    "So, all that we know is that the true measured values are most likely to be somewhere between 0.75 and 1.25. Therefore, the best that a modelled value should be expected to achieve is 1 +/- 0.25. \n",
    "\n",
    "Let's say we obtain modelled values for each of observation: \n",
    " - 1.25\n",
    " - 0.75\n",
    "\n",
    " Let's calculate Phi for such a case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:37.926927Z",
     "iopub.status.busy": "2022-04-21T02:20:37.926767Z",
     "iopub.status.idle": "2022-04-21T02:20:37.930739Z",
     "shell.execute_reply": "2022-04-21T02:20:37.930321Z"
    }
   },
   "outputs": [],
   "source": [
    "weight = 1/0.25\n",
    "# res = (weight * (meas - sim)) ^ 2\n",
    "res1 = (weight * (1 - 1.25))**2\n",
    "res2 = (weight * (1 - 0.75))**2\n",
    "# sum the squared weighted residuals\n",
    "phi = res1 + res2\n",
    "phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it, the value of Phi is equal to the number of observations. \n",
    "\n",
    "So! Back to Freyberg.\n",
    "\n",
    "How many non-zero observations do we have in the dataset? Recall this number is recorded in the control file and easily accessible through the `Pst.nnz_obs` attribute. So our current Phi (see `pst.phi`) is quite a bit higher than that. Hopefully history matching will help us bring it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:37.933104Z",
     "iopub.status.busy": "2022-04-21T02:20:37.932959Z",
     "iopub.status.idle": "2022-04-21T02:20:37.936745Z",
     "shell.execute_reply": "2022-04-21T02:20:37.936067Z"
    }
   },
   "outputs": [],
   "source": [
    "pst.nnz_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Phi Components\n",
    "\n",
    "Before we race off and start running PEST we should take a look at the components of Phi. \n",
    "\n",
    "The `pst.phi_components` attribute returns a dictionary of the observation group names and their contribution to the overal value of Phi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:37.939179Z",
     "iopub.status.busy": "2022-04-21T02:20:37.938924Z",
     "iopub.status.idle": "2022-04-21T02:20:38.117864Z",
     "shell.execute_reply": "2022-04-21T02:20:38.117426Z"
    }
   },
   "outputs": [],
   "source": [
    "pst.phi_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unfortunately, in this case we have too many observation groups to easily display (we assigned each individual time series to its own observation group; this is a default setting in `pyemu.PstFrom`). \n",
    "\n",
    "So let's use `Pandas` to make help us sumamrize this information (note: `pyemu.plot_utils.res_phi_pie()` does the same thing, but it looks a bit ugly because of the large number of observation groups):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:38.120505Z",
     "iopub.status.busy": "2022-04-21T02:20:38.120263Z",
     "iopub.status.idle": "2022-04-21T02:20:39.177513Z",
     "shell.execute_reply": "2022-04-21T02:20:39.177077Z"
    }
   },
   "outputs": [],
   "source": [
    "phicomp = pd.Series(pst.phi_components)\n",
    "\n",
    "plt.pie(phicomp, labels=phicomp.index.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try displaying this with `pyemu.plot_utils.res_phi_pie()` instead. Because of the large number of columns it's not going to be pretty, but it gets the job done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:39.180046Z",
     "iopub.status.busy": "2022-04-21T02:20:39.179894Z",
     "iopub.status.idle": "2022-04-21T02:20:39.182298Z",
     "shell.execute_reply": "2022-04-21T02:20:39.181947Z"
    }
   },
   "outputs": [],
   "source": [
    "#pyemu.plot_utils.res_phi_pie(pst,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's a bit disturbing. Phi is almost entirely dominated by certain observation groups!\n",
    "\n",
    "If we were to history match with these weights, PEST would prioritize achieveing a good fit with observations of that group. This could mean other observations are ignored. Worse, we might end up with worse fits with other observations in PEST's quest to improve fits with the dominant obsveration group. And, well, if we have gone to the trouble of including these observations in our history matching dataset they must include some usefull information. Why would we throw that out?\n",
    "\n",
    "\n",
    "### 4. Weighting for Visibility\n",
    "\n",
    "As discussed above, a practical means of accommodating this situation is to weight all observation groups\n",
    "such that they contribute an equal amount to the starting measurement objective function. In this\n",
    "manner, no single group dominates the objective function, or is dominated by others; the information\n",
    "contained in each group is therefore equally “visible” to PEST.\n",
    "\n",
    "The `Pst.adjust_weights()` method provides a mechanism to fine tune observation weights according to their contribution to the objective function. (*Side note: the PWTADJ1 utility from the PEST-suite automates this same process of \"weighting for visibility\".*) \n",
    "\n",
    "We start by creating a dictionary of non-zero weighted observation group names and their respective contributions to the objective function. We will specify that we want each group to contribute a value of 100 to the objective function. (Why 100? No particular reason. Could just as easily be 1000. Or 578. Doesn't really matter. 100 is a nice round number though.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:39.184326Z",
     "iopub.status.busy": "2022-04-21T02:20:39.184190Z",
     "iopub.status.idle": "2022-04-21T02:20:39.187919Z",
     "shell.execute_reply": "2022-04-21T02:20:39.187496Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a dictionary of group names and weights\n",
    "balanced_groups = {grp:100 for grp in pst.nnz_obs_groups}\n",
    "balanced_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `pst.adjust_weights()` method to adjust the observation weights in the `pst` control file object. (*Remember! This all only happens in memory. It does not get written to the PEST control file yet!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:39.190485Z",
     "iopub.status.busy": "2022-04-21T02:20:39.190356Z",
     "iopub.status.idle": "2022-04-21T02:20:39.211439Z",
     "shell.execute_reply": "2022-04-21T02:20:39.210992Z"
    }
   },
   "outputs": [],
   "source": [
    "# make all non-zero weighted groups have a contribution of 100.0\n",
    "pst.adjust_weights(obsgrp_dict=balanced_groups,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now plot the phi components again, voila! We have a nice even distribution of phi from each observation group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:39.214335Z",
     "iopub.status.busy": "2022-04-21T02:20:39.214165Z",
     "iopub.status.idle": "2022-04-21T02:20:40.350085Z",
     "shell.execute_reply": "2022-04-21T02:20:40.349629Z"
    }
   },
   "outputs": [],
   "source": [
    "phicomp = pd.Series(pst.phi_components)\n",
    "plt.pie(phicomp, labels=phicomp.index.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to write the updated pest control file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:40.352600Z",
     "iopub.status.busy": "2022-04-21T02:20:40.352444Z",
     "iopub.status.idle": "2022-04-21T02:20:40.520098Z",
     "shell.execute_reply": "2022-04-21T02:20:40.519600Z"
    }
   },
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d, 'freyberg_wv.pst')) # we are using the \"_wv\" sufix to indicate Weighted for Visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Understanding Observation Weights and Measurement Noise\n",
    "\n",
    "There is a common source of confusion: observation weights for history matching *versus* observation weights representing observation error. \n",
    "\n",
    " - when **history-matching**, observation weights listed in the control file determine their contribution to the objective function and therefore their influence on the parameter estimation process. Here, observation weights may be assigned to reflect observation uncertainty, the balance required for equal \"visibility\", or other modeller-defined (and perhaps subjective...) measures of observation worth.  \n",
    " - when undertaking **FOSM** or when **generating an observation ensemble** for use with `pestpp-ies`, weights should reflect the inverse of the standard deviation of measurement noise. \n",
    "    - Keep in mind that, unless instructed otherwise, `pestpp-ies` will generate an observation ensemble *using observation weights in the PEST control file*. Therefore, when history-matching with `pestpp-ies` and using weights that **do not** reflect observation uncertainty, it is important to provide `pestpp-ies` with a previously prepared observation ensemble. \n",
    "    - Another aspect to consider is that, (unless instructed not to) `pestpp-glm` undertakes FOSM during parameter estimation. In doing so, observation weights in the control file influence how observation noise is calculated. The assumption being that weights are the inverse of standard deviation of measurement noise. If a \"weighting for visibility\" approach is adopted, then FOSM undertaken by `pestpp-glm` will no longer be representative. Instead, FOSM should be undertaken separetly (e.g. using pyEMU or the PEST utilities).\n",
    "\n",
    "So, in the current tutorial our observation weights no longer reflect measurement noise. If we were to use this control file with `pestpp-ies` later, we would have to explicitly provide an observation ensemble. If we were to use it for parameter estimation with `pestpp-glm`, we would need to undertake FOSM in a subsequent step.\n",
    "\n",
    "Now we will generate an observation ensemble and visualize the effect of different weighting schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by visualizing what weights actually imply. The function in the next cell does a bunch of funky things in order to plot observation time series (convenient that we stored all the necessary information in observation names, hey?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:40.522928Z",
     "iopub.status.busy": "2022-04-21T02:20:40.522740Z",
     "iopub.status.idle": "2022-04-21T02:20:40.529377Z",
     "shell.execute_reply": "2022-04-21T02:20:40.528889Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_obs_ts(pst, oe, obsgrps=pst.nnz_obs_groups):\n",
    "    # get times and \"measured values\"\n",
    "    nz_obs = pst.observation_data.loc[pst.nnz_obs_names,:].copy()\n",
    "    nz_obs['time'] = nz_obs['time'].astype(float)\n",
    "    nz_obs.sort_values(['obgnme','time'], inplace=True)\n",
    "    \n",
    "    # to plot crrent model outputs\n",
    "    res = pst.res.copy()\n",
    "    res['time'] = pst.observation_data['time'].astype(float)\n",
    "    res.sort_values(['group','time'], inplace=True)\n",
    "\n",
    "    for nz_group in obsgrps:\n",
    "        nz_obs_group = nz_obs.loc[nz_obs.obgnme==nz_group,:]\n",
    "        nz_obs_meas = res.loc[(res['group']==nz_group) & res['weight']!=0]\n",
    "\n",
    "        fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "        \n",
    "        [ax.plot(nz_obs_group.time, oe.loc[r,nz_obs_group.obsnme] ,color=\"r\",lw=0.3) for r in oe.index]\n",
    "        ax.plot(nz_obs_group.time,nz_obs_group.obsval,\"b\")\n",
    "        #ax.plot(nz_obs_meas.time, nz_obs_meas.modelled, 'k', linestyle='--')\n",
    "        ax.set_title(nz_group)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's start with the \"weighting for visibility\" weights wich we have just specified:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:40.531878Z",
     "iopub.status.busy": "2022-04-21T02:20:40.531714Z",
     "iopub.status.idle": "2022-04-21T02:20:40.555377Z",
     "shell.execute_reply": "2022-04-21T02:20:40.554851Z"
    }
   },
   "outputs": [],
   "source": [
    "oe = pyemu.ObservationEnsemble.from_gaussian_draw(pst=pst, \n",
    "                                                num_reals=50) # the number of realisations; usualy would be more here we just use a few for demo purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the time series of observations for the SFR measurment types. Run the next cell.\n",
    "\n",
    "The blue line is the time series of \"field measured\" values (in the control file). The red lines are time series from the observation ensemble. What we are saying with these weights is that any of these red lines could also have been \"measured\". Due to the noise (e.g. uncertainty) in our measurements, the true value can fall any where in that range.\n",
    "\n",
    "Do they look right to you? It may not be imediately apaprent, but the spread of values for the \"sfrtd\" observation group is a bit wide...that's odd isn't it? We said that uncertainty of \"differences\" between observations tends to be smaller... \n",
    "\n",
    "Well this is because the weights here do not reflect mearument uncertainty. They should **not** be used to generate observation ensembles as they do not reflect measurement noise..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:40.557970Z",
     "iopub.status.busy": "2022-04-21T02:20:40.557775Z",
     "iopub.status.idle": "2022-04-21T02:20:41.691122Z",
     "shell.execute_reply": "2022-04-21T02:20:41.690681Z"
    }
   },
   "outputs": [],
   "source": [
    "sfr_ts_obs = [i for i in pst.nnz_obs_groups if 'sfr' in i]\n",
    "\n",
    "plot_obs_ts(pst, oe, sfr_ts_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that again. But this time, let's use the correct weights. Recall from the \"freyberg pstfrom pest setup\" notebook that we made a point of recording two observation uncertainty covariance matrices. We can use these to inform our observation ensemble (convenient huh?). Let's start by reading the covariance matrix which assumes no correlation between noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:41.693731Z",
     "iopub.status.busy": "2022-04-21T02:20:41.693552Z",
     "iopub.status.idle": "2022-04-21T02:20:41.706349Z",
     "shell.execute_reply": "2022-04-21T02:20:41.705568Z"
    }
   },
   "outputs": [],
   "source": [
    "# load a Cov object from the covraiance matrix assuming no observation uncertantiny autocorrleation\n",
    "obs_cov_diag = pyemu.Cov.from_binary(os.path.join(t_d,'obs_cov_diag.jcb' ))\n",
    "\n",
    "# generate an ensemble using the Cov\n",
    "oe2 = pyemu.ObservationEnsemble.from_gaussian_draw(pst=pst, cov=obs_cov_diag, \n",
    "                                                num_reals=50) # the number of realisations; usualy would be more here we just use a few for demo purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now if we plot this ensemble...there we go, that cloud of spaghetti lines for the \"difference\" observations are nice and tight, reflecting our greater confidence in the accuracy of these \"observed\" values.\n",
    "\n",
    "Take a look at the first two plots (time series of the absolute SFR flux values). They don't look too bad, hey? Although...the red lines do still jump around quite a bit...looks a bit like white noise. Good thing we generated a covariance matrix that accounts for autocorrelated transient noise, hey? (see the \"freyberg pstfrom pest setup\" notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:41.710101Z",
     "iopub.status.busy": "2022-04-21T02:20:41.709868Z",
     "iopub.status.idle": "2022-04-21T02:20:41.717645Z",
     "shell.execute_reply": "2022-04-21T02:20:41.717147Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_obs_ts(pst, oe2, sfr_ts_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that one last time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:41.720348Z",
     "iopub.status.busy": "2022-04-21T02:20:41.720183Z",
     "iopub.status.idle": "2022-04-21T02:20:41.729726Z",
     "shell.execute_reply": "2022-04-21T02:20:41.728873Z"
    }
   },
   "outputs": [],
   "source": [
    "# load a Cov object from the covraiance matrix assuming there is observation uncertantiny autocorrleation\n",
    "obs_cov = pyemu.Cov.from_binary(os.path.join(t_d,'obs_cov.jcb' ))\n",
    "\n",
    "# generate an ensemble using the Cov\n",
    "oe3 = pyemu.ObservationEnsemble.from_gaussian_draw(pst=pst, cov=obs_cov, \n",
    "                                                num_reals=50) # the number of realisations; usualy would be more here we just use a few for demo purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now the red lines are a bit less erratic, reflecting the correlation we have specified for noise in observations that were measured closer together (in time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T02:20:41.732877Z",
     "iopub.status.busy": "2022-04-21T02:20:41.732691Z",
     "iopub.status.idle": "2022-04-21T02:20:41.739303Z",
     "shell.execute_reply": "2022-04-21T02:20:41.738829Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_obs_ts(pst, oe3, sfr_ts_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78da96ffabf66bf78726b926aec53534d931d5dbfb06e33f81c5816276fdb121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
